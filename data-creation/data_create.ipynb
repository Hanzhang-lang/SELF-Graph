{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Critic data creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "with open(\"../output/testtime1_1105_dev.json\", 'r') as f:\n",
    "    dev_data = json.load(f)\n",
    "len(dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [No Retrieval]\n",
      " [No Retrieval]\n"
     ]
    }
   ],
   "source": [
    "real_count = 0\n",
    "false_count = 0\n",
    "for d in dev_data:\n",
    "    content = d['instruction'] + d['input']\n",
    "    completion = client.chat.completions.create(\n",
    "    model=\"/media/disk1/chatgpt/LLaMA-Factory/saves/llama3-8b/full/sft/\", \n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": content}\n",
    "    ]\n",
    "    )\n",
    "    if completion.choices[0].message.content == d['output']:\n",
    "        real_count += 1\n",
    "    else:\n",
    "        print(completion.choices[0].message.content, d['output'])\n",
    "        false_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='你好！有什么我可以帮助你的吗？', additional_kwargs={'refusal': None}, response_metadata={'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}, id='run-ab72e491-733a-43cc-881f-61f90729e518-0', usage_metadata={'input_tokens': 8, 'output_tokens': 28, 'total_tokens': 36, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke('你好')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"../data/WebQSP.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "\n",
    "multi_graph_three_way = \"\"\"You will be provided with a query, evidence, output sentence, and preceding sentences (optional). Your task is to determine whether the information in the output sentence can be fully verified by the evidence or if it requires further external verification. There are three cases:\\n\\n",
    "- If the output sentence can be verified solely with the evidence and the preceding sentences, then respond with [No Retrieval].\\n",
    "- If additional information about the tail entity in evidence is needed to verify the output sentence, respond with [Continue to Retrieve Evidence]\\n",
    "- If more information unrelated to the evidence is needed, e.g. totally new relationship or new entity, reponse with [New Retrieval]. Please provide explanations for your judgments. \\n\\n\\n",
    "###\\nQuery: {query}\\n\\n",
    "Preceding sentences: {preceding_sentences}\\n\\n",
    "Evidende: {evidence}\n",
    "Output: {target_output}\n",
    "Rating: \\n",
    "\"\"\"\n",
    "multi_graph_relevance =  \"\"\"You'll be provided with a query, along with evidence and possibly some preceding sentences. \\n",
    "Your job is to determine if the evidence is relevant to the initial query and the preceding context. \\n",
    "If the evidence meets this requirement, respond with [Relevant]; otherwise, generate [Irrelevant].\\n\\n\\n",
    "###\\nQuery: {query}\\n\\n",
    "Preceding sentences: {preceding_sentences}\\n\\n",
    "Evidende: {evidence}\n",
    "Rating: \"\"\"\n",
    "\n",
    "multi_graph_groundness =  \"\"\"You'll be provided with a query, along with evidence and possibly some preceding sentences. \\n",
    "Your task is to evaluate if the query can be fully supported by the information provided in the evidence, and provide explanations on your judgement.\\n",
    "Use the following entailment scale to generate a score:\\n",
    "[Fully supported] - All information needed is supported by the evidence. \\n",
    "[Partially supported] - The query is supported by the evidence to some extent, but there is major information that is not discussed in the evidence. For example, if the query asks about two concepts and the evidence only discusses either of them, it should be considered a [Partially supported].\\n",
    "[No support / Contradictory] - The query is completely unrelated to the evidence, or contradicts the evidence.\\n",
    "###\\nQuery: {query}\\n\\n",
    "Preceding sentences: {preceding_sentences}\\n\\n",
    "Evidende: {evidence}\n",
    "Rating: \\n",
    "\"\"\"\n",
    "\n",
    "multi_graph_utility = \"\"\"\\n",
    "Given a query and an output, rate whether the response appears to be a helpful and informative answer to the query, from 1 (lowest) - 5 (highest). We call this score perceived utility.\\n",
    "\\n",
    "The detailed criterion is as follows:\\n",
    "5: The response provides a complete, highly detailed, and informative response to the query, fully satisfying the information needs.\\n",
    "4: The response mostly fulfills the need in the query, while there can be some minor improvements such as discussing more detailed information, having better structure of the response, or improving coherence.\\n",
    "3: The response is acceptable, but some major additions or improvements are needed to satisfy users' needs.\\n",
    "2: The response still addresses the main request, but it is not complete or not relevant to the query.\\n",
    "1: The response is barely on-topic or completely irrelevant.\\n",
    "##\\n",
    "\\n",
    "Query: Who is the current prime minister of the UK as of 2023?\\n",
    "Output: Boris Johnson was the prime minister of the UK from 2019 - 2022.\\n",
    "Perceived utility: 2\\n",
    "Explanation: While the output provides a factually correct statement about the UK prime minister from 2019 to 2022, this instruction asks who the prime minister is as of 2023, so it doesn't answer the instruction. Therefore, the utility is 2.\\n",
    "\\n",
    "##\\n",
    "\\n",
    "Query: Given a description of a travel destination, recommend 10 tourist attractions with detailed explanations of each. The travel destination is Tokyo, Japan.\\n",
    "Output: 'Tokyo is a vibrant city full of exciting tourist attractions. Some must-see sights include the Tokyo Skytree, Tokyo Disneyland, Sensoji Temple, Meiji Shrine, Tsukiji Fish Market, Harajuku, and Shinjuku Gyoen.\\n",
    "Perceived utility: 3\\n",
    "Explanation: This output doesn't provide descriptions of each attraction and the number of the attractions is also less than 10. While this output partially answers the instructions, it doesn't match the instructions strictly.\\n",
    "\\n",
    "##\\n",
    "\\n",
    "Query: {query}\\n",
    "Output:{output}\\n",
    "\"\"\"\n",
    "\n",
    "multi_graph_three_way_prompt = PromptTemplate(input_variables=[\"query\", \"preceding_sentences\", \"evidence\", \"target_output\"], template=\n",
    "multi_graph_three_way)\n",
    "multi_graph_relevance_prmpt = PromptTemplate(input_variables=[\"query\", \"preceding_sentences\", \"evidence\"], template=\n",
    "multi_graph_relevance)\n",
    "multi_graph_groundness_prompt = PromptTemplate(input_variables=[\"query\", \"preceding_sentences\", \"evidence\",], template=\n",
    "multi_graph_groundness)\n",
    "multi_graph_utility_prompt = PromptTemplate(input_variables=[\"query\", \"output\"], template=multi_graph_utility)\n",
    "# prompt_template = FewShotPromptTemplate(\\n",
    "#         examples=examples_dict,\\n",
    "#         example_prompt=examples_prompt,\\n",
    "#         prefix=\\n",
    "#         \"\"\"\\n",
    "# Based on the Table below, your task is to accurately output columns related to the query or contain useful information about the query. This process involves linking similar words or semantically similar terms to columns in the Table.\\n",
    "# Approach this task as follows:\\n",
    "# Read the query and extra information thoroughly and list every possible link from query term to column in the Table. \\n",
    "# Then based on the column linking, output all useful columns at last. Make sure all columns in the linking step are included and every column is in the Table.\"\"\",\\n",
    "#         suffix=\\n",
    "#         \"\"\"\\n",
    "# Table: {table}\\n",
    "# Extra information: {aug}\\n",
    "\\n",
    "# Query: {claim}\"\"\",\\n",
    "#         input_variables=[\"table\", \"claim\", \"aug\"],\\n",
    "# )\\n",
    "# \"[Continue to Retrieve Evidence]\\n\\nExplanation: The evidence provided only mentions Viggo Mortensen as a film actor, but does not provide any information about his role as Aragorn. To verify if Aragorn influenced Samuel Taylor Coleridge, we would need additional information about Viggo Mortensen's portrayal of Aragorn in a specific context that could have influenced Coleridge.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(llm=model, prompt=multi_graph_relevance_prmpt, verbose=True)\n",
    "batch_pred = llm_chain.batch([{\"query\": data[0]['RawQuestion'], \"preceding_sentences\": \"\", \"evidence\": \" relationship: influence.influence_node.influenced_by\",}], return_only_outputs=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_pred[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(llm=model, prompt=examples_prompt, verbose=True)\n",
    "batch_pred = llm_chain.batch([{\"query\": data[0]['RawQuestion'], \"preceding_sentences\": \"\", \"evidence\": \"(Viggo Mortensen, film.actor.film, Unknown_Name)\", \"target_output\": \"Aragorn\"}], return_only_outputs=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_DICT = {\n",
    "    \"ground_instruction\": (\n",
    "        \"You will be given a query and evidence. Your objective is to assess the extent to which the query is supported by the information presented in the evidence.\\n\"\n",
    "        \"Rate the level of support from [Ignore / Contradictory], [Little support], [Partially supported], [Mostly supported], [Fully supported].\"\n",
    "    ),\n",
    "    \"ground_input\": (\n",
    "        \"Query: {query}\\n\"\n",
    "        \"Evidence: {evidence}\"\n",
    "    ),\n",
    "    \"ground_instruction_multi\": (\n",
    "        \"You will receive a query, evidence, and output, and optional preceding sentences. Your task is to evaluate whether the output filters out valid information from the evidence to answer the given query, and provide explanations on your judgement\\n\"\n",
    "        \"Use the following entailment scale to generate a score:\\n\"\n",
    "        \"[Fully supported] - the output contains sufficient information needed to answer the query, fully satisfying the information needs.\\n\"\n",
    "        \"[Partially supported] - The output contains information to some extent, but there is major information in evidence that is not discussed in the output. For example, if a query asks about two concepts and the output only discusses either of them, it should be considered a [Partially supported].\\n\" \n",
    "        \"[No support / Contradictory] - The output completely ignores evidence, or is unrelated to the query.\\n\\n\"\n",
    "        # \"Make sure to not use any external information/knowledge to judge whether the output is true or not.\\n\\n\"\n",
    "    ),\n",
    "    \"relevance_instruction\": (\n",
    "        \"You will receive a query, topic entity, evidence and optional preceding sentences containing history information. The evidence contains graph relationships or entities may be useful to answer the query. Your task is to filters out 3 valid information from the evidence that contribute to answering the query and provide a relevance score for each output.\\n\"\n",
    "        \"The score of relevance range from [Fully Relavant], [Partially Relevant] to [Unrelevant].\"\n",
    "    ),\n",
    "    \"relevance_input\": (\n",
    "        \"Query: {query}\\n\"\n",
    "        \"Topic entity: {topic}\\n\"\n",
    "        \"Evidence: {evidence}\\n\"\n",
    "        \"Preceding sentences: {preceding_sentences}\"\n",
    "    ),\n",
    "    \"entity_instruction\": (\n",
    "        \"You will receive a query, evidence and optional preceding sentences containing history information. The evidence contains the associated retrieved knowledge graph triplets (head_entity, relation, tail_entity). Your task is to rate relevance score for each tail_entity from the evidence.\"\n",
    "        \"The score of relevance range from [Fully Relevant], [Partially Relevant] to [Unrelevant].\"\n",
    "    ),\n",
    "    \"entity_input\": (\n",
    "        \"Query: {query}\\n\"\n",
    "        \"Evidence: {evidence}\\n\"\n",
    "        \"Preceding sentences: {preceding_sentences}\"\n",
    "    ),\n",
    "    \"ground_multi_input\": (\n",
    "        \"Task instruction: {instruction}\\n\"\n",
    "        \"Preceding sentences: {preceding_sentences}\\n\"\n",
    "        \"Output: {target_output}\\n\"\n",
    "        \"Evidence: {evidence}\"\n",
    "    ),\n",
    "    \"ground_multi_input_wo_preceding\": (\n",
    "        \"Task instruction: {instruction}\\n\"\n",
    "        \"Output: {target_output}\\n\"\n",
    "        \"Evidence: {evidence}\"\n",
    "    ),\n",
    "    \"retrieval_multi_instruction\": (\n",
    "        \"You will be provided with an instruction, evidence, output sentence, and preceding sentences (optional). If the preceding sentence is given, the output should be the sentence that follows those preceding sentences.  Your task is to determine whether the information in the output sentence can be fully verified by the evidence or if it requires further external verification. If the output sentence can be verified solely with the evidence or doesn’t require any verification, respond with [No Retrieval]. If additional information is needed to verify the output sentence, respond with [Retrieval]. Please provide explanations for your judgments.\\n\\n\" \n",
    "    ),\n",
    "    \"retrieval_multi_input\": (\n",
    "        \"Task instruction: {instruction}\\n\"\n",
    "        \"Preceding sentences: {preceding_sentences}\\n\"\n",
    "        \"Evidence: {evidence}\\n\"\n",
    "        \"Output: {target_output}\"\n",
    "    ),\n",
    "    \"multi_retrieval_three_way_instruction\": (\n",
    "        \"You will be provided with a query, evidence, output sentence, and preceding sentences (optional). Your task is to determine whether the information in the output sentence can be fully verified by the evidence or if it requires further external verification. There are three cases:\\n\" \n",
    "        \"- If the output sentence can be verified solely with the evidence and the preceding sentences, then respond with [No Retrieval]. \\n\"\n",
    "        \"- If additional information about the tail entity in evidence is needed to verify the output sentence, respond with [Continue to Retrieve Evidence] \\n\"\n",
    "        \"- If more information unrelated to the evidence is needed, e.g. totally new relationship or new entity, reponse with [New Retrieval].\\n\\n\"\n",
    "    ),\n",
    "    \"multi_retrieval_three_way_input\": (\n",
    "        \"Query: {query}\\n\"\n",
    "        \"Preceding sentences: {preceding_sentences}\\n\"\n",
    "        \"Evidence: {evidence}\\n\"\n",
    "        \"Output: {target_output}\"\n",
    "    ),\n",
    "     \"multi_retrieval_three_way_input_wo_preceding\": (\n",
    "        \"Query: {query}\\n\"\n",
    "        \"Evidence: {evidence}\\n\"\n",
    "        \"Output: {target_output}\"\n",
    "    ),\n",
    "    \"utility_instruction\": (\n",
    "        \"Given an instruction and an output, rate whether the response appears to be a helpful and informative answer to the query, from 1 (lowest) - 5 (highest). We call this score perceived utility.\\n\"\n",
    "        \"[Utility:5]: The response provides a complete, highly detailed, and informative response to the query, fully satisfying the information needs.\\n\"\n",
    "        \"[Utility:4]: The response mostly fulfills the need in the query, while there can be some minor improvements such as discussing more detailed information, having better structure of the response, or improving coherence. \\n\"\n",
    "        \"[Utility:3]: The response is acceptable, but some major additions or improvements are needed to satisfy users' needs.\\n\"\n",
    "        \"[Utility:2]: The response still addresses the main request, but it is not complete or not relevant to the query.\\n\"\n",
    "        \"[Utility:1]: The response is barely on-topic or completely irrelevant.\\n\"\n",
    "    ),\n",
    "    \"utility_input\": (\n",
    "        \"Task instruction: {instruction}\\n\"\n",
    "        \"Output: {output}\"\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "#解决retrieve entity 过多的问题\n",
    "with open(\"../data/WebQSP.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "input_data = []\n",
    "for line in data:\n",
    "    candidate_answer = []\n",
    "    relation_paths = []\n",
    "    continue_paths = []\n",
    "    for p in line['Parses']:\n",
    "        candidate_answer.extend([ans[\"EntityName\"] for ans in p['Answers'] if ans[\"EntityName\"] and ans[\"EntityName\"] not in candidate_answer ])\n",
    "        if p['InferentialChain'] and p['InferentialChain'] not in relation_paths:\n",
    "            relation_paths.append(p['InferentialChain'])\n",
    "    for path in relation_paths:\n",
    "        if len(path) > 1:\n",
    "            continue_paths.append([path[:i] for i in range(len(path))])\n",
    "    input_data.append({\"input\": {\"query\": line['RawQuestion'], \"preceding_sentences\": \"\", \"evidence\": \"\", \"target_output\": ';'.join(candidate_answer)}, \"decision_token\": \"[New Retrieval]\"})\n",
    "    for c in continue_paths:\n",
    "        if len(c) == 1:\n",
    "            input_data.append({\"input\": {\"query\": line['RawQuestion'], \"preceding_sentences\": \"\", \"evidence\": f\"Relations retrieved: {c[-1]}\", \"target_output\": ';'.join(candidate_answer)}, \"decision_token\": \"[Continue to Retrieve Evidence]\"})\n",
    "        else:\n",
    "            input_data.append({\"input\": {\"query\": line['RawQuestion'], \"preceding_sentences\": f\"Relations retrieved: {c[:-1]}\", \"evidence\": f\"Relations retrieved: {c[-1]}\", \"target_output\": ';'.join(candidate_answer)}, \"decision_token\": \"[Continue to Retrieve Evidence]\"})\n",
    "    #生成relation path，根据relation path构造\n",
    "    for p in relation_paths:\n",
    "        if len(p) ==1:\n",
    "            input_data.append({\"input\": {\"query\": line['RawQuestion'], \"preceding_sentences\": \"\", \"evidence\": f\"{p[-1]}\", \"target_output\": ';'.join(candidate_answer)}, \"decision_token\": \"[No Retrieval]\"})\n",
    "        else:\n",
    "            input_data.append({\"input\": {\"query\": line['RawQuestion'], \"preceding_sentences\": f\"Relations retrieved: {p[:-1]}\", \"evidence\": f\"Relations retrieved: {p[-1]}\", \"target_output\": ';'.join(candidate_answer)}, \"decision_token\": \"[No Retrieval]\"})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def create_retrieval_data(input_data, multi_retrieval=False):\n",
    "    print(\"creating multi sentence retrieval data\")\n",
    "    processed_data = []\n",
    "    for item in input_data:\n",
    "        input = item[\"input\"]\n",
    "        output = item[\"decision_token\"]\n",
    "        if len(str(output)) == 0:\n",
    "            continue\n",
    "        if output not in [ \"[New Retrieval]\", \"[Continue to Retrieve Evidence]\", \"[No Retrieval]\"]:\n",
    "            continue\n",
    "\n",
    "        if len(input[\"preceding_sentences\"]) == 0:\n",
    "            processed_data.append({\"instruction\": PROMPT_DICT[\"multi_retrieval_three_way_instruction\"], \"input\": PROMPT_DICT[\"multi_retrieval_three_way_input_wo_preceding\"].format_map(input), \"output\": output, \"task\": \"multi_retrieval\"})\n",
    "        else:\n",
    "            processed_data.append({\"instruction\": PROMPT_DICT[\"multi_retrieval_three_way_instruction\"], \"input\": PROMPT_DICT[\"multi_retrieval_three_way_input\"].format_map(input), \"output\": output, \"task\": \"retrieval\"})\n",
    "            \n",
    "    print(processed_data[-1])\n",
    "    print(\"total data number: {}\".format(len(processed_data)))\n",
    "    print(Counter([item[\"output\"] for item in processed_data ]))\n",
    "    return processed_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "from collections import Counter\n",
    "def create_relevance_data(input_data):\n",
    "    print(\"creating relevance data\")\n",
    "    processed_data = []\n",
    "    for item in input_data:\n",
    "        output = ''\n",
    "        match_dict= item[\"match\"]\n",
    "        for r, s in match_dict.items():\n",
    "            output += f\"{r}{s}; \"\n",
    "        # if label == \"[Relevant]\" and random.random() > 0.7:\n",
    "        #     continue\n",
    "        processed_data.append({\"instruction\": PROMPT_DICT[\"relevance_instruction\"], \"input\": PROMPT_DICT[\"relevance_input\"].format_map(item['input']), \"output\": output, \"task\": \"relation_relevance\"})\n",
    "    print(processed_data[-1])\n",
    "    print(\"total data number: {}\".format(len(processed_data)))\n",
    "    # print(Counter([item[\"output\"] for item in processed_data ]))\n",
    "    return processed_data\n",
    "\n",
    "def create_entity_data(input_data):\n",
    "    print(\"creating entity data\")\n",
    "    processed_data = []\n",
    "    \n",
    "    for item in input_data:\n",
    "        output = ''\n",
    "        match_dict= item[\"match\"]\n",
    "        for r, s in match_dict.items():\n",
    "            output += f\"{r}{s}; \"\n",
    "        # if label == \"[Relevant]\" and random.random() > 0.7:\n",
    "        #     continue\n",
    "        processed_data.append({\"instruction\": PROMPT_DICT[\"entity_instruction\"], \"input\": PROMPT_DICT[\"entity_input\"].format_map({\"query\": item['query'], \"evidence\": item['evidence'], \"preceding_sentences\": \"\"}), \"output\": output, \"task\": \"entity_relevance\"})\n",
    "    print(processed_data[-1])\n",
    "    print(\"total data number: {}\".format(len(processed_data)))\n",
    "    # print(Counter([item[\"output\"] for item in processed_data ]))\n",
    "    return processed_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../training_data/entity_relation_relevance.json', 'w') as f:\n",
    "    json.dump(relevance_data + entity_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating entity data\n",
      "{'instruction': 'You will receive a query, evidence and optional preceding sentences containing history information. The evidence contains the associated retrieved knowledge graph triplets (head_entity, relation, tail_entity). Your task is to rate relevance score for each tail_entity from the evidence.The score of relevance range from [Fully Relavant], [Partially Relevant] to [Unrelevant].', 'input': 'Query: what do you call russian currency\\nEvidence: (Russia, location.country.currency_used, Russian ruble)Preceding sentences: ', 'output': 'Russian ruble[Fully Relavant]; ', 'task': 'relevance'}\n",
      "total data number: 6110\n"
     ]
    }
   ],
   "source": [
    "entity_data = create_entity_data(relevance_entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_groundness_data(input_data, multi_retrieval=False):\n",
    "    print(\"creating groundness data\")\n",
    "    processed_data = []\n",
    "    for item in input_data:\n",
    "        if multi_retrieval is True:\n",
    "            if \"sent_idx\" not in item or item[\"sent_idx\"] == 0 or len(item[\"preceding_sentences\"]) == 0:\n",
    "                processed_data.append({\"instruction\": PROMPT_DICT[\"ground_multi_instruction\"], \"input\": PROMPT_DICT[\"ground_multi_input_wo_preceding\"].format_map(input), \"output\": item[\"score\"], \"task\": \"groudness\"})\n",
    "            else:\n",
    "                processed_data.append({\"instruction\": PROMPT_DICT[\"ground_multi_instruction\"], \"input\": PROMPT_DICT[\"ground_multi_input\"].format_map(input), \"output\": item[\"score\"], \"task\": \"groudness\"})\n",
    "        else: \n",
    "            processed_data.append({\"instruction\": PROMPT_DICT[\"ground_instruction\"], \"input\": PROMPT_DICT[\"ground_input\"].format_map(item['input']), \"output\": item[\"score\"], \"task\": \"groudness\"})\n",
    "    print(processed_data[-1])\n",
    "    print(\"total data number: {}\".format(len(processed_data)))\n",
    "    print(Counter([item[\"output\"] for item in processed_data ]))\n",
    "    return processed_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### complete chain create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"2b219db0d2984f9dae28b651ab8ab3d9\"\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://smsh.openai.azure.com/\"\n",
    "os.environ[\"AZURE_OPENAI_API_VERSION\"] = \"2024-03-01-preview\"\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",\n",
    ")\n",
    "\n",
    "from langchain.storage import LocalFileStore, RedisStore\n",
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "# store = RedisStore(redis_url=\"redis://localhost:6379\")\n",
    "# cached_embedder = CacheBackedEmbeddings.from_bytes_store(\n",
    "# embeddings, store, namespace=\"openai\"\n",
    "# )\n",
    "# row_string = []\n",
    "# with open('../data/clean_relations', 'r') as f:\n",
    "#     data = f.readlines()\n",
    "# db = FAISS.from_texts(data, cached_embedder)\n",
    "# retriever = db.as_retriever(search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/media/disk1/chatgpt/zh/graph_data')\n",
    "from src.graph_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c39d1c92bc44639a0f7c1c74fcfcd4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5499d47036d54be594559c3d21e26b9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datasets\n",
    "import networkx as nx\n",
    "from collections import deque\n",
    "import walker\n",
    "import json\n",
    "def build_Digraph(graph: list) -> nx.Graph:\n",
    "    G = nx.DiGraph()\n",
    "    for triplet in graph:\n",
    "        h, r, t = triplet\n",
    "        G.add_edge(h, t, relation=r.strip())\n",
    "    return G\n",
    "\n",
    "def bfs_with_rule(graph, start_node, target_rule, max_p = 10):\n",
    "    result_paths = []\n",
    "    queue = deque([(start_node, [])])\n",
    "    while queue:\n",
    "        current_node, current_path = queue.popleft()\n",
    "        if len(current_path) == len(target_rule):\n",
    "            result_paths.append(current_path)\n",
    "        if len(current_path) < len(target_rule):\n",
    "            if current_node not in graph:\n",
    "                continue\n",
    "            for neighbor in graph.neighbors(current_node):\n",
    "                rel = graph[current_node][neighbor]['relation']\n",
    "                if rel != target_rule[len(current_path)] or len(current_path) > len(target_rule):\n",
    "                    continue\n",
    "                queue.append((neighbor, current_path + [(current_node, rel,neighbor)]))\n",
    "            for neighbor in graph.predecessors(current_node):\n",
    "                rel = graph[neighbor][current_node]['relation']\n",
    "                if rel != target_rule[len(current_path)] or len(current_path) > len(target_rule):\n",
    "                    continue\n",
    "                queue.append((neighbor, current_path + [(current_node, rel,neighbor)]))\n",
    "    \n",
    "    return result_paths\n",
    "# relation_data_train = datasets.load_dataset('rmanluo/RoG-webqsp', split='train')\n",
    "relation_data_train = datasets.load_dataset('rmanluo/RoG-cwq', split='validation')\n",
    "\n",
    "\n",
    "# llm_chain = LLMChain(llm=model, prompt=multi_graph_groundness_prompt, verbose=True)\n",
    "# batch_pred = llm_chain.batch([{\"query\": data[0]['RawQuestion'], \"preceding_sentences\": \"\", \"evidence\": \" relationship: influence.influence_node.influenced_by\",}], return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1468851539.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[9], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    id, sent_idx, q_entity, a_entity, inferential_chain, candidate relation, candidate entity\u001b[0m\n\u001b[0m                                                                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "id, sent_idx, q_entity, a_entity, inferential_chain, candidate relation, candidate entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "def save_to_json(data: List, data_path='../output/chain_data.json'):\n",
    "    if not os.path.isfile(data_path):\n",
    "        # 文件不存在，创建新列表并写入文件\n",
    "        with open(data_path, 'w', encoding='utf-8') as file:\n",
    "            json.dump(data, file, ensure_ascii=False, indent=4)\n",
    "        return\n",
    "    try:\n",
    "        # 尝试读取现有文件\n",
    "        with open(data_path, 'r', encoding='utf-8') as file:\n",
    "            # 加载现有的JSON数据\n",
    "            existing_data = json.load(file)\n",
    "            existing_data.extend(data)\n",
    "    except json.JSONDecodeError:\n",
    "        # 文件不是有效的JSON，打印错误信息并退出\n",
    "        print(f\"文件 {data_path} 不是有效的JSON格式。\")\n",
    "        return\n",
    "    except ValueError as e:\n",
    "        # 打印错误信息并退出\n",
    "        print(e)\n",
    "        return\n",
    "    # 将更新后的数据写回文件\n",
    "    with open(data_path, 'w', encoding='utf-8') as file:\n",
    "        json.dump(existing_data, file, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../output/chain_data_cwq_top_5.json', 'r') as f:\n",
    "    now_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_data = []\n",
    "for index, line in enumerate(relation_data_train):\n",
    "\n",
    "    id = line['id']\n",
    "    topic_entity = line['q_entity']\n",
    "    answer = line['a_entity']\n",
    "    di_graph = build_Digraph(line['graph'])\n",
    "    paths = get_truth_paths(topic_entity, answer, di_graph)\n",
    "    # every question sample at most 3 paths\n",
    "    paths = random.sample(paths, min(3, len(paths)))\n",
    "    sent_idx = 0\n",
    "    for pid,  p in enumerate(paths):\n",
    "        preceding = ''\n",
    "        for pidx, step in enumerate(p):\n",
    "            candidate_relations = []\n",
    "            real_relation = step[1]\n",
    "            real_entity = step[2]\n",
    "            candidate_entities =[tail for head, tail in di_graph.out_edges(step[0]) if di_graph[head][tail].get('relation') == step[1]]\n",
    "            candidate_relation = [page.page_content.strip() for page in retriever.invoke(line['question'] + real_relation + real_entity)]\n",
    "            if step[1] in candidate_relation:\n",
    "                # recall precise\n",
    "                chain_data.append({\"qid\": id,\"sent_idx\": sent_idx, \"query\": line['question'], \"chain_step\": pidx+1, \"candidate_relation\": candidate_relation, \"candidate_entity\": candidate_entities, \"real_relation\": real_relation, \"real_entity\": real_entity,\"paths\": p, \"answer\": answer, \"effective\": True})\n",
    "            else:\n",
    "            #     candidate_relation.append(step[1])\n",
    "                chain_data.append({\"qid\": id,\"sent_idx\": sent_idx, \"query\": line['question'], \"chain_step\": pidx+1, \"candidate_relation\": candidate_relation, \"candidate_entity\": candidate_entities, \"real_relation\": real_relation, \"real_entity\": real_entity,\"paths\": p, \"answer\": answer, \"effective\": False})\n",
    "            preceding += f' {step[1]}'\n",
    "            sent_idx += 1\n",
    "    if len(chain_data) > 100:\n",
    "        print(f'Saving {index}')\n",
    "        save_to_json(chain_data, f'../output/chain_data_top_5.json')\n",
    "        chain_data = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('../output/chain_data_top_5.json', 'r', encoding='utf-8') as file:\n",
    "    chain_data_web = json.load(file)\n",
    "with open('../output/chain_data_cwq_top_5.json', 'r',encoding='utf-8') as file:\n",
    "    chain_data_cwq =json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create Retrieval data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "input_data = []\n",
    "# def construct_relevance(chain_data):\n",
    "ids = []\n",
    "for s, line in enumerate(random.sample(chain_data_cwq, 20000)):\n",
    "    random.seed(s + 1)\n",
    "    rand_num = random.random()\n",
    "    if line['qid'] in ids:\n",
    "        continue\n",
    "    ids.append(line['qid'])\n",
    "    if line['sent_idx'] == 0:\n",
    "        if rand_num < 0.5:\n",
    "            input_data.append({\"input\": {\"query\": line['query'], \"preceding_sentences\": \"\", \"evidence\": \"\", \"target_output\": ';'.join(line['answer'])}, \"decision_token\": \"[New Retrieval]\"})\n",
    "            continue\n",
    "    step = line['chain_step']\n",
    "    if len(line['paths']) == 1:\n",
    "        assert line['chain_step'] == 1\n",
    "        if rand_num < 0.6:\n",
    "            input_data.append({\"input\": {\"query\": line['query'], \"preceding_sentences\": \"\", \"evidence\": f\"Relations retrieved: {line['real_relation']}\\n Entities retrieved: {line['real_entity']}\", \"target_output\": ';'.join(line['answer'])}, \"decision_token\": \"[No Retrieval]\"})\n",
    "    else:\n",
    "        if step == 1:\n",
    "            if rand_num < 0.9:\n",
    "                input_data.append({\"input\": {\"query\": line['query'], \"preceding_sentences\": \"\", \"evidence\": f\"Relations retrieved: {line['real_relation']}\\n Entities retrieved: {line['real_entity']}\", \"target_output\": ';'.join(line['answer'])}, \"decision_token\": \"[Continue to Retrieve Evidence]\"})\n",
    "        elif step < len(line['paths']):\n",
    "            \n",
    "            if rand_num < 0.8:\n",
    "                input_data.append({\"input\": {\"query\": line['query'], \"preceding_sentences\": f\"Information retrieved: {line['paths'][:step]}\", \"evidence\": f\"Relations retrieved: {line['real_relation']}\\n Entities retrieved: {line['real_entity']}\", \"target_output\": ';'.join(line['answer'])}, \"decision_token\": \"[Continue to Retrieve Evidence]\"})\n",
    "        else:\n",
    "            if rand_num < 0.4:\n",
    "                input_data.append({\"input\": {\"query\": line['query'], \"preceding_sentences\": f\"Information retrieved: {line['paths'][:step]}\", \"evidence\": f\"Relations retrieved: {line['real_relation']}\\n Entities retrieved: {line['real_entity']}\", \"target_output\": ';'.join(line['answer'])}, \"decision_token\": \"[No Retrieval]\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# input_data = []\n",
    "\n",
    "for s, line in enumerate(chain_data_web):\n",
    "    random.seed(s + 1)\n",
    "    rand_num = random.random()\n",
    "    if line['sent_idx'] == 0:\n",
    "        if rand_num < 0.5:\n",
    "            input_data.append({\"input\": {\"query\": line['query'], \"preceding_sentences\": \"\", \"evidence\": \"\", \"target_output\": ';'.join(line['answer'])}, \"decision_token\": \"[New Retrieval]\"})\n",
    "            continue\n",
    "    step = line['chain_step']\n",
    "    if len(line['paths']) == 1:\n",
    "        assert line['chain_step'] == 1\n",
    "        if rand_num < 0.6:\n",
    "            input_data.append({\"input\": {\"query\": line['query'], \"preceding_sentences\": \"\", \"evidence\": f\"Relations retrieved: {line['real_relation']}\\n Entities retrieved: {line['real_entity']}\", \"target_output\": ';'.join(line['answer'])}, \"decision_token\": \"[No Retrieval]\"})\n",
    "    else:\n",
    "        if step == 1:\n",
    "            if rand_num < 0.9:\n",
    "                input_data.append({\"input\": {\"query\": line['query'], \"preceding_sentences\": \"\", \"evidence\": f\"Relations retrieved: {line['real_relation']}\\n Entities retrieved: {line['real_entity']}\", \"target_output\": ';'.join(line['answer'])}, \"decision_token\": \"[Continue to Retrieve Evidence]\"})\n",
    "        elif step < len(line['paths']):\n",
    "            if rand_num < 0.8:\n",
    "                input_data.append({\"input\": {\"query\": line['query'], \"preceding_sentences\": f\"Information retrieved: {line['paths'][:step]}\", \"evidence\": f\"Relations retrieved: {line['real_relation']}\\n Entities retrieved: {line['real_entity']}\", \"target_output\": ';'.join(line['answer'])}, \"decision_token\": \"[Continue to Retrieve Evidence]\"})\n",
    "        else:\n",
    "            if rand_num < 0.4:\n",
    "                input_data.append({\"input\": {\"query\": line['query'], \"preceding_sentences\": f\"Information retrieved: {line['paths'][:step]}\", \"evidence\": f\"Relations retrieved: {line['real_relation']}\\n Entities retrieved: {line['real_entity']}\", \"target_output\": ';'.join(line['answer'])}, \"decision_token\": \"[No Retrieval]\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating multi sentence retrieval data\n",
      "{'instruction': 'You will be provided with a query, evidence, output sentence, and preceding sentences (optional). Your task is to determine whether the information in the output sentence can be fully verified by the evidence or if it requires further external verification. There are three cases:\\n- If the output sentence can be verified solely with the evidence and the preceding sentences, then respond with [No Retrieval]. \\n- If additional information about the tail entity in evidence is needed to verify the output sentence, respond with [Continue to Retrieve Evidence] \\n- If more information unrelated to the evidence is needed, e.g. totally new relationship or new entity, reponse with [New Retrieval].\\n\\n', 'input': 'Query: what kind government does the us have\\nEvidence: Relations retrieved: location.country.form_of_government\\n Entities retrieved: Federal republic\\nOutput: Presidential system;Federal republic;Constitutional republic', 'output': '[No Retrieval]', 'task': 'multi_retrieval'}\n",
      "total data number: 12985\n",
      "Counter({'[Continue to Retrieve Evidence]': 5602, '[No Retrieval]': 4384, '[New Retrieval]': 2999})\n"
     ]
    }
   ],
   "source": [
    "retrival_critic = create_retrieval_data(input_data=input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../training_data/relevance_critic.json', 'w')as f:\n",
    "    json.dump(retrival_critic, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create Support groundness data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relationship relevance\n",
    "graph_intepretable =  \"\"\"You will receive a query, evidence and optional preceding sentences containing history information. The evidence contains graph relationships or entities may be useful to answer the query. Your task is to filters out valid information from the evidence to answer the given query, evaluate your output and provide explanations on your result.\n",
    "\n",
    "###\n",
    "Query: Name the president of the country whose main spoken language was Brahui in 1980?\n",
    "Topic Entity: Brahui Language\n",
    "Evidence: language.human_language.main_country; language.human_language.language_family; language.human_language.iso_639_3_code; base.rosetta.languoid.parent; language.human_language.writing_system; base.rosetta.languoid.languoid_class; language.human_language.countries_spoken_in; kg.object_profile.prominent_type; base.rosetta.languoid.document; base.ontologies.ontology_instance.equivalent_instances; base.rosetta.languoid.local_name; language.human_language.region\n",
    "Preceding sentences: \n",
    "Output: \n",
    "1. {{language.human_language.main_country (Score: Fully relavant))}}: This relation is highly relevant as it directly relates to the country whose president is being asked for, and the main country where Brahui language is spoken in 1980.\n",
    "2. {{language.human_language.countries_spoken_in (Score: Fully relavant)}}: This relation is also relevant as it provides information on the countries where Brahui language is spoken, which could help narrow down the search for the president.\n",
    "3. {{base.rosetta.languoid.parent (Score: Partially relevant)}}: This relation is less relevant but still provides some context on the language family to which Brahui belongs, which could be useful in understanding the linguistic and cultural background of the country in question.\n",
    "\n",
    "###\n",
    "Query: {query}\n",
    "Topic Entity: {topic}\n",
    "Evidence: {evidence}\n",
    "Preceding sentences: {preceding_sentences}\n",
    "Output: \n",
    "\"\"\"\n",
    "# The name of Justin Bieber's brother is Jaxon Bieber. This is based on the reasoning path that connects Justin Bieber to Jaxon Bieber through the relationship of sibling. The other paths that connect Justin Bieber to Jazmyn Bieber or back to Justin Bieber himself are incorrect in this context.\n",
    "few_shot_intepretable_prompt = FewShotPromptTemplate(\n",
    "        examples=[{\n",
    "            \"query\": \"Name the president of the country whose main spoken language was Brahui in 1980?\",\n",
    "            \"topic\": \"Brahui Language\",\n",
    "            \"evidence\": \"language.human_language.main_country; language.human_language.language_family; language.human_language.iso_639_3_code; base.rosetta.languoid.parent; language.human_language.writing_system; base.rosetta.languoid.languoid_class; language.human_language.countries_spoken_in; kg.object_profile.prominent_type; base.rosetta.languoid.document; base.ontologies.ontology_instance.equivalent_instances; base.rosetta.languoid.local_name; language.human_language.region\",\n",
    "            \"preceding_sentences\": \"\",\n",
    "            \"output\": \"\"\"1. {{language.human_language.main_country (Score: [Fully Relavant])}}: This relation is highly relevant as it directly relates to the country whose president is being asked for, and the main country where Brahui language is spoken in 1980.\n",
    "2. {{language.human_language.countries_spoken_in (Score: [Fully Relavant])}}: This relation is also relevant as it provides information on the countries where Brahui language is spoken, which could help narrow down the search for the president.\n",
    "3. {{base.rosetta.languoid.parent (Score: [Partially Relevant])}}: This relation is less relevant but still provides some context on the language family to which Brahui belongs, which could be useful in understanding the linguistic and cultural background of the country in question.\"\"\"\n",
    "        }],\n",
    "        example_prompt=PromptTemplate.from_template(\"\"\"###\n",
    "Query: {query}\n",
    "Topic Entity: {topic}\n",
    "Evidence: {evidence}\n",
    "Preceding sentences: {preceding_sentences}\n",
    "Output: {output}\n",
    "\"\"\"),\n",
    "        prefix=\n",
    "        \"\"\"You will receive a query, topic entity, evidence and optional preceding sentences containing history information. The evidence contains graph relationships or entities may be useful to answer the query. Your task is to filters out 3 valid information from the evidence that contribute to answering the query and provide a relevance score for each output, output your explanations for the score.\n",
    "The score of relevance range from [Fully Relavant], [Partially Relevant] to [Unrelevant].\"\"\",\n",
    "        suffix=\n",
    "        \"\"\"###\n",
    "Query: {query}\n",
    "Topic Entity: {topic}\n",
    "Evidence: {evidence}\n",
    "Preceding sentences: {preceding_sentences}\n",
    "Output: \"\"\",\n",
    "        input_variables=[\"query\", \"evidence\", \"preceding_sentences\", \"topic\"],\n",
    ")\n",
    "graph_intepretable_prompt = PromptTemplate(input_variables=[\"query\", \"evidence\", \"preceding_sentences\", \"topic\"], template=\n",
    "graph_intepretable)\n",
    "llm_chain = LLMChain(llm=model, prompt=few_shot_intepretable_prompt, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou will receive a query, topic entity, evidence and optional preceding sentences containing history information. The evidence contains graph relationships or entities may be useful to answer the query. Your task is to filters out 3 valid information from the evidence that contribute to answering the query and provide a relevance score for each output, output your explanations for the score.\n",
      "The score of relevance range from [Fully Relavant], [Partially Relevant] to [Unrelevant].\n",
      "\n",
      "###\n",
      "Query: Name the president of the country whose main spoken language was Brahui in 1980?\n",
      "Topic Entity: Brahui Language\n",
      "Evidence: language.human_language.main_country; language.human_language.language_family; language.human_language.iso_639_3_code; base.rosetta.languoid.parent; language.human_language.writing_system; base.rosetta.languoid.languoid_class; language.human_language.countries_spoken_in; kg.object_profile.prominent_type; base.rosetta.languoid.document; base.ontologies.ontology_instance.equivalent_instances; base.rosetta.languoid.local_name; language.human_language.region\n",
      "Preceding sentences: \n",
      "Output: 1. {language.human_language.main_country (Score: [Fully Relavant])}: This relation is highly relevant as it directly relates to the country whose president is being asked for, and the main country where Brahui language is spoken in 1980.\n",
      "2. {language.human_language.countries_spoken_in (Score: [Fully Relavant])}: This relation is also relevant as it provides information on the countries where Brahui language is spoken, which could help narrow down the search for the president.\n",
      "3. {base.rosetta.languoid.parent (Score: [Partially Relevant])}: This relation is less relevant but still provides some context on the language family to which Brahui belongs, which could be useful in understanding the linguistic and cultural background of the country in question.\n",
      "\n",
      "\n",
      "###\n",
      "Query: when is the last time the the team has a team moscot named Lou Seal won the world series\n",
      "Topic Entity: Lou Seal\n",
      "Evidence: baseball.baseball_league.teams;baseball.lifetime_batting_statistics.last_statistics_season;sports.sports_league.teams;sports.sports_team.team_mascot;baseball.baseball_team_stats.season\n",
      "Preceding sentences: \n",
      "Output: \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'query': 'when is the last time the the team has a team moscot named Lou Seal won the world series',\n",
       "  'evidence': 'baseball.baseball_league.teams;baseball.lifetime_batting_statistics.last_statistics_season;sports.sports_league.teams;sports.sports_team.team_mascot;baseball.baseball_team_stats.season',\n",
       "  'preceding_sentences': '',\n",
       "  'topic': 'Lou Seal',\n",
       "  'text': '1. {sports.sports_team.team_mascot (Score: [Partially Relevant])}: This relation provides information on the team mascot, in this case Lou Seal, which is directly related to the query about the team with that mascot winning the world series. However, it does not specify when the team last won.\\n2. {baseball.baseball_league.teams (Score: [Unrelevant])}: While this relation provides information on baseball teams, it does not specify any details about Lou Seal or when a team with that mascot last won the world series. \\n3. {baseball.baseball_team_stats.season (Score: [Partially Relevant])}: This relation provides information on baseball team statistics by season, which could potentially give insights into when the last world series win occurred. Although not directly answering the query, it could be relevant in finding the answer.'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.batch([{\"query\": \"when is the last time the the team has a team moscot named Lou Seal won the world series\", \"evidence\": \"baseball.baseball_league.teams;baseball.lifetime_batting_statistics.last_statistics_season;sports.sports_league.teams;sports.sports_team.team_mascot;baseball.baseball_team_stats.season\", \"preceding_sentences\": '', 'topic': \"Lou Seal\"}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "def extract_relationship_and_score(s):\n",
    "    match_dict = dict()\n",
    "    pattern = r'{(.+?) \\(Score: (.+?)\\)}'\n",
    "    for match in re.findall(pattern, s):\n",
    "        match_dict[match[0]] = match[1]\n",
    "    return match_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = []\n",
    "batch_input = []\n",
    "random.seed(42)\n",
    "from openai import BadRequestError\n",
    "qids = []\n",
    "for line in random.sample(chain_data, 5000):\n",
    "    if line['qid'] in qids:\n",
    "        continue\n",
    "    qids.append(line['qid'])\n",
    "    batch_input.append({\"query\": line['query'], \"evidence\": ';'.join(line['candidate_relation']), \"preceding_sentences\": '', 'topic': line['paths'][line['chain_step'] - 1][0]})\n",
    "    # try:\n",
    "    #     batch_pred = llm_chain.batch([{\"query\": line['query'], \"evidence\": ';'.join(line['candidate_relation']), \"preceding_sentences\": '', 'topic': line['paths'][line['chain_step'] - 1][0]}], return_only_outputs=True)\n",
    "    # except BadRequestError as e:\n",
    "    #     print('*************************Bad Request**************')\n",
    "    # except ValueError as e:\n",
    "    #     print(f'******************Value Error****************************')\n",
    "    # match_dict = extract_relationship_and_score(batch_pred[0]['text'])\n",
    "    # input_data.append({\"input\": {\"query\": line['query'], \"evidence\": ';'.join(line['candidate_relation']), \"preceding_sentences\": '', 'topic': line['paths'][line['chain_step'] - 1][0]}, \"match\":match_dict, 'real_relation': line['real_relation']})\n",
    "    # if len(input_data) >= 3:\n",
    "    #     print('Saving')\n",
    "    #     save_to_json(input_data, f'../output/relevance_cwq_critic_data_top_5_5000.json')\n",
    "    #     input_data = []\n",
    "# create_relevance_data(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "input_data = []\n",
    "for num_batch in range(1, len(batch_input) // batch_size):\n",
    "    start_index = (num_batch-1) * batch_size\n",
    "    try:\n",
    "        batch_pred = llm_chain.batch(batch_input[start_index: start_index + batch_size], return_only_outputs=True, max_concurrency=8)\n",
    "    except BadRequestError as e:\n",
    "        print('*************************Bad Request**************')\n",
    "    except ValueError as e:\n",
    "        print(f'******************Value Error****************************')\n",
    "    match_dict = [extract_relationship_and_score(batch_pred[i]['text']) for i in range(len(batch_pred))]\n",
    "    assert len(batch_pred) == batch_size\n",
    "    for i in range(batch_size):\n",
    "        line = batch_input[start_index + i]\n",
    "        input_data.append({\"input\": line, \"match\":match_dict[i]})\n",
    "    print(f'Saving batch{num_batch}')\n",
    "    save_to_json(input_data, f'../output/relevance_cwq_critic_data_top_5_5000.json')\n",
    "    input_data = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../output/relevance_critic_data_top_5_5000.json', 'r') as f:\n",
    "    data_1 = json.load(f)\n",
    "with open('../output/relevance_cwq_critic_data_top_5_5000.json', 'r') as f:\n",
    "    data_2 = json.load(f) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating relevance data\n",
      "{'instruction': 'You will receive a query, topic entity, evidence and optional preceding sentences containing history information. The evidence contains graph relationships or entities may be useful to answer the query. Your task is to filters out 3 valid information from the evidence that contribute to answering the query and provide a relevance score for each output.\\nThe score of relevance range from [Fully relavant], [Partially relevant] to [Unrelevant].', 'input': 'Query: Which state has the official symbol of Charter Oak and is where Glastonbury is located?\\nTopic entity: Glastonbury\\nEvidence: government.governmental_jurisdiction.official_symbols;location.us_state.capital;location.country.national_anthem;location.country.form_of_government;location.location_symbol_relationship.symbolPreceding sentences: ', 'output': 'government.governmental_jurisdiction.official_symbols[Fully Relevant]; location.us_state.capital[Fully Relevant]; location.location_symbol_relationship.symbol[Partially Relevant]; ', 'task': 'relevance'}\n",
      "total data number: 6500\n"
     ]
    }
   ],
   "source": [
    "relevance_data = create_relevance_data(data_1 + data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2200\n"
     ]
    }
   ],
   "source": [
    "with open('../output/relevance_critic_data_top_5_5000.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "    print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create triplet relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The name of Justin Bieber's brother is Jaxon Bieber. This is based on the reasoning path that connects Justin Bieber to Jaxon Bieber through the relationship of sibling. The other paths that connect Justin Bieber to Jazmyn Bieber or back to Justin Bieber himself are incorrect in this context.\n",
    "few_shot_entity_prompt = FewShotPromptTemplate(\n",
    "        examples=[{\n",
    "            \"query\": \"Name the president of the country whose main spoken language was Brahui in 1980\",\n",
    "            \"evidence\": \"(Unknown-Entity, people.place_lived.location, De Smet)\",\n",
    "            \"preceding_sentences\": \"(The Long Winter, book.written_work.author, Laura Ingalls Wilder);(Laura Ingalls Wilder, people.person.places_lived, Unknown-Entity);\",\n",
    "            \"output\": \"\"\"1. {{De Smet (Score: [Fully Relavant])}}. The De Smet is fully relevant to the query as the triplet directly provide the answer to the query.\"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"query\": \"what is the name of justin bieber brother\",\n",
    "            \"evidence\": \"(Justin Bieber, people.person.parents, Jeremy Bieber);(Justin Bieber, people.person.parents, Pattie Mallette)\",\n",
    "            \"preceding_sentences\": \"\",\n",
    "            \"output\": \"\"\"1. {{Jeremy Bieber (Score: [Partially Relavant])}}. Jeremy Bieber is rated as partially relevant because he is a potential candidate as Justin Bieber's brother based on the evidence provided.\n",
    "2. {{Pattie Mallette (Score: [Unrelavant])}}. Pattie Mallette is rated as unrelevant as she is not the brother of Justin Bieber.\"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"query\": \"who did draco malloy end up marrying\",\n",
    "            \"evidence\": \"(m.09k254w, fictional_universe.marriage_of_fictional_characters.spouses, Astoria Greengrass);(m.09k254w, fictional_universe.marriage_of_fictional_characters.spouses, Draco Malfoy)\",\n",
    "            \"preceding_sentences\": \"(Draco Malfoy', 'fictional_universe.fictional_character.married_to', 'm.09k254w)\",\n",
    "            \"output\": \"\"\"1. {{Astoria Greengrass (Score: [Fully Relavant])}}. Astoria Greengrass is fully relevant as she is identified as the character who Draco Malfoy ended up marrying based on the evidence provided.\n",
    "2. {{Draco Malfoy (Score: [Unrelavant])}}.Draco Malfoy is rated as partially relevant because while he is mentioned in the evidence, the focus of the query is on who he married, not himself.\"\"\"\n",
    "        }],\n",
    "        example_prompt=PromptTemplate.from_template(\"\"\"###\n",
    "Query: {query}\n",
    "Evidence: {evidence}\n",
    "Preceding sentences: {preceding_sentences}\n",
    "Output: {output}\n",
    "\"\"\"),\n",
    "        prefix=\n",
    "        \"\"\"You will receive a query, evidence and optional preceding sentences containing history information. The evidence contains the associated retrieved knowledge graph triplets (head_entity, relation, tail_entity). Your task is to rate relevance score for each tail_entity from the evidence and output your explanations for the score.\n",
    "The score of relevance range from [Fully Relavant], [Partially Relevant] to [Unrelevant].\"\"\",\n",
    "        suffix=\n",
    "        \"\"\"###\n",
    "Query: {query}\n",
    "Evidence: {evidence}\n",
    "Preceding sentences: {preceding_sentences}\n",
    "Output: \"\"\",\n",
    "        input_variables=[\"query\", \"evidence\", \"preceding_sentences\", ],\n",
    ")\n",
    "\n",
    "llm_chain = LLMChain(llm=model, prompt=few_shot_entity_prompt, verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "dump() missing 1 required positional argument: 'fp'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../output/relevance_entity_webqsp.json\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: dump() missing 1 required positional argument: 'fp'"
     ]
    }
   ],
   "source": [
    "with open(f'../output/relevance_entity_webqsp.json', 'w') as f:\n",
    "    json.dump(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in relevance_entity:\n",
    "    r['query'] = evidence_match[r['qid'] + '-' + str(r['sent_idx'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['(Justin Bieber, people.person.parents, Pattie Mallette)',\n",
       " '(Justin Bieber, people.person.parents, Jeremy Bieber)']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evidence_match[relevance_entity[0]['qid'] + '-' + str(relevance_entity[0]['sent_idx'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "evidence_match = dict({})\n",
    "for line in chain_data_web:\n",
    "    if line['real_entity'].startswith('m.'):\n",
    "        continue\n",
    "    # evidence = ['({head}, {relation}, {tail})'.format_map({\"head\": line['paths'][line['chain_step'] - 1][0], \"relation\": line['paths'][line['chain_step'] - 1][1], \"tail\": tail}) for tail in line['candidate_entity']]\n",
    "    evidence_match[line['qid'] + '-' + str(line['sent_idx'])] = line['query']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving 15\n",
      "Saving 30\n",
      "Saving 41\n",
      "Saving 54\n",
      "******************Value Error****************************\n",
      "Saving 69\n",
      "******************Value Error****************************\n",
      "Saving 80\n",
      "Saving 91\n",
      "Saving 104\n",
      "Saving 117\n",
      "Saving 131\n",
      "Saving 147\n",
      "Saving 159\n",
      "Saving 170\n",
      "Saving 183\n",
      "Saving 198\n",
      "Saving 210\n",
      "Saving 227\n",
      "Saving 245\n",
      "Saving 260\n",
      "Saving 275\n",
      "Saving 287\n",
      "Saving 303\n",
      "Saving 315\n",
      "Saving 326\n",
      "Saving 345\n",
      "Saving 357\n",
      "Saving 367\n",
      "Saving 384\n",
      "Saving 396\n",
      "Saving 408\n",
      "Saving 425\n",
      "Saving 435\n",
      "Saving 453\n",
      "Saving 467\n",
      "Saving 477\n",
      "Saving 489\n",
      "Saving 499\n",
      "Saving 509\n",
      "Saving 519\n",
      "Saving 530\n",
      "Saving 540\n",
      "Saving 553\n",
      "Saving 563\n",
      "Saving 578\n",
      "Saving 589\n",
      "Saving 604\n",
      "Saving 615\n",
      "Saving 625\n",
      "Saving 637\n",
      "Saving 647\n",
      "Saving 658\n",
      "Saving 673\n",
      "Saving 686\n",
      "Saving 702\n",
      "Saving 718\n",
      "Saving 730\n",
      "Saving 746\n",
      "Saving 757\n",
      "Saving 769\n",
      "Saving 782\n",
      "Saving 793\n",
      "Saving 803\n",
      "Saving 813\n",
      "Saving 824\n",
      "Saving 835\n",
      "Saving 846\n",
      "Saving 861\n",
      "Saving 874\n",
      "Saving 884\n",
      "Saving 897\n",
      "Saving 910\n",
      "Saving 920\n",
      "Saving 930\n",
      "Saving 940\n",
      "Saving 950\n",
      "Saving 962\n",
      "Saving 976\n",
      "Saving 990\n",
      "Saving 1000\n",
      "Saving 1014\n",
      "Saving 1027\n",
      "Saving 1038\n",
      "Saving 1053\n",
      "Saving 1064\n",
      "Saving 1077\n",
      "Saving 1088\n",
      "Saving 1101\n",
      "Saving 1111\n",
      "Saving 1127\n",
      "******************Value Error****************************\n",
      "Saving 1142\n",
      "Saving 1153\n",
      "Saving 1166\n",
      "Saving 1180\n",
      "Saving 1192\n",
      "Saving 1206\n",
      "Saving 1216\n",
      "Saving 1229\n",
      "Saving 1243\n",
      "Saving 1253\n",
      "Saving 1263\n",
      "Saving 1275\n",
      "Saving 1289\n",
      "Saving 1301\n",
      "Saving 1314\n",
      "Saving 1325\n",
      "Saving 1339\n",
      "Saving 1351\n",
      "Saving 1364\n",
      "Saving 1378\n",
      "Saving 1394\n",
      "Saving 1404\n",
      "Saving 1418\n",
      "Saving 1432\n",
      "Saving 1445\n",
      "Saving 1456\n",
      "Saving 1468\n",
      "Saving 1481\n",
      "Saving 1496\n",
      "Saving 1510\n",
      "Saving 1521\n",
      "Saving 1532\n",
      "Saving 1546\n",
      "Saving 1556\n",
      "Saving 1572\n",
      "Saving 1583\n",
      "Saving 1599\n",
      "Saving 1615\n",
      "Saving 1629\n",
      "Saving 1645\n",
      "Saving 1655\n",
      "Saving 1668\n",
      "Saving 1680\n",
      "Saving 1694\n",
      "Saving 1709\n",
      "Saving 1719\n",
      "Saving 1729\n",
      "Saving 1744\n",
      "Saving 1757\n",
      "Saving 1773\n",
      "Saving 1786\n",
      "Saving 1796\n",
      "Saving 1810\n",
      "Saving 1822\n",
      "Saving 1835\n",
      "Saving 1845\n",
      "Saving 1855\n",
      "Saving 1871\n",
      "Saving 1884\n",
      "Saving 1898\n",
      "Saving 1909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function tqdm.__del__ at 0x7f5230167b50>\n",
      "Traceback (most recent call last):\n",
      "  File \"/media/disk1/chatgpt/miniconda3/envs/raptor/lib/python3.10/site-packages/tqdm/std.py\", line 1148, in __del__\n",
      "    self.close()\n",
      "  File \"/media/disk1/chatgpt/miniconda3/envs/raptor/lib/python3.10/site-packages/tqdm/notebook.py\", line 279, in close\n",
      "    self.disp(bar_style='danger', check_delay=False)\n",
      "AttributeError: 'tqdm_notebook' object has no attribute 'disp'\n",
      "Exception ignored in: <function tqdm.__del__ at 0x7f5230167b50>\n",
      "Traceback (most recent call last):\n",
      "  File \"/media/disk1/chatgpt/miniconda3/envs/raptor/lib/python3.10/site-packages/tqdm/std.py\", line 1148, in __del__\n",
      "    self.close()\n",
      "  File \"/media/disk1/chatgpt/miniconda3/envs/raptor/lib/python3.10/site-packages/tqdm/notebook.py\", line 279, in close\n",
      "    self.disp(bar_style='danger', check_delay=False)\n",
      "AttributeError: 'tqdm_notebook' object has no attribute 'disp'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving 1922\n",
      "Saving 1934\n",
      "Saving 1947\n",
      "Saving 1960\n",
      "Saving 1974\n",
      "Saving 1988\n",
      "Saving 1998\n",
      "Saving 2009\n",
      "Saving 2022\n",
      "Saving 2032\n",
      "Saving 2045\n",
      "Saving 2056\n",
      "Saving 2071\n",
      "Saving 2082\n",
      "Saving 2092\n",
      "Saving 2110\n",
      "Saving 2123\n",
      "Saving 2133\n",
      "Saving 2144\n",
      "Saving 2156\n",
      "Saving 2173\n",
      "Saving 2185\n",
      "Saving 2197\n",
      "Saving 2209\n",
      "Saving 2223\n",
      "Saving 2236\n",
      "Saving 2247\n",
      "Saving 2258\n",
      "Saving 2273\n",
      "Saving 2287\n",
      "Saving 2300\n",
      "Saving 2313\n",
      "Saving 2326\n",
      "Saving 2336\n",
      "Saving 2349\n",
      "Saving 2361\n",
      "Saving 2372\n",
      "Saving 2383\n",
      "Saving 2400\n",
      "Saving 2413\n",
      "Saving 2423\n",
      "Saving 2434\n",
      "Saving 2445\n",
      "Saving 2461\n",
      "Saving 2472\n",
      "Saving 2486\n",
      "Saving 2498\n",
      "Saving 2510\n",
      "Saving 2521\n",
      "Saving 2531\n",
      "Saving 2542\n",
      "Saving 2556\n",
      "Saving 2568\n",
      "Saving 2582\n",
      "Saving 2597\n",
      "Saving 2613\n",
      "Saving 2625\n",
      "Saving 2639\n",
      "Saving 2652\n",
      "Saving 2665\n",
      "Saving 2679\n",
      "Saving 2692\n",
      "Saving 2705\n",
      "Saving 2718\n",
      "Saving 2731\n",
      "Saving 2747\n",
      "Saving 2760\n",
      "Saving 2776\n",
      "Saving 2786\n",
      "Saving 2799\n",
      "Saving 2816\n",
      "Saving 2829\n",
      "******************Value Error****************************\n",
      "******************Value Error****************************\n",
      "Saving 2843\n",
      "Saving 2860\n",
      "Saving 2875\n",
      "Saving 2885\n",
      "Saving 2900\n",
      "Saving 2916\n",
      "Saving 2926\n",
      "Saving 2939\n",
      "Saving 2952\n",
      "Saving 2963\n",
      "Saving 2978\n",
      "Saving 2990\n",
      "Saving 3004\n",
      "Saving 3016\n",
      "Saving 3030\n",
      "Saving 3047\n",
      "Saving 3063\n",
      "Saving 3075\n",
      "Saving 3091\n",
      "Saving 3108\n",
      "Saving 3120\n",
      "Saving 3132\n",
      "Saving 3144\n",
      "Saving 3156\n",
      "Saving 3169\n",
      "Saving 3182\n",
      "Saving 3194\n",
      "Saving 3204\n",
      "Saving 3219\n",
      "Saving 3235\n",
      "Saving 3248\n",
      "Saving 3260\n",
      "Saving 3277\n",
      "Saving 3288\n",
      "Saving 3303\n",
      "Saving 3314\n",
      "Saving 3324\n",
      "Saving 3338\n",
      "Saving 3354\n",
      "Saving 3364\n",
      "Saving 3374\n",
      "Saving 3390\n",
      "Saving 3403\n",
      "Saving 3413\n",
      "Saving 3429\n",
      "Saving 3442\n",
      "Saving 3455\n",
      "Saving 3469\n",
      "Saving 3484\n",
      "Saving 3495\n",
      "Saving 3510\n",
      "Saving 3521\n",
      "Saving 3538\n",
      "Saving 3553\n",
      "Saving 3566\n",
      "Saving 3578\n",
      "Saving 3592\n",
      "Saving 3603\n",
      "Saving 3620\n",
      "Saving 3636\n",
      "Saving 3646\n",
      "Saving 3658\n",
      "Saving 3675\n",
      "Saving 3686\n",
      "Saving 3696\n",
      "Saving 3706\n",
      "Saving 3718\n",
      "Saving 3732\n",
      "Saving 3746\n",
      "Saving 3759\n",
      "Saving 3770\n",
      "Saving 3782\n",
      "Saving 3799\n",
      "Saving 3811\n",
      "Saving 3825\n",
      "Saving 3837\n",
      "Saving 3849\n",
      "Saving 3861\n",
      "Saving 3872\n",
      "Saving 3883\n",
      "Saving 3899\n",
      "Saving 3911\n",
      "Saving 3923\n",
      "Saving 3942\n",
      "Saving 3953\n",
      "Saving 3965\n",
      "Saving 3979\n",
      "Saving 3990\n",
      "Saving 4004\n",
      "Saving 4018\n",
      "Saving 4031\n",
      "Saving 4041\n",
      "Saving 4052\n",
      "Saving 4067\n",
      "Saving 4081\n",
      "Saving 4092\n",
      "Saving 4105\n",
      "Saving 4120\n",
      "Saving 4133\n",
      "Saving 4149\n",
      "Saving 4164\n",
      "Saving 4175\n",
      "Saving 4193\n",
      "Saving 4203\n",
      "Saving 4214\n",
      "Saving 4224\n",
      "Saving 4238\n",
      "Saving 4248\n",
      "Saving 4260\n",
      "Saving 4271\n",
      "Saving 4283\n",
      "Saving 4298\n",
      "Saving 4311\n",
      "Saving 4322\n",
      "Saving 4334\n",
      "Saving 4347\n",
      "Saving 4362\n",
      "Saving 4374\n",
      "Saving 4386\n",
      "Saving 4399\n",
      "Saving 4413\n",
      "Saving 4429\n",
      "Saving 4442\n",
      "Saving 4453\n",
      "Saving 4468\n",
      "Saving 4479\n",
      "Saving 4489\n",
      "Saving 4499\n",
      "Saving 4515\n",
      "Saving 4528\n",
      "Saving 4539\n",
      "Saving 4555\n",
      "Saving 4568\n",
      "Saving 4584\n",
      "Saving 4600\n",
      "Saving 4613\n",
      "Saving 4626\n",
      "Saving 4640\n",
      "Saving 4651\n",
      "Saving 4667\n",
      "Saving 4677\n",
      "Saving 4690\n",
      "Saving 4703\n",
      "Saving 4715\n",
      "Saving 4728\n",
      "Saving 4739\n",
      "Saving 4751\n",
      "Saving 4767\n",
      "Saving 4783\n",
      "Saving 4797\n",
      "Saving 4809\n",
      "Saving 4824\n",
      "Saving 4835\n",
      "Saving 4848\n",
      "Saving 4859\n",
      "Saving 4874\n",
      "Saving 4885\n",
      "Saving 4897\n",
      "Saving 4909\n",
      "Saving 4920\n",
      "Saving 4932\n",
      "Saving 4945\n",
      "Saving 4958\n",
      "Saving 4968\n",
      "Saving 4979\n",
      "Saving 4993\n",
      "Saving 5008\n",
      "Saving 5023\n",
      "Saving 5037\n",
      "Saving 5054\n",
      "Saving 5067\n",
      "Saving 5081\n",
      "Saving 5091\n",
      "Saving 5107\n",
      "Saving 5118\n",
      "Saving 5129\n",
      "Saving 5145\n",
      "Saving 5160\n",
      "Saving 5173\n",
      "Saving 5188\n",
      "Saving 5203\n",
      "Saving 5214\n",
      "Saving 5225\n",
      "Saving 5238\n",
      "Saving 5250\n",
      "Saving 5262\n",
      "Saving 5276\n",
      "Saving 5289\n",
      "Saving 5299\n",
      "Saving 5310\n",
      "Saving 5324\n",
      "Saving 5336\n",
      "Saving 5346\n",
      "Saving 5359\n",
      "Saving 5375\n",
      "Saving 5389\n",
      "Saving 5403\n",
      "Saving 5418\n",
      "Saving 5432\n",
      "Saving 5445\n",
      "Saving 5456\n",
      "Saving 5471\n",
      "Saving 5484\n",
      "Saving 5497\n",
      "Saving 5510\n",
      "Saving 5521\n",
      "Saving 5534\n",
      "Saving 5545\n",
      "Saving 5556\n",
      "Saving 5567\n",
      "Saving 5581\n",
      "******************Value Error****************************\n",
      "Saving 5593\n",
      "Saving 5604\n",
      "Saving 5620\n",
      "Saving 5636\n",
      "Saving 5649\n",
      "Saving 5661\n",
      "Saving 5673\n",
      "Saving 5687\n",
      "Saving 5703\n",
      "Saving 5720\n",
      "Saving 5734\n",
      "Saving 5744\n",
      "Saving 5758\n",
      "Saving 5772\n",
      "Saving 5787\n",
      "Saving 5801\n",
      "Saving 5815\n",
      "Saving 5827\n",
      "Saving 5843\n",
      "Saving 5858\n",
      "Saving 5868\n",
      "Saving 5878\n",
      "Saving 5890\n",
      "Saving 5906\n",
      "Saving 5920\n",
      "Saving 5935\n",
      "Saving 5948\n",
      "Saving 5959\n",
      "Saving 5972\n",
      "Saving 5984\n",
      "Saving 6000\n",
      "Saving 6013\n",
      "Saving 6026\n",
      "Saving 6042\n",
      "******************Value Error****************************\n",
      "Saving 6056\n",
      "Saving 6072\n",
      "Saving 6086\n",
      "Saving 6096\n",
      "Saving 6107\n",
      "Saving 6119\n",
      "Saving 6135\n",
      "Saving 6145\n",
      "Saving 6157\n",
      "Saving 6171\n",
      "Saving 6187\n",
      "Saving 6201\n",
      "Saving 6215\n",
      "Saving 6229\n",
      "Saving 6243\n",
      "Saving 6256\n",
      "Saving 6268\n",
      "Saving 6280\n",
      "Saving 6297\n",
      "Saving 6310\n",
      "Saving 6322\n",
      "Saving 6337\n",
      "Saving 6348\n",
      "Saving 6363\n",
      "Saving 6381\n",
      "Saving 6391\n",
      "Saving 6401\n",
      "Saving 6417\n",
      "Saving 6431\n",
      "Saving 6446\n",
      "Saving 6457\n",
      "Saving 6467\n",
      "Saving 6478\n",
      "Saving 6493\n",
      "Saving 6504\n",
      "Saving 6515\n",
      "Saving 6528\n",
      "Saving 6540\n",
      "Saving 6551\n",
      "Saving 6561\n",
      "Saving 6571\n",
      "Saving 6586\n",
      "Saving 6600\n",
      "Saving 6615\n",
      "Saving 6626\n",
      "Saving 6639\n",
      "Saving 6649\n",
      "Saving 6661\n",
      "Saving 6674\n",
      "Saving 6687\n",
      "Saving 6701\n",
      "Saving 6716\n",
      "Saving 6727\n",
      "Saving 6741\n",
      "Saving 6754\n",
      "Saving 6765\n",
      "Saving 6776\n",
      "Saving 6789\n",
      "Saving 6801\n",
      "Saving 6816\n",
      "Saving 6830\n",
      "Saving 6840\n",
      "Saving 6850\n",
      "Saving 6861\n",
      "Saving 6872\n",
      "Saving 6887\n",
      "Saving 6899\n",
      "Saving 6910\n",
      "Saving 6920\n",
      "Saving 6931\n",
      "Saving 6941\n",
      "Saving 6952\n",
      "Saving 6968\n",
      "Saving 6983\n",
      "Saving 6993\n",
      "******************Value Error****************************\n",
      "Saving 7006\n",
      "Saving 7017\n",
      "******************Value Error****************************\n",
      "Saving 7028\n",
      "Saving 7041\n",
      "Saving 7052\n",
      "Saving 7067\n",
      "Saving 7083\n",
      "Saving 7094\n",
      "Saving 7104\n",
      "Saving 7116\n",
      "Saving 7126\n",
      "Saving 7140\n",
      "Saving 7153\n",
      "Saving 7167\n",
      "Saving 7182\n",
      "Saving 7198\n",
      "Saving 7208\n",
      "Saving 7218\n",
      "Saving 7228\n",
      "Saving 7238\n",
      "Saving 7254\n",
      "Saving 7270\n",
      "Saving 7283\n",
      "Saving 7293\n",
      "Saving 7308\n",
      "Saving 7322\n",
      "Saving 7340\n",
      "Saving 7352\n",
      "Saving 7362\n",
      "Saving 7374\n",
      "Saving 7384\n",
      "Saving 7394\n",
      "Saving 7406\n",
      "Saving 7420\n",
      "Saving 7436\n",
      "Saving 7446\n",
      "Saving 7461\n",
      "Saving 7472\n",
      "Saving 7483\n",
      "Saving 7496\n",
      "Saving 7509\n",
      "Saving 7520\n",
      "Saving 7533\n",
      "Saving 7545\n",
      "Saving 7557\n",
      "Saving 7568\n",
      "Saving 7579\n",
      "Saving 7589\n",
      "Saving 7601\n",
      "Saving 7614\n",
      "Saving 7624\n",
      "Saving 7635\n",
      "Saving 7646\n",
      "Saving 7659\n",
      "Saving 7671\n",
      "Saving 7687\n",
      "Saving 7703\n",
      "Saving 7717\n",
      "Saving 7728\n",
      "Saving 7739\n",
      "Saving 7755\n",
      "Saving 7766\n",
      "Saving 7779\n",
      "Saving 7791\n",
      "Saving 7806\n",
      "Saving 7821\n",
      "Saving 7832\n",
      "Saving 7847\n",
      "Saving 7860\n"
     ]
    }
   ],
   "source": [
    "input_data = []\n",
    "random.seed(42)\n",
    "#TODO:修改match匹配形式,修改preceding_sentences, candidate_entity个数过多规则问题\n",
    "from openai import BadRequestError\n",
    "qids = []\n",
    "for index, line in enumerate(chain_data):\n",
    "    # if line['qid'] in qids:\n",
    "    #     continue\n",
    "    if line['real_entity'].startswith('m.'):\n",
    "        continue\n",
    "    qids.append(line['qid'])\n",
    "    evidence = ['({head}, {relation}, {tail})'.format_map({\"head\": line['paths'][line['chain_step'] - 1][0], \"relation\": line['paths'][line['chain_step'] - 1][1], \"tail\": tail}) for tail in line['candidate_entity']]\n",
    "    try:\n",
    "        batch_pred = llm_chain.batch([{\"query\": line['query'], \"evidence\": ';'.join(evidence), \"preceding_sentences\": ''}], return_only_outputs=True)\n",
    "    except BadRequestError as e:\n",
    "        print('*************************Bad Request**************')\n",
    "    except ValueError as e:\n",
    "        print(f'******************Value Error****************************')\n",
    "    match_dict = extract_relationship_and_score(batch_pred[0]['text'])\n",
    "    input_data.append({\"qid\":  line['qid'], \"sent_idx\": line['sent_idx'] ,\"match\":match_dict, 'output': batch_pred[0]['text'], \"evidence\": ';'.join(evidence)})\n",
    "    if len(input_data) >= 10:\n",
    "        print(f'Saving { index}')\n",
    "        save_to_json(input_data, f'../output/relevance_entity_webqsp.json')\n",
    "        input_data = []\n",
    "# create_relevance_data(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6110\n"
     ]
    }
   ],
   "source": [
    "with open(f'../output/relevance_entity_webqsp.json', 'r') as f:\n",
    "    relevance_entity = json.load(f)\n",
    "    print(len(relevance_entity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create confidence data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "few_shot_intepretable_prompt = FewShotPromptTemplate(\n",
    "        examples=[{\n",
    "            \"query\": \"what is the name of justin bieber brother?\",\n",
    "            \"output\": \"[Retreive New Relationship]<paragraph>people.sibling_relationship.sibling;fictional_universe.fictional_character.siblings;fictional_universe.sibling_relationship_of_fictional_characters.siblings;people.person.sibling_s;people.family.members;people.person.parents</paragraph>Retrieved relationship: people.person.parents[Fully Relevant][Retrieve Entity]<paragraph>(Justin Bieber, people.person.parents, Pattie Mallette);(Justin Bieber, people.person.parents, Jeremy Bieber);(Justin Bieber, people.person.parents, Jeremy Bieber)</paragraph>Retrieved triplet: (Justin Bieber, people.person.parents, Jeremy Bieber)[Fully Relevant][Continue to Retrieve Evidence]<paragraph>people.sibling_relationship.sibling;people.person.sibling_s;people.person.parents;fictional_universe.fictional_character.siblings;people.family.members;people.person.children</paragraph>Retrieved relationship: people.person.children[Fully Relevant][Retrieve Entity]<paragraph>(Jeremy Bieber, people.person.children, Jazmyn Bieber);(Jeremy Bieber, people.person.children, Justin Bieber);(Jeremy Bieber, people.person.children, Jaxon Bieber);(Jeremy Bieber, people.person.children, Jaxon Bieber)</paragraph>Retrieved triplet: (Jeremy Bieber, people.person.children, Jaxon Bieber)[Fully Relevant][No Retrieval] Answer: Jaxon Bieber\",\n",
    "            \"explanation\": \"The output provides the name of justin bieber's brother. This is based on the reasoning path that connects Justin Bieber to Jaxon Bieber through the relationship of sibling. The other paths that connect Justin Bieber to Jazmyn Bieber or back to Justin Bieber himself are incorrect in this context.\",\n",
    "            \"rating\": \"[Confidence:5]\"\n",
    "        }],\n",
    "        example_prompt=PromptTemplate.from_template(\"\"\"\n",
    "Query: {query}\\n\\n",
    "Output: {output}\n",
    "Explanation: {explanation}\n",
    "Rating: {rating}\n",
    "\"\"\"),\n",
    "        prefix=\"\"\"Given a query and an output, rate whether the response and the thought process appears to be a helpful and informative answer to the query, from 1 (lowest) - 5 (highest). We call this confidence score.\n",
    "[Confidence:5]: The response provides a complete and correct reasoning chain to the query, and the final answer is complete and logically correct.\n",
    "[Confidence:4]: The response mostly fulfills the need in the query and provides correct answers, while there can be some minor improvements such as shorter reasoning chain or less repetition.\n",
    "[Confidence:3]: The response is acceptable, but the answers may be not complete or needs minor improvement.\n",
    "[Confidence:2]: The reasoning process still addresses the main request, but the answers are not correct or not relevant to the query.\n",
    "[Confidence:1]: The reasoning is barely irrelevant or does not give an answer in the end.\"\"\",\n",
    "        suffix=\n",
    "        \"\"\"\n",
    "Query: {query}\\n\\n",
    "Output: {output}\\n\"\"\",\n",
    "        input_variables=[\"query\", \"output\"],\n",
    ")\n",
    "# confidence_prompt = PromptTemplate(input_variables=[\"query\", \"output\"], template=\n",
    "# graph_intepretable)\n",
    "llm_chain = LLMChain(llm=model, prompt=few_shot_intepretable_prompt, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven a query and an output, rate whether the response and the thought process appears to be a helpful and informative answer to the query, from 1 (lowest) - 5 (highest). We call this confidence score.\n",
      "[Confidence:5]: The response provides a complete and correct reasoning chain to the query, and the final answer is complete and logically correct.\n",
      "[Confidence:4]: The response mostly fulfills the need in the query and provides correct answers, while there can be some minor improvements such as shorter reasoning chain or less repetition.\n",
      "[Confidence:3]: The response is acceptable, but the answers may be not complete or needs minor improvement.\n",
      "[Confidence:2]: The reasoning process still addresses the main request, but the answers are not correct or not relevant to the query.\n",
      "[Confidence:1]: The reasoning is barely irrelevant or does not give an answer in the end.\n",
      "\n",
      "\n",
      "Query: what is the name of justin bieber brother?\n",
      "\n",
      "Output: [Retreive New Relationship]<paragraph>people.sibling_relationship.sibling;fictional_universe.fictional_character.siblings;fictional_universe.sibling_relationship_of_fictional_characters.siblings;people.person.sibling_s;people.family.members;people.person.parents</paragraph>Retrieved relationship: people.person.parents[Fully Relevant][Retrieve Entity]<paragraph>(Justin Bieber, people.person.parents, Pattie Mallette);(Justin Bieber, people.person.parents, Jeremy Bieber);(Justin Bieber, people.person.parents, Jeremy Bieber)</paragraph>Retrieved triplet: (Justin Bieber, people.person.parents, Jeremy Bieber)[Fully Relevant][Continue to Retrieve Evidence]<paragraph>people.sibling_relationship.sibling;people.person.sibling_s;people.person.parents;fictional_universe.fictional_character.siblings;people.family.members;people.person.children</paragraph>Retrieved relationship: people.person.children[Fully Relevant][Retrieve Entity]<paragraph>(Jeremy Bieber, people.person.children, Jazmyn Bieber);(Jeremy Bieber, people.person.children, Justin Bieber);(Jeremy Bieber, people.person.children, Jaxon Bieber);(Jeremy Bieber, people.person.children, Jaxon Bieber)</paragraph>Retrieved triplet: (Jeremy Bieber, people.person.children, Jaxon Bieber)[Fully Relevant][No Retrieval] Answer: Jaxon Bieber\n",
      "Explanation: The output provides the name of justin bieber's brother. This is based on the reasoning path that connects Justin Bieber to Jaxon Bieber through the relationship of sibling. The other paths that connect Justin Bieber to Jazmyn Bieber or back to Justin Bieber himself are incorrect in this context.\n",
      "Rating: [Confidence:5]\n",
      "\n",
      "\n",
      "\n",
      "Query: what is the money called in peru\n",
      "\n",
      "Output: [Retreive New Relationship]<paragraph>base.coinsdaily.denomination.money_value;finance.currency.currency_code;measurement_unit.money_value.amount;location.country.currency_used;location.country.currency_formerly_used</paragraph>Retrieved relationship: location.country.currency_used[Fully Relevant][Retrieve Entity] Justin\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'what is the money called in peru',\n",
       " 'output': '[Retreive New Relationship]<paragraph>base.coinsdaily.denomination.money_value;finance.currency.currency_code;measurement_unit.money_value.amount;location.country.currency_used;location.country.currency_formerly_used</paragraph>Retrieved relationship: location.country.currency_used[Fully Relevant][Retrieve Entity] Justin',\n",
       " 'text': 'Rating: [Confidence:2]\\nExplanation: The response does not provide the name of the currency in Peru, which is the main request of the query. Therefore, the reasoning process is not relevant to the query.'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.invoke({'query': 'what is the money called in peru',\"output\": '[Retreive New Relationship]<paragraph>base.coinsdaily.denomination.money_value;finance.currency.currency_code;measurement_unit.money_value.amount;location.country.currency_used;location.country.currency_formerly_used</paragraph>Retrieved relationship: location.country.currency_used[Fully Relevant][Retrieve Entity] Justin' })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/WebQSP.train.json\", \"r\") as f:\n",
    "    train_webqsp = json.load(f)\n",
    "id2chain = dict()\n",
    "for line in train_webqsp['Questions']:\n",
    "    unique_chain = []\n",
    "    for p in line['Parses']:\n",
    "        if p['InferentialChain'] not in unique_chain:\n",
    "            unique_chain.append(p['InferentialChain'])\n",
    "    id2chain[line['QuestionId']] = unique_chain\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = []\n",
    "#添加constraint\n",
    "line = relation_data_train[1]\n",
    "topic_entity = line['q_entity']\n",
    "answer = line['a_entity']\n",
    "di_graph = build_Digraph(line['graph'])\n",
    "for chain in id2chain[line['id']]:\n",
    "    paths = bfs_with_rule(di_graph, topic_entity[0], chain)\n",
    "    # for p in paths:\n",
    "        # if p[-1][-1] == answer[0]:\n",
    "    input_data.append({\"query\": line['question'], \"path\": paths})\n",
    "batch_pred = llm_chain.batch(input_data, return_only_outputs=True)\n",
    "\n",
    "                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = []\n",
    "output = []\n",
    "#添加constraint\n",
    "from tqdm.notebook import tqdm\n",
    "from openai import BadRequestError\n",
    "ind = 1\n",
    "with tqdm(total=len(relation_data_train), desc=f\"Processing\",ncols=1500) as pbar:\n",
    "    while ind < len(relation_data_train):\n",
    "        line = relation_data_train[ind]\n",
    "        topic_entity = line['q_entity']\n",
    "        answer = line['a_entity']\n",
    "        di_graph = build_Digraph(line['graph'])\n",
    "        for chain in id2chain[line['id']]:\n",
    "            if chain:\n",
    "                paths = bfs_with_rule(di_graph, topic_entity[0], chain)\n",
    "            # for p in paths:\n",
    "                # if p[-1][-1] == answer[0]:\n",
    "            input_data.append({\"query\": line['question'], \"path\": paths})\n",
    "        if len(input_data) > 8:\n",
    "            try:\n",
    "                batch_pred = llm_chain.batch(input_data, return_only_outputs=True)\n",
    "            except BadRequestError as e:\n",
    "                print('*************************Bad Request**************')\n",
    "            except ValueError as e:\n",
    "                print(f'******************Value Error {ind}****************************')\n",
    "            output.extend(batch_pred)\n",
    "            input_data = []\n",
    "\n",
    "        ind += 1\n",
    "        pbar.update(1)\n",
    "        \n",
    "                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_utility_data(input_data, multi_retrieval=False):\n",
    "    print(\"creating groundness data\")\n",
    "    processed_data = []\n",
    "    for item in input_data:\n",
    "        if multi_retrieval is True:\n",
    "            if \"sent_idx\" not in item or item[\"sent_idx\"] == 0 or len(item[\"preceding_sentences\"]) == 0:\n",
    "                processed_data.append({\"instruction\": PROMPT_DICT[\"ground_multi_instruction\"], \"input\": PROMPT_DICT[\"ground_multi_input_wo_preceding\"].format_map(input), \"output\": item[\"score\"], \"task\": \"groudness\"})\n",
    "            else:\n",
    "                processed_data.append({\"instruction\": PROMPT_DICT[\"ground_multi_instruction\"], \"input\": PROMPT_DICT[\"ground_multi_input\"].format_map(input), \"output\": item[\"score\"], \"task\": \"groudness\"})\n",
    "        else: \n",
    "            processed_data.append({\"instruction\": PROMPT_DICT[\"ground_instruction\"], \"input\": PROMPT_DICT[\"ground_input\"].format_map(item['input']), \"output\": item[\"score\"], \"task\": \"groudness\"})\n",
    "    print(processed_data[-1])\n",
    "    print(\"total data number: {}\".format(len(processed_data)))\n",
    "    print(Counter([item[\"output\"] for item in processed_data ]))\n",
    "    return processed_data\n",
    "process_data = create_groundness_data(input_data, multi_retrieval=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_422534/2392507187.py:32: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  llm_chain = LLMChain(llm=model, prompt=few_shot_intepretable_prompt, verbose=False)\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "graph_intepretable =  \"\"\"Based on the reasoning paths, please answer the given question and explain why.\n",
    "Question: {query}\\n\n",
    "Reasoning Paths: {path}\\n\n",
    "\"\"\"\n",
    "# The name of Justin Bieber's brother is Jaxon Bieber. This is based on the reasoning path that connects Justin Bieber to Jaxon Bieber through the relationship of sibling. The other paths that connect Justin Bieber to Jazmyn Bieber or back to Justin Bieber himself are incorrect in this context.\n",
    "few_shot_intepretable_prompt = FewShotPromptTemplate(\n",
    "        examples=[{\n",
    "            \"query\": \"what is the name of justin bieber brother?\",\n",
    "            \"path\": \"[[('Justin Bieber', 'people.person.sibling_s', 'm.0gxnnwc'), ('m.0gxnnwc', 'people.sibling_relationship.sibling', 'Jazmyn Bieber')], [('Justin Bieber', 'people.person.sibling_s', 'm.0gxnnwc'), ('m.0gxnnwc', 'people.sibling_relationship.sibling', 'Justin Bieber')], [('Justin Bieber', 'people.person.sibling_s', 'm.0gxnnwp'), ('m.0gxnnwp', 'people.sibling_relationship.sibling', 'Jaxon Bieber')], [('Justin Bieber', 'people.person.sibling_s', 'm.0gxnnwp'), ('m.0gxnnwp', 'people.sibling_relationship.sibling', 'Justin Bieber')]]\",\n",
    "            \"answer\": \"Jaxon Bieber\",\n",
    "            \"explanation\": \"This is based on the reasoning path that connects Justin Bieber to Jaxon Bieber through the relationship of sibling. The other paths that connect Justin Bieber to Jazmyn Bieber or back to Justin Bieber himself are incorrect in this context.Therefore, the name of Justin Bieber's brother is Jaxon Bieber\"\n",
    "        }],\n",
    "        example_prompt=PromptTemplate.from_template(\"\"\"\n",
    "Question: {query}\\n\n",
    "Reasoning Paths: {path}\\n\n",
    "Answer: {answer}\n",
    "Explanation: {explanation}\n",
    "\"\"\"),\n",
    "        prefix=\n",
    "        \"\"\"Based on the reasoning paths, please answer the given question and explain why.\"\"\",\n",
    "        suffix=\n",
    "        \"\"\"\n",
    "Question: {query}\\n\n",
    "Reasoning Paths: {path}\\n\"\"\",\n",
    "        input_variables=[\"query\", \"path\"],\n",
    ")\n",
    "graph_intepretable_prompt = PromptTemplate(input_variables=[\"query\", \"path\"], template=\n",
    "graph_intepretable)\n",
    "llm_chain = LLMChain(llm=model, prompt=few_shot_intepretable_prompt, verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "本质是reflection的能力，reflection即对于思维和检索的内容进行打分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../output/groudness_webqsp.json\", \"w\") as outfile:\n",
    "        json.dump(process_data, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "random.shuffle(new_data)\n",
    "output_file_name = '../output/testtime1_1105'\n",
    "train_data = new_data[500:]\n",
    "dev_data = new_data[:500]\n",
    "\n",
    "with open(output_file_name + \"_train.json\", \"w\") as outfile:\n",
    "    json.dump(train_data, outfile)\n",
    "with open(output_file_name + \"_dev.json\", \"w\") as outfile:\n",
    "    json.dump(dev_data, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "new_data = []\n",
    "for d in data:\n",
    "    if d['output'] == '[Fully supported]':\n",
    "        if random.random() < 0.2:\n",
    "            new_data.append(d)\n",
    "    else:\n",
    "        new_data.append(d)\n",
    "Counter([item[\"output\"] for item in new_data ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../output/intepretable_answer.json\", \"r\") as f:\n",
    "    data= json.load(f)\n",
    "input_data = []\n",
    "ind = 1\n",
    "count = 0\n",
    "llm_chain = LLMChain(llm=model, prompt=multi_graph_utility_prompt, verbose=True)   \n",
    "while ind < len(relation_data_train):\n",
    "        line = relation_data_train[ind]\n",
    "        topic_entity = line['q_entity']\n",
    "        answer = line['a_entity']\n",
    "        di_graph = build_Digraph(line['graph'])\n",
    "        for chain in id2chain[line['id']]:\n",
    "            if chain:\n",
    "                paths = bfs_with_rule(di_graph, topic_entity[0], chain)\n",
    "                input_data.append({\"query\": line['question'], \"output\": '.'.join(data[count]['text'].split('\\nExplanation: '))})\n",
    "                count += 1\n",
    "        if len(input_data) > 8:\n",
    "            try:\n",
    "                batch_pred = llm_chain.batch(input_data, return_only_outputs=True)\n",
    "            except BadRequestError as e:\n",
    "                print('*************************Bad Request**************')\n",
    "            except ValueError as e:\n",
    "                print(f'******************Value Error {ind}****************************')\n",
    "            input_data = []\n",
    "            break\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference data creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../output/chain_data_top_5.json', 'r') as f:\n",
    "    chain_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7867"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chain_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_start = True\n",
    "generate_data = []\n",
    "for line in chain_data:\n",
    "    if new_start:\n",
    "        starter = '[Retreive New Relationship]'\n",
    "    if line['effective'] == False:\n",
    "        processed_relationship = ';'.join(line['candidate_relation']  + [line['real_relation']])\n",
    "    else:\n",
    "        processed_relationship = ';'.join(line['candidate_relation'])\n",
    "    candidate_entities = line['candidate_entity'][:5] if line['real_entity'] not in line['candidate_entity'] else line['candidate_entity'][:4] + [line['real_entity']]\n",
    "    precessed_triplet = [f\"({line['paths'][line['chain_step']-1][0]}, {line['paths'][line['chain_step']-1][1]}, {entity})\" for entity in candidate_entities]\n",
    "    starter += \"<paragraph>{}</paragraph>\".format(processed_relationship)\n",
    "    starter += 'Retrieved relationship: {}[Fully Relevant][Retrieve Entity]'.format(line['real_relation'])\n",
    "    starter += \"<paragraph>{}</paragraph>\".format(';'.join(precessed_triplet))\n",
    "    starter +=f\"Retrieved triplet: ({line['paths'][line['chain_step']-1][0]}, {line['paths'][line['chain_step']-1][1]}, {line['paths'][line['chain_step']-1][-1]})[Fully Relevant]\"\n",
    "    if line['chain_step'] == len(line['paths']):\n",
    "        starter += f\"[No Retrieval] {';'.join(line['answer'])} [Confidence: 5]\"\n",
    "        generate_data.append({\"instruction\": line['query'], \"output\": starter})\n",
    "        new_start = True\n",
    "        # if len(generate_data) == 10:\n",
    "        #     break\n",
    "    else:\n",
    "        starter += f\"[Continue to Retrieve Evidence]\"\n",
    "        new_start = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../output/test_generate_data_2000.json', 'w') as f:\n",
    "    json.dump(generate_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[New Retrieval] <paragraph> 'people.sibling_relationship.sibling',\n",
    "'people.person.sibling_s',\n",
    "'people.person.parents'</paragraph> Retrieved relationship: people.person.sibling_s[Fully Relevant][Retrieve entity]<paragraph> 'Jazmyn Bieber', 'Justin Bieber', 'Jaxon Bieber'</paragraph> Retrieved triplet: ('Jazmyn Bieber', 'sibling_s', 'Justin Bieber')[Fully Relevant][No Retrieval] Jaxon Bieber [Confidence: 5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:8000/v1\",\n",
    "    api_key=\"token-abc123\",\n",
    "    # skip_special_tokens=False,\n",
    ")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"/media/disk1/chatgpt/LLaMA-Factory/saves/llama3-8b/full/sft/\", \n",
    "  messages=[\n",
    "    {\"role\": \"user\", \"content\": \"You will be provided with a query, evidence, output sentence, and preceding sentences (optional). Your task is to determine whether the information in the output sentence can be fully verified by the evidence or if it requires further external verification. There are three cases:\\n- If the output sentence can be verified solely with the evidence and the preceding sentences, then respond with [No Retrieval]. \\n- If additional information about the tail entity in evidence is needed to verify the output sentence, respond with [Continue to Retrieve Evidence] \\n- If more information unrelated to the evidence is needed, e.g. totally new relationship or new entity, reponse with [New Retrieval].\\n\\nQuery: what city was michael jackson born in?\\nEvidence: \\nOutput: Gary\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "step = 0\n",
    "prediction = ''\n",
    "preceding_sentences = ''\n",
    "evidence = ''\n",
    "i = 0\n",
    "query = train_webqsp['Questions'][i]['RawQuestion']\n",
    "candidate_entity = train_webqsp['Questions'][i]['Parses'][0]['TopicEntityName']\n",
    "while prediction != '[No Retrieval]' and step < 3:\n",
    "    step += 1\n",
    "\n",
    "    current_input = PROMPT_DICT['multi_retrieval_three_way_instruction'] + PROMPT_DICT['multi_retrieval_three_way_input'].format(query=query, evidence=evidence, output=prediction, preceding_sentences=preceding_sentences)\n",
    "    print(current_input)\n",
    "    completion = client.chat.completions.create(\n",
    "      model=\"/media/disk1/chatgpt/LLaMA-Factory/saves/llama3-8b/full/sft/\", \n",
    "    \n",
    "      messages=[\n",
    "        {\"role\": \"user\", \"content\": current_input}\n",
    "      ]\n",
    "    )   \n",
    "    prediction = completion.choices[0].message.content\n",
    "        \n",
    "    if prediction == '[Continue to Retrieve Evidence]':\n",
    "      completion = client.chat.completions.create(\n",
    "        model=\"/media/disk1/chatgpt/LLaMA-Factory/saves/llama3-8b/full/sft/\", \n",
    "      \n",
    "        messages=[\n",
    "          {\"role\": \"user\", \"content\": }\n",
    "        ]\n",
    "      )\n",
    "    prediction = completion.choices[0].message.content\n",
    "      \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ground_instruction', 'ground_input', 'ground_instruction_multi', 'relevance_instruction', 'relevance_input', 'ground_multi_input', 'ground_multi_input_wo_preceding', 'retrieval_instruction', 'retrieval_input', 'retrieval_multi_instruction', 'retrieval_multi_input', 'multi_retrieval_three_way_instruction', 'multi_retrieval_three_way_input', 'multi_retrieval_three_way_input_wo_preceding', 'utility_instruction', 'utility_input'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PROMPT_DICT.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Query: {query}\\nPreceding sentences: {preceding_sentences}\\nEvidence: {evidence}\\nOutput: {target_output}'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PROMPT_DICT['multi_retrieval_three_way_input']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "raptor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
