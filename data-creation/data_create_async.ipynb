{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Critic data creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "import os\n",
    "\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"2b219db0d2984f9dae28b651ab8ab3d9\"\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://smsh.openai.azure.com/\"\n",
    "os.environ[\"AZURE_OPENAI_API_VERSION\"] = \"2024-02-01\"\n",
    "os.environ[\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"] = \"gpt-35-turbo\"\n",
    "\n",
    "\n",
    "# os.environ[\"AZURE_OPENAI_API_KEY\"] = \"0c75de50975e4f278b882fe90da47f2f\"\n",
    "# os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://ces.openai.azure.com\"\n",
    "# os.environ[\"AZURE_OPENAI_API_VERSION\"] = \"2024-02-01\"\n",
    "# os.environ[\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"] = \"gpt-35-turbo\"\n",
    "\n",
    "# os.environ[\"AZURE_OPENAI_API_KEY\"] = \"aa183bb914bb4858b15bed161fb47ba5\"\n",
    "# os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://bxcl-prod.openai.azure.com/\"\n",
    "# os.environ[\"AZURE_OPENAI_API_VERSION\"] = \"2024-08-01-preview\"\n",
    "# os.environ[\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"] = \"gpt-4o\"\n",
    "model = AzureChatOpenAI(\n",
    "    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    "    azure_deployment=os.environ[\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"],\n",
    "    temperature=1,\n",
    "    n = 3,\n",
    "    max_retries=5, request_timeout=600\n",
    ")\n",
    "import os\n",
    "from uuid import uuid4\n",
    "\n",
    "# unique_id = uuid4().hex[0:8]\n",
    "# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "# os.environ[\"LANGCHAIN_PROJECT\"] = f\"Tracing Walkthrough - {unique_id}\"\n",
    "# os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "# os.environ[\"LANGCHAIN_API_KEY\"] = \"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\"  \n",
    "\n",
    "# from langsmith import Client\n",
    "\n",
    "# client = Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### complete chain create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3893815/954298537.py:9: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n",
      "/media/disk1/chatgpt/miniconda3/envs/self-rag/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain.embeddings import (\n",
    "    HuggingFaceEmbeddings,\n",
    ")\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"2b219db0d2984f9dae28b651ab8ab3d9\"\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://smsh.openai.azure.com/\"\n",
    "os.environ[\"AZURE_OPENAI_API_VERSION\"] = \"2024-03-01-preview\"\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "                model_name=\"BAAI/bge-large-en-v1.5\",\n",
    "                model_kwargs={'device': 'cpu'},\n",
    "                encode_kwargs={'normalize_embeddings': False},\n",
    "            )\n",
    "\n",
    "from langchain.storage import LocalFileStore, RedisStore\n",
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "store = RedisStore(redis_url=\"redis://localhost:6379\")\n",
    "cached_embedder = CacheBackedEmbeddings.from_bytes_store(\n",
    "embeddings, store, namespace=\"bge-large\"\n",
    ")\n",
    "row_string = []\n",
    "with open('../data/clean_relations', 'r') as f:\n",
    "    data = f.readlines()\n",
    "db = FAISS.from_texts(data, cached_embedder)\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/media/disk1/chatgpt/zh/graph_data')\n",
    "from src.graph_utils import *\n",
    "from src.sparql_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/disk1/chatgpt/miniconda3/envs/self-rag/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Resolving data files: 100%|██████████| 18/18 [00:01<00:00, 12.76it/s]\n",
      "Resolving data files: 100%|██████████| 18/18 [00:00<00:00, 117780.77it/s]\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "import networkx as nx\n",
    "from collections import deque\n",
    "import walker\n",
    "import json\n",
    "def build_Digraph(graph: list) -> nx.Graph:\n",
    "    G = nx.DiGraph()\n",
    "    for triplet in graph:\n",
    "        h, r, t = triplet\n",
    "        G.add_edge(h, t, relation=r.strip())\n",
    "    return G\n",
    "\n",
    "def bfs_with_rule(graph, start_node, target_rule, max_p = 10):\n",
    "    result_paths = []\n",
    "    queue = deque([(start_node, [])])\n",
    "    while queue:\n",
    "        current_node, current_path = queue.popleft()\n",
    "        if len(current_path) == len(target_rule):\n",
    "            result_paths.append(current_path)\n",
    "        if len(current_path) < len(target_rule):\n",
    "            if current_node not in graph:\n",
    "                continue\n",
    "            for neighbor in graph.neighbors(current_node):\n",
    "                rel = graph[current_node][neighbor]['relation']\n",
    "                if rel != target_rule[len(current_path)] or len(current_path) > len(target_rule):\n",
    "                    continue\n",
    "                queue.append((neighbor, current_path + [(current_node, rel,neighbor)]))\n",
    "            for neighbor in graph.predecessors(current_node):\n",
    "                rel = graph[neighbor][current_node]['relation']\n",
    "                if rel != target_rule[len(current_path)] or len(current_path) > len(target_rule):\n",
    "                    continue\n",
    "                queue.append((neighbor, current_path + [(current_node, rel,neighbor)]))\n",
    "    \n",
    "    return result_paths\n",
    "# relation_data_train = datasets.load_dataset('rmanluo/RoG-webqsp', split='train')\n",
    "\n",
    "relation_data_train_cwq = datasets.load_dataset('rmanluo/RoG-cwq', split='train')\n",
    "relation_data_train_wbq = datasets.load_dataset('rmanluo/RoG-webqsp', split='train')\n",
    "\n",
    "# llm_chain = LLMChain(llm=model, prompt=multi_graph_groundness_prompt, verbose=True)\n",
    "# batch_pred = llm_chain.batch([{\"query\": data[0]['RawQuestion'], \"preceding_sentences\": \"\", \"evidence\": \" relationship: influence.influence_node.influenced_by\",}], return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2826"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(relation_data_train_wbq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "def save_to_json(data: List, data_path='../output/chain_data.json'):\n",
    "    if not os.path.isfile(data_path):\n",
    "        # 文件不存在，创建新列表并写入文件\n",
    "        with open(data_path, 'w', encoding='utf-8') as file:\n",
    "            json.dump(data, file, ensure_ascii=False, indent=4)\n",
    "        return\n",
    "    try:\n",
    "        # 尝试读取现有文件\n",
    "        with open(data_path, 'r', encoding='utf-8') as file:\n",
    "            # 加载现有的JSON数据\n",
    "            existing_data = json.load(file)\n",
    "            existing_data.extend(data)\n",
    "    except json.JSONDecodeError:\n",
    "        # 文件不是有效的JSON，打印错误信息并退出\n",
    "        print(f\"文件 {data_path} 不是有效的JSON格式。\")\n",
    "        return\n",
    "    except ValueError as e:\n",
    "        # 打印错误信息并退出\n",
    "        print(e)\n",
    "        return\n",
    "    # 将更新后的数据写回文件\n",
    "    with open(data_path, 'w', encoding='utf-8') as file:\n",
    "        json.dump(existing_data, file, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = relation_data_train_cwq[13]\n",
    "line = data\n",
    "id = line['id']\n",
    "topic_entity = line['q_entity']\n",
    "answer = line['a_entity']\n",
    "di_graph = build_Digraph(line['graph'])\n",
    "paths = get_truth_paths(topic_entity, answer, di_graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'John Harbaugh': 'm.0c2fvr'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_another_entity('m.01ct6', 'american_football.football_team.current_head_coach')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Georgetown University']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'relation': 'royalty.kingdom.rulers'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "di_graph.adj[topic_entity[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base.marchmadness.ncaa_basketball_team.ncaa_tournament_seeds',\n",
       " 'base.marchmadness.ncaa_basketball_team.tournament_games_lost',\n",
       " 'base.marchmadness.ncaa_basketball_team.tournament_games_won',\n",
       " 'basketball.basketball_team.conference',\n",
       " 'basketball.basketball_team.division',\n",
       " 'basketball.basketball_team.head_coach',\n",
       " 'common.topic.article',\n",
       " 'common.topic.notable_for',\n",
       " 'common.topic.notable_types',\n",
       " 'sports.school_sports_team.athletics_brand',\n",
       " 'sports.school_sports_team.school',\n",
       " 'sports.sports_team.arena_stadium',\n",
       " 'sports.sports_team.colors',\n",
       " 'sports.sports_team.roster',\n",
       " 'sports.sports_team.sport',\n",
       " 'sports.sports_team.venue'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#获取rog dataset中的邻接边\n",
    "set([r['relation'] for d, r in di_graph.adj[topic_entity[0]].items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_tmp = FAISS.from_texts(set([r['relation'] for d, r in di_graph.adj[topic_entity[0]].items()]), cached_embedder)\n",
    "retriever_tmp = db_tmp.as_retriever(search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3098\n"
     ]
    }
   ],
   "source": [
    "with open('../data/WebQSP.train.json', 'r') as f:\n",
    "    constraint_data = json.load(f)\n",
    "print(len(constraint_data['Questions']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27639\n"
     ]
    }
   ],
   "source": [
    "with open('../data/Constraint/train_CWQ/con.txt', 'r') as f:\n",
    "    const = f.readlines()\n",
    "# with open('../data/Constraint/train_CWQ/q.txt', 'r') as f:\n",
    "#     que = list(map(remove_punctuation_and_lowercase, f.readlines()))\n",
    "print(len(const))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"what state is home to the university that is represented in sports by george washington colonials men's basketball\""
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "def remove_punctuation_and_lowercase(sentence):\n",
    "    # 使用正则表达式去除末尾的标点符号\n",
    "    sentence = re.sub(r'[.!?]$', '', sentence)\n",
    "    # 转换为小写\n",
    "    sentence = sentence.lower().strip()\n",
    "    return sentence\n",
    "remove_punctuation_and_lowercase(relation_data_train_cwq[0]['question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constraint = []\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "for ind, data in tqdm(enumerate(relation_data_train_cwq)):\n",
    "    question_id = data['id']\n",
    "    const_d = const[ind].strip()\n",
    "    if len(const_d) == 0:\n",
    "        continue\n",
    "    id = data['id']\n",
    "    topic_entity = data['q_entity']\n",
    "    answer = data['a_entity']\n",
    "    di_graph = build_Digraph(data['graph'])\n",
    "    paths = get_truth_paths(topic_entity, answer, di_graph)\n",
    "    paths = random.sample(paths, min(3, len(paths)))\n",
    "    all_entities = set()\n",
    "    for path in paths:\n",
    "        for tri in path:\n",
    "            all_entities.add(tri[0])\n",
    "            all_entities.add(tri[-1])\n",
    "    for ent in all_entities:\n",
    "        if get_label(const_d)  in di_graph[ent]:\n",
    "            constraint.append({\"qid\":question_id, \"constraint\": {\"node\": const_d.strip(), \"node_name\": get_label(const_d.strip()), \"relation\": di_graph[ent][get_label(const_d)]['relation']}})\n",
    "            break\n",
    "    if len(constraint):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "constraint = []\n",
    "for d in constraint_data['Questions']:\n",
    "    question_id = d['QuestionId']\n",
    "    for p_data in d['Parses']:\n",
    "        if len(p_data['Constraints']):\n",
    "            cur = p_data['Constraints'][0]\n",
    "            if cur['ArgumentType'] ==  \"Entity\":\n",
    "                constraint.append({\"qid\":question_id, \"constraint\": {\"node\": cur['Argument'], \"node_name\": cur['EntityName'], 'relation': cur['NodePredicate']}})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "can = {}\n",
    "if len(can):\n",
    "    print('su')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function process_relation_data at 0x7f2263687640> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "Map (num_proc=4): 100%|██████████| 2826/2826 [2:27:37<00:00,  3.13s/ examples]  \n",
      "Creating json from Arrow format: 100%|██████████| 3/3 [00:00<00:00, 19.60ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6247874"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "import random\n",
    "import json\n",
    "import multiprocessing as mp\n",
    "# from multiprocess import set_start_method\n",
    "# set_start_method(\"spawn\", force=True)\n",
    "def process_relation_data(line):\n",
    "    # import multiprocess\n",
    "    chain_data = []\n",
    "    id = line['id']\n",
    "    topic_entity = line['q_entity']\n",
    "    answer = line['a_entity']\n",
    "    di_graph = build_Digraph(line['graph'])\n",
    "    paths = get_truth_paths(topic_entity, answer, di_graph)\n",
    "    # every question sample at most 3 paths\n",
    "    paths = random.sample(paths, min(3, len(paths)))\n",
    "    sent_idx = 0\n",
    "    for pid, p in enumerate(paths):\n",
    "        for pidx, step in enumerate(p):\n",
    "            real_relation = step[1]\n",
    "            real_entity = step[2]\n",
    "            candidate_entities = [tail for head, tail in di_graph.out_edges(step[0]) if di_graph[head][tail].get('relation') == step[1]]\n",
    "            candidate_relation = [page.page_content.strip() for page in retriever.invoke(line['question'] + real_relation + real_entity)]\n",
    "            if step[1] in candidate_relation:\n",
    "                chain_data.append({\n",
    "                    \"sent_idx\": sent_idx,\n",
    "                    \"chain_step\": pidx + 1,\n",
    "                    \"candidate_relation\": candidate_relation,\n",
    "                    \"candidate_entity\": candidate_entities,\n",
    "                    \"real_relation\": real_relation,\n",
    "                    \"real_entity\": real_entity,\n",
    "                    \"paths\": p,\n",
    "                    \"effective\": True\n",
    "                })\n",
    "            else:\n",
    "                chain_data.append({\n",
    "                    \"sent_idx\": sent_idx,\n",
    "                    \"chain_step\": pidx + 1,\n",
    "                    \"candidate_relation\": candidate_relation,\n",
    "                    \"candidate_entity\": candidate_entities,\n",
    "                    \"real_relation\": real_relation,\n",
    "                    \"real_entity\": real_entity,\n",
    "                    \"paths\": p,\n",
    "                    \"effective\": False\n",
    "                })\n",
    "        sent_idx += 1\n",
    "    return {\"qid\": id, \"query\": line['question'], \"topic_entity\": topic_entity, \"answer\": answer, \"chains\": chain_data}\n",
    "\n",
    "# processed_data = relation_data_train.shard(num_shards=4, index=2).map(process_relation_data, num_proc=16, remove_columns=relation_data_train.column_names)\n",
    "# processed_data.to_json('../output/cwq_train_chain_top_5_shard_3.json')\n",
    "\n",
    "\n",
    "        # with lock:\n",
    "        #     save_to_json(chain_data, f'../output/async_data_top_5_cwp.json')\n",
    "# process_relation_data(relation_data_train.shard(num_shards=4, index=0))\n",
    "# num_proc = 4\n",
    "# with mp.Pool() as pool:\n",
    "#     results = pool.imap_unordered(process_relation_data, [relation_data_train.shard(num_shards=num_proc, index=0)]) \n",
    "# for result in results:\n",
    "#     # 处理结果\n",
    "#     print(len(result))\n",
    "# webqsp_chain = relation_data_train.map(process_relation_data, num_proc=4).remove_columns(['graph'])\n",
    "\n",
    "# # 创建线程池\n",
    "# m = multiprocessing.Manager()\n",
    "# lock = m.Lock()\n",
    "# with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:\n",
    "#     # 提交任务到线程池\n",
    "#     for index, line in enumerate(relation_data):\n",
    "#         executor.submit(process_relation_data, line, lock)\n",
    "\n",
    "# # 等待所有任务完成\n",
    "# executor.shutdown()\n",
    "\n",
    "processed_data = relation_data_train_wbq.map(process_relation_data, num_proc=4, remove_columns=relation_data_train_wbq.column_names)\n",
    "# processed_data = relation_data_train_cwq.shard(num_shards=4, index=3).map(process_relation_data, num_proc=8, remove_columns=relation_data_train_cwq.column_names)\n",
    "processed_data.to_json('../output/chain_data/wbq_train_chain_top_5.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../output/chain_data.json', 'r', encoding='utf-8') as file:\n",
    "    chain_data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_relation_data(line):\n",
    "    chains = []\n",
    "    id = line['id']\n",
    "    topic_entity = line['q_entity']\n",
    "    answer = line['a_entity']\n",
    "    di_graph = build_Digraph(line['graph'])\n",
    "    paths = get_truth_paths(topic_entity, answer, di_graph)\n",
    "    # every question sample at most 3 paths\n",
    "    paths = random.sample(paths, min(3, len(paths)))\n",
    "    sent_idx = 0\n",
    "    for pid, p in enumerate(paths):\n",
    "        chain_data = []\n",
    "        for pidx, step in enumerate(p):\n",
    "            real_relation = step[1]\n",
    "            real_entity = step[2]\n",
    "            candidate_entities = [tail for head, tail in di_graph.out_edges(step[0]) if di_graph[head][tail].get('relation') == step[1]]\n",
    "            candidate_relation = [page.page_content.strip() for page in retriever.invoke(line['question'] + real_relation + real_entity)]\n",
    "            if step[1] in candidate_relation:\n",
    "                chain_data.append({\n",
    "                    \"p_id\": pid,\n",
    "                    \"chain_step\": pidx + 1,\n",
    "                    \"candidate_relation\": candidate_relation,\n",
    "                    \"candidate_entity\": candidate_entities,\n",
    "                    \"real_relation\": real_relation,\n",
    "                    \"real_entity\": real_entity,\n",
    "                    \"paths\": p,\n",
    "                    \"effective\": True\n",
    "                })\n",
    "            else:\n",
    "                chain_data.append({\n",
    "                    \"p_id\": pid,\n",
    "                    \"chain_step\": pidx + 1, \n",
    "                    \"candidate_relation\": candidate_relation,\n",
    "                    \"candidate_entity\": candidate_entities,\n",
    "                    \"real_relation\": real_relation,\n",
    "                    \"real_entity\": real_entity,\n",
    "                    \"paths\": p,\n",
    "                    \"effective\": False\n",
    "                })\n",
    "    chains.append(chain_data)\n",
    "    return {\"qid\": id, \"query\": line['question'], \"topic_entity\": topic_entity, \"answer\": answer, \"chains\": chains}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create Retrieval data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "input_data = []\n",
    "\n",
    "for s, line in enumerate(chain_data):\n",
    "    random.seed(s + 1)\n",
    "    rand_num = random.random()\n",
    "    if line['sent_idx'] == 0:\n",
    "        if rand_num < 0.5:\n",
    "            input_data.append({\"input\": {\"query\": line['query'], \"preceding_sentences\": \"\", \"evidence\": \"\", \"target_output\": ';'.join(line['answer'])}, \"decision_token\": \"[New Retrieval]\"})\n",
    "            continue\n",
    "    step = line['chain_step']\n",
    "    if len(line['paths']) == 1:\n",
    "        assert line['chain_step'] == 1\n",
    "        if rand_num < 0.6:\n",
    "            input_data.append({\"input\": {\"query\": line['query'], \"preceding_sentences\": \"\", \"evidence\": f\"Relations retrieved: {line['real_relation']}\\n Entities retrieved: {line['real_entity']}\", \"target_output\": ';'.join(line['answer'])}, \"decision_token\": \"[No Retrieval]\"})\n",
    "    else:\n",
    "        if step == 1:\n",
    "            if rand_num < 0.9:\n",
    "                input_data.append({\"input\": {\"query\": line['query'], \"preceding_sentences\": \"\", \"evidence\": f\"Relations retrieved: {line['real_relation']}\\n Entities retrieved: {line['real_entity']}\", \"target_output\": ';'.join(line['answer'])}, \"decision_token\": \"[Continue to Retrieve Evidence]\"})\n",
    "        elif step < len(line['paths']):\n",
    "            if rand_num < 0.8:\n",
    "                input_data.append({\"input\": {\"query\": line['query'], \"preceding_sentences\": f\"Information retrieved: {line['paths'][:step]}\", \"evidence\": f\"Relations retrieved: {line['real_relation']}\\n Entities retrieved: {line['real_entity']}\", \"target_output\": ';'.join(line['answer'])}, \"decision_token\": \"[Continue to Retrieve Evidence]\"})\n",
    "        else:\n",
    "            if rand_num < 0.4:\n",
    "                input_data.append({\"input\": {\"query\": line['query'], \"preceding_sentences\": f\"Information retrieved: {line['paths'][:step]}\", \"evidence\": f\"Relations retrieved: {line['real_relation']}\\n Entities retrieved: {line['real_entity']}\", \"target_output\": ';'.join(line['answer'])}, \"decision_token\": \"[No Retrieval]\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating multi sentence retrieval data\n",
      "{'instruction': 'You will be provided with a query, evidence, output sentence, and preceding sentences (optional). Your task is to determine whether the information in the output sentence can be fully verified by the evidence or if it requires further external verification. There are three cases:\\n- If the output sentence can be verified solely with the evidence and the preceding sentences, then respond with [No Retrieval]. \\n- If additional information about the tail entity in evidence is needed to verify the output sentence, respond with [Continue to Retrieve Evidence] \\n- If more information unrelated to the evidence is needed, e.g. totally new relationship or new entity, reponse with [New Retrieval].\\n\\n', 'input': 'Query: who played alf on the tv show\\nEvidence: \\nOutput: Paul Fusco', 'output': '[New Retrieval]', 'task': 'multi_retrieval'}\n",
      "total data number: 6498\n",
      "Counter({'[Continue to Retrieve Evidence]': 3241, '[No Retrieval]': 2946, '[New Retrieval]': 311})\n"
     ]
    }
   ],
   "source": [
    "retrival_critic = create_retrieval_data(input_data=input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create Relevant data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_intepretable =  \"\"\"Based on the reasoning paths, please answer the given question and explain why.\n",
    "Question: {query}\\n\n",
    "Reasoning Paths: {path}\\n\n",
    "\"\"\"\n",
    "# The name of Justin Bieber's brother is Jaxon Bieber. This is based on the reasoning path that connects Justin Bieber to Jaxon Bieber through the relationship of sibling. The other paths that connect Justin Bieber to Jazmyn Bieber or back to Justin Bieber himself are incorrect in this context.\n",
    "few_shot_intepretable_prompt = FewShotPromptTemplate(\n",
    "        examples=[{\n",
    "            \"query\": \"what is the name of justin bieber brother?\",\n",
    "            \"path\": \"[[('Justin Bieber', 'people.person.sibling_s', 'm.0gxnnwc'), ('m.0gxnnwc', 'people.sibling_relationship.sibling', 'Jazmyn Bieber')], [('Justin Bieber', 'people.person.sibling_s', 'm.0gxnnwc'), ('m.0gxnnwc', 'people.sibling_relationship.sibling', 'Justin Bieber')], [('Justin Bieber', 'people.person.sibling_s', 'm.0gxnnwp'), ('m.0gxnnwp', 'people.sibling_relationship.sibling', 'Jaxon Bieber')], [('Justin Bieber', 'people.person.sibling_s', 'm.0gxnnwp'), ('m.0gxnnwp', 'people.sibling_relationship.sibling', 'Justin Bieber')]]\",\n",
    "            \"answer\": \"Jaxon Bieber\",\n",
    "            \"explanation\": \"This is based on the reasoning path that connects Justin Bieber to Jaxon Bieber through the relationship of sibling. The other paths that connect Justin Bieber to Jazmyn Bieber or back to Justin Bieber himself are incorrect in this context.Therefore, the name of Justin Bieber's brother is Jaxon Bieber\"\n",
    "        }],\n",
    "        example_prompt=PromptTemplate.from_template(\"\"\"\n",
    "Question: {query}\\n\n",
    "Reasoning Paths: {path}\\n\n",
    "Answer: {answer}\n",
    "Explanation: {explanation}\n",
    "\"\"\"),\n",
    "        prefix=\n",
    "        \"\"\"Based on the reasoning paths, please answer the given question and explain why.\"\"\",\n",
    "        suffix=\n",
    "        \"\"\"\n",
    "Question: {query}\\n\n",
    "Reasoning Paths: {path}\\n\"\"\",\n",
    "        input_variables=[\"query\", \"path\"],\n",
    ")\n",
    "graph_intepretable_prompt = PromptTemplate(input_variables=[\"query\", \"path\"], template=\n",
    "graph_intepretable)\n",
    "llm_chain = LLMChain(llm=model, prompt=few_shot_intepretable_prompt, verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = []\n",
    "for line in chain_data:\n",
    "    if line['effective'] == True:\n",
    "        input_data.append({\"input\": {\"query\": line['query'], \"evidence\": f\"Relations retrieved: {';'.join(line['candidate_relation'])}\"}, \"decision_token\": '[Relevant]'})\n",
    "    else:\n",
    "        input_data.append({\"input\": {\"query\": line['query'], \"evidence\": f\"Relations retrieved: {';'.join(line['candidate_relation'])}\"}, \"decision_token\": '[Irrelevant]'})\n",
    "create_relevance_data(input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create Support groundness data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_intepretable =  \"\"\"You will receive a query, evidence and optional preceding sentences containing history information. The evidence contains graph relationships or entities may be useful to answer the query. Your task is to filters out valid information from the evidence to answer the given query, evaluate your output and provide explanations on your result.\n",
    "\n",
    "###\n",
    "Query: Name the president of the country whose main spoken language was Brahui in 1980?\n",
    "Topic Entity: Brahui Language\n",
    "Evidence: language.human_language.main_country; language.human_language.language_family; language.human_language.iso_639_3_code; base.rosetta.languoid.parent; language.human_language.writing_system; base.rosetta.languoid.languoid_class; language.human_language.countries_spoken_in; kg.object_profile.prominent_type; base.rosetta.languoid.document; base.ontologies.ontology_instance.equivalent_instances; base.rosetta.languoid.local_name; language.human_language.region\n",
    "Preceding sentences: \n",
    "Output: \n",
    "1. {{language.human_language.main_country (Score: Fully relavant))}}: This relation is highly relevant as it directly relates to the country whose president is being asked for, and the main country where Brahui language is spoken in 1980.\n",
    "2. {{language.human_language.countries_spoken_in (Score: Fully relavant)}}: This relation is also relevant as it provides information on the countries where Brahui language is spoken, which could help narrow down the search for the president.\n",
    "3. {{base.rosetta.languoid.parent (Score: Partially relevant)}}: This relation is less relevant but still provides some context on the language family to which Brahui belongs, which could be useful in understanding the linguistic and cultural background of the country in question.\n",
    "\n",
    "###\n",
    "Query: {query}\n",
    "Topic Entity: {topic}\n",
    "Evidence: {evidence}\n",
    "Preceding sentences: {preceding_sentences}\n",
    "Output: \n",
    "\"\"\"\n",
    "# The name of Justin Bieber's brother is Jaxon Bieber. This is based on the reasoning path that connects Justin Bieber to Jaxon Bieber through the relationship of sibling. The other paths that connect Justin Bieber to Jazmyn Bieber or back to Justin Bieber himself are incorrect in this context.\n",
    "few_shot_intepretable_prompt = FewShotPromptTemplate(\n",
    "        examples=[{\n",
    "            \"query\": \"Name the president of the country whose main spoken language was Brahui in 1980?\",\n",
    "            \"topic\": \"Brahui Language\",\n",
    "            \"evidence\": \"language.human_language.main_country; language.human_language.language_family; language.human_language.iso_639_3_code; base.rosetta.languoid.parent; language.human_language.writing_system; base.rosetta.languoid.languoid_class; language.human_language.countries_spoken_in; kg.object_profile.prominent_type; base.rosetta.languoid.document; base.ontologies.ontology_instance.equivalent_instances; base.rosetta.languoid.local_name; language.human_language.region\",\n",
    "            \"preceding_sentences\": \"\",\n",
    "            \"output\": \"\"\"1. {{language.human_language.main_country (Score: [Fully Relavant])}}: This relation is highly relevant as it directly relates to the country whose president is being asked for, and the main country where Brahui language is spoken in 1980.\n",
    "2. {{language.human_language.countries_spoken_in (Score: [Fully Relavant])}}: This relation is also relevant as it provides information on the countries where Brahui language is spoken, which could help narrow down the search for the president.\n",
    "3. {{base.rosetta.languoid.parent (Score: [Partially Relevant])}}: This relation is less relevant but still provides some context on the language family to which Brahui belongs, which could be useful in understanding the linguistic and cultural background of the country in question.\"\"\"\n",
    "        }],\n",
    "        example_prompt=PromptTemplate.from_template(\"\"\"###\n",
    "Query: {query}\n",
    "Topic Entity: {topic}\n",
    "Evidence: {evidence}\n",
    "Preceding sentences: {preceding_sentences}\n",
    "Output: {output}\n",
    "\"\"\"),\n",
    "        prefix=\n",
    "        \"\"\"You will receive a query, topic entity, evidence and optional preceding sentences containing history information. The evidence contains graph relationships or entities may be useful to answer the query. Your task is to filters out 3 valid information from the evidence that contribute to answering the query and provide a relevance score for each output, output your explanations for the score.\n",
    "The score of relevance range from [Fully Relavant], [Partially Relevant] to [Unrelevant].\"\"\",\n",
    "        suffix=\n",
    "        \"\"\"###\n",
    "Query: {query}\n",
    "Topic Entity: {topic}\n",
    "Evidence: {evidence}\n",
    "Preceding sentences: {preceding_sentences}\n",
    "Output: \"\"\",\n",
    "        input_variables=[\"query\", \"evidence\", \"preceding_sentences\", \"topic\"],\n",
    ")\n",
    "graph_intepretable_prompt = PromptTemplate(input_variables=[\"query\", \"evidence\", \"preceding_sentences\", \"topic\"], template=\n",
    "graph_intepretable)\n",
    "llm_chain = LLMChain(llm=model, prompt=few_shot_intepretable_prompt, verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def extract_relationship_and_score(s):\n",
    "    match_dict = dict()\n",
    "    pattern = r'{(.+?) \\(Score: (.+?)\\)}'\n",
    "    for match in re.findall(pattern, s):\n",
    "        match_dict[match[0]] = match[1]\n",
    "    return match_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'qid': 'WebQTrn-0',\n",
       " 'sent_idx': 0,\n",
       " 'query': 'what is the name of justin bieber brother',\n",
       " 'chain_step': 1,\n",
       " 'candidate_relation': ['people.sibling_relationship.sibling',\n",
       "  'fictional_universe.fictional_character.siblings',\n",
       "  'fictional_universe.sibling_relationship_of_fictional_characters.siblings'],\n",
       " 'candidate_entity': [['Pattie Mallette', 'Jeremy Bieber']],\n",
       " 'real_relation': 'people.person.parents',\n",
       " 'real_entity': 'Jeremy Bieber',\n",
       " 'paths': [['Justin Bieber', 'people.person.parents', 'Jeremy Bieber'],\n",
       "  ['Jeremy Bieber', 'people.person.children', 'Jaxon Bieber']],\n",
       " 'answer': ['Jaxon Bieber'],\n",
       " 'effective': False}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = []\n",
    "random.seed(42)\n",
    "from openai import BadRequestError\n",
    "qids = []\n",
    "for line in random.sample(chain_data, 2000):\n",
    "    if line['qid'] in qids:\n",
    "        continue\n",
    "    qids.append(line['qid'])\n",
    "    try:\n",
    "        batch_pred = llm_chain.batch([{\"query\": line['query'], \"evidence\": ';'.join(line['candidate_relation']), \"preceding_sentences\": '', 'topic': line['paths'][line['chain_step'] - 1][0]}], return_only_outputs=True)\n",
    "    except BadRequestError as e:\n",
    "        print('*************************Bad Request**************')\n",
    "    except ValueError as e:\n",
    "        print(f'******************Value Error****************************')\n",
    "    match_dict = extract_relationship_and_score(batch_pred[0]['text'])\n",
    "    input_data.append({\"input\": {\"query\": line['query'], \"evidence\": ';'.join(line['candidate_relation']), \"preceding_sentences\": '', 'topic': line['paths'][line['chain_step'] - 1][0]}, \"match\":match_dict, 'real_relation': line['real_relation']})\n",
    "    if len(input_data) >= 3:\n",
    "        print('Saving')\n",
    "        save_to_json(input_data, f'../output/relevance_critic_data_top_3.json')\n",
    "        input_data = []\n",
    "# create_relevance_data(input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create confidence data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "few_shot_intepretable_prompt = FewShotPromptTemplate(\n",
    "        examples=[{\n",
    "            \"query\": \"what is the name of justin bieber brother?\",\n",
    "            \"output\": \"[Retreive New Relationship]<paragraph>people.sibling_relationship.sibling;fictional_universe.fictional_character.siblings;fictional_universe.sibling_relationship_of_fictional_characters.siblings;people.person.sibling_s;people.family.members;people.person.parents</paragraph>Retrieved relationship: people.person.parents[Fully Relevant][Retrieve Entity]<paragraph>(Justin Bieber, people.person.parents, Pattie Mallette);(Justin Bieber, people.person.parents, Jeremy Bieber);(Justin Bieber, people.person.parents, Jeremy Bieber)</paragraph>Retrieved triplet: (Justin Bieber, people.person.parents, Jeremy Bieber)[Fully Relevant][Continue to Retrieve Evidence]<paragraph>people.sibling_relationship.sibling;people.person.sibling_s;people.person.parents;fictional_universe.fictional_character.siblings;people.family.members;people.person.children</paragraph>Retrieved relationship: people.person.children[Fully Relevant][Retrieve Entity]<paragraph>(Jeremy Bieber, people.person.children, Jazmyn Bieber);(Jeremy Bieber, people.person.children, Justin Bieber);(Jeremy Bieber, people.person.children, Jaxon Bieber);(Jeremy Bieber, people.person.children, Jaxon Bieber)</paragraph>Retrieved triplet: (Jeremy Bieber, people.person.children, Jaxon Bieber)[Fully Relevant][No Retrieval] Answer: Jaxon Bieber\",\n",
    "            \"explanation\": \"The output provides the name of justin bieber's brother. This is based on the reasoning path that connects Justin Bieber to Jaxon Bieber through the relationship of sibling. The other paths that connect Justin Bieber to Jazmyn Bieber or back to Justin Bieber himself are incorrect in this context.\",\n",
    "            \"rating\": \"[Confidence:5]\"\n",
    "        }],\n",
    "        example_prompt=PromptTemplate.from_template(\"\"\"\n",
    "Query: {query}\\n\n",
    "Output: {output}\n",
    "Explanation: {explanation}\n",
    "Rating: {rating}\n",
    "\"\"\"),\n",
    "        prefix=\"\"\"Given a query and an output, rate whether the response and the thought process appears to be a helpful and informative answer to the query, from 1 (lowest) - 5 (highest). We call this confidence score.\n",
    "[Confidence:5]: The response provides a complete and correct reasoning chain to the query, and the final answer is complete and logically correct.\n",
    "[Confidence:4]: The response mostly fulfills the need in the query and provides correct answers, while there can be some minor improvements such as shorter reasoning chain or less repetition.\n",
    "[Confidence:3]: The response is acceptable, but the answers may be not complete or needs minor improvement.\n",
    "[Confidence:2]: The reasoning process still addresses the main request, but the answers are not correct or not relevant to the query.\n",
    "[Confidence:1]: The reasoning is barely irrelevant or does not give an answer in the end.\"\"\",\n",
    "        suffix=\n",
    "        \"\"\"\n",
    "Query: {query}\\n\n",
    "Output: {output}\\n\"\"\",\n",
    "        input_variables=[\"query\", \"output\"],\n",
    ")\n",
    "# confidence_prompt = PromptTemplate(input_variables=[\"query\", \"output\"], template=\n",
    "# graph_intepretable)\n",
    "llm_chain = LLMChain(llm=model, prompt=few_shot_intepretable_prompt, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven a query and an output, rate whether the response and the thought process appears to be a helpful and informative answer to the query, from 1 (lowest) - 5 (highest). We call this confidence score.\n",
      "[Confidence:5]: The response provides a complete and correct reasoning chain to the query, and the final answer is complete and logically correct.\n",
      "[Confidence:4]: The response mostly fulfills the need in the query and provides correct answers, while there can be some minor improvements such as shorter reasoning chain or less repetition.\n",
      "[Confidence:3]: The response is acceptable, but the answers may be not complete or needs minor improvement.\n",
      "[Confidence:2]: The reasoning process still addresses the main request, but the answers are not correct or not relevant to the query.\n",
      "[Confidence:1]: The reasoning is barely irrelevant or does not give an answer in the end.\n",
      "\n",
      "\n",
      "Query: what is the name of justin bieber brother?\n",
      "\n",
      "Output: [Retreive New Relationship]<paragraph>people.sibling_relationship.sibling;fictional_universe.fictional_character.siblings;fictional_universe.sibling_relationship_of_fictional_characters.siblings;people.person.sibling_s;people.family.members;people.person.parents</paragraph>Retrieved relationship: people.person.parents[Fully Relevant][Retrieve Entity]<paragraph>(Justin Bieber, people.person.parents, Pattie Mallette);(Justin Bieber, people.person.parents, Jeremy Bieber);(Justin Bieber, people.person.parents, Jeremy Bieber)</paragraph>Retrieved triplet: (Justin Bieber, people.person.parents, Jeremy Bieber)[Fully Relevant][Continue to Retrieve Evidence]<paragraph>people.sibling_relationship.sibling;people.person.sibling_s;people.person.parents;fictional_universe.fictional_character.siblings;people.family.members;people.person.children</paragraph>Retrieved relationship: people.person.children[Fully Relevant][Retrieve Entity]<paragraph>(Jeremy Bieber, people.person.children, Jazmyn Bieber);(Jeremy Bieber, people.person.children, Justin Bieber);(Jeremy Bieber, people.person.children, Jaxon Bieber);(Jeremy Bieber, people.person.children, Jaxon Bieber)</paragraph>Retrieved triplet: (Jeremy Bieber, people.person.children, Jaxon Bieber)[Fully Relevant][No Retrieval] Answer: Jaxon Bieber\n",
      "Explanation: The output provides the name of justin bieber's brother. This is based on the reasoning path that connects Justin Bieber to Jaxon Bieber through the relationship of sibling. The other paths that connect Justin Bieber to Jazmyn Bieber or back to Justin Bieber himself are incorrect in this context.\n",
      "Rating: [Confidence:5]\n",
      "\n",
      "\n",
      "\n",
      "Query: what is the money called in peru\n",
      "\n",
      "Output: [Retreive New Relationship]<paragraph>base.coinsdaily.denomination.money_value;finance.currency.currency_code;measurement_unit.money_value.amount;location.country.currency_used;location.country.currency_formerly_used</paragraph>Retrieved relationship: location.country.currency_used[Fully Relevant][Retrieve Entity] Justin\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'what is the money called in peru',\n",
       " 'output': '[Retreive New Relationship]<paragraph>base.coinsdaily.denomination.money_value;finance.currency.currency_code;measurement_unit.money_value.amount;location.country.currency_used;location.country.currency_formerly_used</paragraph>Retrieved relationship: location.country.currency_used[Fully Relevant][Retrieve Entity] Justin',\n",
       " 'text': 'Rating: [Confidence:2]\\nExplanation: The response does not provide the name of the currency in Peru, which is the main request of the query. Therefore, the reasoning process is not relevant to the query.'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.invoke({'query': 'what is the money called in peru',\"output\": '[Retreive New Relationship]<paragraph>base.coinsdaily.denomination.money_value;finance.currency.currency_code;measurement_unit.money_value.amount;location.country.currency_used;location.country.currency_formerly_used</paragraph>Retrieved relationship: location.country.currency_used[Fully Relevant][Retrieve Entity] Justin' })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/WebQSP.train.json\", \"r\") as f:\n",
    "    train_webqsp = json.load(f)\n",
    "id2chain = dict()\n",
    "for line in train_webqsp['Questions']:\n",
    "    unique_chain = []\n",
    "    for p in line['Parses']:\n",
    "        if p['InferentialChain'] not in unique_chain:\n",
    "            unique_chain.append(p['InferentialChain'])\n",
    "    id2chain[line['QuestionId']] = unique_chain\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = []\n",
    "#添加constraint\n",
    "line = relation_data_train[1]\n",
    "topic_entity = line['q_entity']\n",
    "answer = line['a_entity']\n",
    "di_graph = build_Digraph(line['graph'])\n",
    "for chain in id2chain[line['id']]:\n",
    "    paths = bfs_with_rule(di_graph, topic_entity[0], chain)\n",
    "    # for p in paths:\n",
    "        # if p[-1][-1] == answer[0]:\n",
    "    input_data.append({\"query\": line['question'], \"path\": paths})\n",
    "batch_pred = llm_chain.batch(input_data, return_only_outputs=True)\n",
    "\n",
    "                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = []\n",
    "output = []\n",
    "#添加constraint\n",
    "from tqdm.notebook import tqdm\n",
    "from openai import BadRequestError\n",
    "ind = 1\n",
    "with tqdm(total=len(relation_data_train), desc=f\"Processing\",ncols=1500) as pbar:\n",
    "    while ind < len(relation_data_train):\n",
    "        line = relation_data_train[ind]\n",
    "        topic_entity = line['q_entity']\n",
    "        answer = line['a_entity']\n",
    "        di_graph = build_Digraph(line['graph'])\n",
    "        for chain in id2chain[line['id']]:\n",
    "            if chain:\n",
    "                paths = bfs_with_rule(di_graph, topic_entity[0], chain)\n",
    "            # for p in paths:\n",
    "                # if p[-1][-1] == answer[0]:\n",
    "            input_data.append({\"query\": line['question'], \"path\": paths})\n",
    "        if len(input_data) > 8:\n",
    "            try:\n",
    "                batch_pred = llm_chain.batch(input_data, return_only_outputs=True)\n",
    "            except BadRequestError as e:\n",
    "                print('*************************Bad Request**************')\n",
    "            except ValueError as e:\n",
    "                print(f'******************Value Error {ind}****************************')\n",
    "            output.extend(batch_pred)\n",
    "            input_data = []\n",
    "\n",
    "        ind += 1\n",
    "        pbar.update(1)\n",
    "        \n",
    "                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_utility_data(input_data, multi_retrieval=False):\n",
    "    print(\"creating groundness data\")\n",
    "    processed_data = []\n",
    "    for item in input_data:\n",
    "        if multi_retrieval is True:\n",
    "            if \"sent_idx\" not in item or item[\"sent_idx\"] == 0 or len(item[\"preceding_sentences\"]) == 0:\n",
    "                processed_data.append({\"instruction\": PROMPT_DICT[\"ground_multi_instruction\"], \"input\": PROMPT_DICT[\"ground_multi_input_wo_preceding\"].format_map(input), \"output\": item[\"score\"], \"task\": \"groudness\"})\n",
    "            else:\n",
    "                processed_data.append({\"instruction\": PROMPT_DICT[\"ground_multi_instruction\"], \"input\": PROMPT_DICT[\"ground_multi_input\"].format_map(input), \"output\": item[\"score\"], \"task\": \"groudness\"})\n",
    "        else: \n",
    "            processed_data.append({\"instruction\": PROMPT_DICT[\"ground_instruction\"], \"input\": PROMPT_DICT[\"ground_input\"].format_map(item['input']), \"output\": item[\"score\"], \"task\": \"groudness\"})\n",
    "    print(processed_data[-1])\n",
    "    print(\"total data number: {}\".format(len(processed_data)))\n",
    "    print(Counter([item[\"output\"] for item in processed_data ]))\n",
    "    return processed_data\n",
    "process_data = create_groundness_data(input_data, multi_retrieval=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_422534/2392507187.py:32: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  llm_chain = LLMChain(llm=model, prompt=few_shot_intepretable_prompt, verbose=False)\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "graph_intepretable =  \"\"\"Based on the reasoning paths, please answer the given question and explain why.\n",
    "Question: {query}\\n\n",
    "Reasoning Paths: {path}\\n\n",
    "\"\"\"\n",
    "# The name of Justin Bieber's brother is Jaxon Bieber. This is based on the reasoning path that connects Justin Bieber to Jaxon Bieber through the relationship of sibling. The other paths that connect Justin Bieber to Jazmyn Bieber or back to Justin Bieber himself are incorrect in this context.\n",
    "few_shot_intepretable_prompt = FewShotPromptTemplate(\n",
    "        examples=[{\n",
    "            \"query\": \"what is the name of justin bieber brother?\",\n",
    "            \"path\": \"[[('Justin Bieber', 'people.person.sibling_s', 'm.0gxnnwc'), ('m.0gxnnwc', 'people.sibling_relationship.sibling', 'Jazmyn Bieber')], [('Justin Bieber', 'people.person.sibling_s', 'm.0gxnnwc'), ('m.0gxnnwc', 'people.sibling_relationship.sibling', 'Justin Bieber')], [('Justin Bieber', 'people.person.sibling_s', 'm.0gxnnwp'), ('m.0gxnnwp', 'people.sibling_relationship.sibling', 'Jaxon Bieber')], [('Justin Bieber', 'people.person.sibling_s', 'm.0gxnnwp'), ('m.0gxnnwp', 'people.sibling_relationship.sibling', 'Justin Bieber')]]\",\n",
    "            \"answer\": \"Jaxon Bieber\",\n",
    "            \"explanation\": \"This is based on the reasoning path that connects Justin Bieber to Jaxon Bieber through the relationship of sibling. The other paths that connect Justin Bieber to Jazmyn Bieber or back to Justin Bieber himself are incorrect in this context.Therefore, the name of Justin Bieber's brother is Jaxon Bieber\"\n",
    "        }],\n",
    "        example_prompt=PromptTemplate.from_template(\"\"\"\n",
    "Question: {query}\\n\n",
    "Reasoning Paths: {path}\\n\n",
    "Answer: {answer}\n",
    "Explanation: {explanation}\n",
    "\"\"\"),\n",
    "        prefix=\n",
    "        \"\"\"Based on the reasoning paths, please answer the given question and explain why.\"\"\",\n",
    "        suffix=\n",
    "        \"\"\"\n",
    "Question: {query}\\n\n",
    "Reasoning Paths: {path}\\n\"\"\",\n",
    "        input_variables=[\"query\", \"path\"],\n",
    ")\n",
    "graph_intepretable_prompt = PromptTemplate(input_variables=[\"query\", \"path\"], template=\n",
    "graph_intepretable)\n",
    "llm_chain = LLMChain(llm=model, prompt=few_shot_intepretable_prompt, verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "本质是reflection的能力，reflection即对于思维和检索的内容进行打分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../output/groudness_webqsp.json\", \"w\") as outfile:\n",
    "        json.dump(process_data, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "random.shuffle(new_data)\n",
    "output_file_name = '../output/testtime1_1105'\n",
    "train_data = new_data[500:]\n",
    "dev_data = new_data[:500]\n",
    "\n",
    "with open(output_file_name + \"_train.json\", \"w\") as outfile:\n",
    "    json.dump(train_data, outfile)\n",
    "with open(output_file_name + \"_dev.json\", \"w\") as outfile:\n",
    "    json.dump(dev_data, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "new_data = []\n",
    "for d in data:\n",
    "    if d['output'] == '[Fully supported]':\n",
    "        if random.random() < 0.2:\n",
    "            new_data.append(d)\n",
    "    else:\n",
    "        new_data.append(d)\n",
    "Counter([item[\"output\"] for item in new_data ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../output/intepretable_answer.json\", \"r\") as f:\n",
    "    data= json.load(f)\n",
    "input_data = []\n",
    "ind = 1\n",
    "count = 0\n",
    "llm_chain = LLMChain(llm=model, prompt=multi_graph_utility_prompt, verbose=True)   \n",
    "while ind < len(relation_data_train):\n",
    "        line = relation_data_train[ind]\n",
    "        topic_entity = line['q_entity']\n",
    "        answer = line['a_entity']\n",
    "        di_graph = build_Digraph(line['graph'])\n",
    "        for chain in id2chain[line['id']]:\n",
    "            if chain:\n",
    "                paths = bfs_with_rule(di_graph, topic_entity[0], chain)\n",
    "                input_data.append({\"query\": line['question'], \"output\": '.'.join(data[count]['text'].split('\\nExplanation: '))})\n",
    "                count += 1\n",
    "        if len(input_data) > 8:\n",
    "            try:\n",
    "                batch_pred = llm_chain.batch(input_data, return_only_outputs=True)\n",
    "            except BadRequestError as e:\n",
    "                print('*************************Bad Request**************')\n",
    "            except ValueError as e:\n",
    "                print(f'******************Value Error {ind}****************************')\n",
    "            input_data = []\n",
    "            break\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference data creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../output/chain_data_top_5.json', 'r') as f:\n",
    "    chain_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7867"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chain_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_start = True\n",
    "generate_data = []\n",
    "for line in chain_data:\n",
    "    if new_start:\n",
    "        starter = '[Retreive New Relationship]'\n",
    "    if line['effective'] == False:\n",
    "        processed_relationship = ';'.join(line['candidate_relation']  + [line['real_relation']])\n",
    "    else:\n",
    "        processed_relationship = ';'.join(line['candidate_relation'])\n",
    "    candidate_entities = line['candidate_entity'][:5] if line['real_entity'] not in line['candidate_entity'] else line['candidate_entity'][:4] + [line['real_entity']]\n",
    "    precessed_triplet = [f\"({line['paths'][line['chain_step']-1][0]}, {line['paths'][line['chain_step']-1][1]}, {entity})\" for entity in candidate_entities]\n",
    "    starter += \"<paragraph>{}</paragraph>\".format(processed_relationship)\n",
    "    starter += 'Retrieved relationship: {}[Fully Relevant][Retrieve Entity]'.format(line['real_relation'])\n",
    "    starter += \"<paragraph>{}</paragraph>\".format(';'.join(precessed_triplet))\n",
    "    starter +=f\"Retrieved triplet: ({line['paths'][line['chain_step']-1][0]}, {line['paths'][line['chain_step']-1][1]}, {line['paths'][line['chain_step']-1][-1]})[Fully Relevant]\"\n",
    "    if line['chain_step'] == len(line['paths']):\n",
    "        starter += f\"[No Retrieval] {';'.join(line['answer'])} [Confidence: 5]\"\n",
    "        generate_data.append({\"instruction\": line['query'], \"output\": starter})\n",
    "        new_start = True\n",
    "        # if len(generate_data) == 10:\n",
    "        #     break\n",
    "    else:\n",
    "        starter += f\"[Continue to Retrieve Evidence]\"\n",
    "        new_start = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../output/test_generate_data_2000.json', 'w') as f:\n",
    "    json.dump(generate_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[New Retrieval] <paragraph> 'people.sibling_relationship.sibling',\n",
    "'people.person.sibling_s',\n",
    "'people.person.parents'</paragraph> Retrieved relationship: people.person.sibling_s[Fully Relevant][Retrieve entity]<paragraph> 'Jazmyn Bieber', 'Justin Bieber', 'Jaxon Bieber'</paragraph> Retrieved triplet: ('Jazmyn Bieber', 'sibling_s', 'Justin Bieber')[Fully Relevant][No Retrieval] Jaxon Bieber [Confidence: 5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:8000/v1\",\n",
    "    api_key=\"token-abc123\",\n",
    "    # skip_special_tokens=False,\n",
    ")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"/media/disk1/chatgpt/LLaMA-Factory/saves/llama3-8b/full/sft/\", \n",
    "  messages=[\n",
    "    {\"role\": \"user\", \"content\": \"You will be provided with a query, evidence, output sentence, and preceding sentences (optional). Your task is to determine whether the information in the output sentence can be fully verified by the evidence or if it requires further external verification. There are three cases:\\n- If the output sentence can be verified solely with the evidence and the preceding sentences, then respond with [No Retrieval]. \\n- If additional information about the tail entity in evidence is needed to verify the output sentence, respond with [Continue to Retrieve Evidence] \\n- If more information unrelated to the evidence is needed, e.g. totally new relationship or new entity, reponse with [New Retrieval].\\n\\nQuery: what city was michael jackson born in?\\nEvidence: \\nOutput: Gary\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "step = 0\n",
    "prediction = ''\n",
    "preceding_sentences = ''\n",
    "evidence = ''\n",
    "i = 0\n",
    "query = train_webqsp['Questions'][i]['RawQuestion']\n",
    "candidate_entity = train_webqsp['Questions'][i]['Parses'][0]['TopicEntityName']\n",
    "while prediction != '[No Retrieval]' and step < 3:\n",
    "    step += 1\n",
    "\n",
    "    current_input = PROMPT_DICT['multi_retrieval_three_way_instruction'] + PROMPT_DICT['multi_retrieval_three_way_input'].format(query=query, evidence=evidence, output=prediction, preceding_sentences=preceding_sentences)\n",
    "    print(current_input)\n",
    "    completion = client.chat.completions.create(\n",
    "      model=\"/media/disk1/chatgpt/LLaMA-Factory/saves/llama3-8b/full/sft/\", \n",
    "    \n",
    "      messages=[\n",
    "        {\"role\": \"user\", \"content\": current_input}\n",
    "      ]\n",
    "    )   \n",
    "    prediction = completion.choices[0].message.content\n",
    "        \n",
    "    if prediction == '[Continue to Retrieve Evidence]':\n",
    "      completion = client.chat.completions.create(\n",
    "        model=\"/media/disk1/chatgpt/LLaMA-Factory/saves/llama3-8b/full/sft/\", \n",
    "      \n",
    "        messages=[\n",
    "          {\"role\": \"user\", \"content\": }\n",
    "        ]\n",
    "      )\n",
    "    prediction = completion.choices[0].message.content\n",
    "      \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ground_instruction', 'ground_input', 'ground_instruction_multi', 'relevance_instruction', 'relevance_input', 'ground_multi_input', 'ground_multi_input_wo_preceding', 'retrieval_instruction', 'retrieval_input', 'retrieval_multi_instruction', 'retrieval_multi_input', 'multi_retrieval_three_way_instruction', 'multi_retrieval_three_way_input', 'multi_retrieval_three_way_input_wo_preceding', 'utility_instruction', 'utility_input'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PROMPT_DICT.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Query: {query}\\nPreceding sentences: {preceding_sentences}\\nEvidence: {evidence}\\nOutput: {target_output}'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PROMPT_DICT['multi_retrieval_three_way_input']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0206"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114196\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "dev_chain_data = []\n",
    "with open('../data/metaqa/metaqa_train_chain_top_5.json', 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        dev_chain_data.append(json.loads(line))\n",
    "print(len(dev_chain_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../output/generate/metaqa_sample.json', 'r') as f:\n",
    "        already_data = json.load(f)\n",
    "already_ids = [d['qid'] for d in already_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "446"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(already_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnotebook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdev_chain_data\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mProcessing\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mncols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1500\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m count \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(dev_chain_data):\n\u001b[1;32m      6\u001b[0m         data \u001b[38;5;241m=\u001b[39m dev_chain_data[count]\n",
      "File \u001b[0;32m~/miniconda3/envs/self-rag/lib/python3.10/site-packages/tqdm/notebook.py:234\u001b[0m, in \u001b[0;36mtqdm_notebook.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    232\u001b[0m unit_scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munit_scale \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munit_scale \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    233\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;241m*\u001b[39m unit_scale \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal\n\u001b[0;32m--> 234\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_printer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mncols\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontainer\u001b[38;5;241m.\u001b[39mpbar \u001b[38;5;241m=\u001b[39m proxy(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplayed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/self-rag/lib/python3.10/site-packages/tqdm/notebook.py:108\u001b[0m, in \u001b[0;36mtqdm_notebook.status_printer\u001b[0;34m(_, total, desc, ncols)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# Fallback to text bar if there's no total\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# DEPRECATED: replaced with an 'info' style bar\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# if not total:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# Prepare IPython progress bar\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m IProgress \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# #187 #451 #558 #872\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(WARN_NOIPYW)\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m total:\n\u001b[1;32m    110\u001b[0m     pbar \u001b[38;5;241m=\u001b[39m IProgress(\u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39mtotal)\n",
      "\u001b[0;31mImportError\u001b[0m: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html"
     ]
    }
   ],
   "source": [
    "outputs = []\n",
    "count = 0\n",
    "from tqdm.notebook import tqdm\n",
    "with tqdm(total=len(dev_chain_data) , desc=f\"Processing\",ncols=1500) as pbar:\n",
    "    while count < len(dev_chain_data):\n",
    "        data = dev_chain_data[count]\n",
    "        query = data['query']\n",
    "        if data['qid'] in already_ids:\n",
    "            count += 1\n",
    "            pbar.update(1)\n",
    "            continue\n",
    "        answer = ', '.join(data['answer'][:10])\n",
    "        content = utility_chain.invoke({'query':query, 'output': answer})['text']\n",
    "        try:\n",
    "            content = eval(content)\n",
    "            individual = content['individual_scores']\n",
    "            overall = content['overall_scores']\n",
    "            outputs.append({\"qid\": data['qid'], \"query\": query, \"answer\": data['answer'], \"scores\": [{\"utility_score\": overall, \"individual_score\": individual}],\"score_type\": 'utility'})\n",
    "            count += 1\n",
    "            pbar.update(1)\n",
    "        except:\n",
    "            print('error')\n",
    "        if count % 10 == 0 and len(outputs):\n",
    "            print(f'Saving {count}')\n",
    "            save_to_json(outputs, './output/generate_data/generate_cwq_utility_0201.json')\n",
    "            outputs = []\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 新的reflection token1\n",
    "few_shot_is_useful = FewShotPromptTemplate(\n",
    "        examples=[{\n",
    "            \"query\": \"What is the name of Justin Bieber's brother?\",\n",
    "            \"output\": \"Jaxon Bieber\",\n",
    "            \"rating\": \"\"\"{{\"individual_scores\": {{\"Jaxon Bieber\": \"[Fully Useful]\"}}, \"overall_scores\": \"[Utility:5]\", \"explanation\": \"The answer Jaxon Bieber provides useful information for the query, so its individual score should be [Fully Useful]. Generally, the query is fully answered, so the overall score should be [Utility:5].\"}}\"\"\"},\n",
    "                  {\n",
    "            \"query\": \"In which year did Donald Trump win the US presidential election?\",\n",
    "            \"output\": \"2024\",\n",
    "            \"rating\": \"\"\"{{\"individual_scores\": {{\"2024\": \"[Fully Useful]\"}}, \"overall_scores\": \"[Utility:4]\", \"explanation\": \"2024 is a correct answer to the query, so its individual score should be [Fully Useful]. Generally, the query is answered correctly, but there should be two correct answers: 2016 and 2024, while the output only provides one. So the overall score should be [Utility:4].\"}}\"\"\"},\n",
    "                  {\n",
    "            \"query\": \"I want to visit Renmin University of China, but I don't know where it is. Tell me its address.\",\n",
    "            \"output\": \"Beijing, Haidian District\",\n",
    "            \"rating\":  \"\"\"{{\"individual_scores\": {{\"Beijing\": \"[Partially Useful]\",\"Haidian District\": \"[Partially Useful]\"}}, \"overall_scores\": \"[Utility:3]\", \"explanation\": \"Both Beijing and Haidian District are correct but too vague to help locate the university campus. So the individual scores should be [Partially Useful], and the overall score should be [Utility:3].\"}}\"\"\"},\n",
    "                  {\n",
    "            \"query\": \"Who was the Governor of Michigan in July 2017?\",\n",
    "            \"output\": \"Gretchen Whitmer\",\n",
    "            \"rating\": \"\"\"{{\"individual_scores\": {{\"Gretchen Whitmer\": \"[Not Useful]\"}}, \"overall_scores\": \"[Utility:2]\", \"explanation\": \"The output is still talking about the governor of Michigan and providing some correct information. But it does not answer who the governor was in July 2017 (Gretchen Whitmer has been serving as the governor from 2019), not providing useful information for the query. So the individual score of Gretchen Whitmer should be [Not Useful], and the overall score should be [Utility:2].\"}}\"\"\"},\n",
    "                  {\n",
    "            \"query\": \"What are the advantages of Python in data analysis?\",\n",
    "            \"output\": \"Guido van Rossum, Portugal\",\n",
    "            \"rating\": \"\"\"{{\"individual_scores\": {{\"Guido van Rossum\": \"[Not Useful]\", \"Portugal\": \"[Not Useful]\"}}, \"overall_scores\": \"[Utility:1]\", \"explanation\": \"The output is completely irrelevant to the query. So the individual scores should be [Not Useful], and the overall score should be [Utility:1].\"}}\"\"\"}],\n",
    "        example_prompt=PromptTemplate.from_template(\"\"\"\n",
    "Query: {query}\n",
    "Answer: {output}\n",
    "Rating: {rating}\n",
    "\"\"\"),\n",
    "        prefix=\"\"\"You will be given a query and the answers, where the answers may consist of one or more individual answers, separated by commas(,). \n",
    "Your task is to generate a **rating** to evaluate whether the answer is a useful response to the query. The rating provide an individual score for each individual answer, and then give an overall score for the entire output.\n",
    "Use the following entailment scale to give individual score(s):\n",
    "[Fully Useful]: The individual answer provides correct and useful information for the query.\n",
    "[Partially Useful]: The individual answer may provide some correct information that is helpful in addressing the query, but it has some flaws, such as lacking specificity.\n",
    "[Not Useful]: The individual answer provides incorrect information or is irrelevant to the query.\n",
    "Use the following entailment scale to give an overall score:\n",
    "[Utility:5]: Generally, the output provides a complete, highly detailed, and informative response to the query, fully satisfying the information needs.\n",
    "[Utility:4]: Generally, the output mostly fulfills the need in the query and provides helpful answers, while there can be some minor improvements, such as discussing more detailed information or providing additional correct answers beyond the current output.\n",
    "[Utility:3]: Generally, the output is correct and acceptable, but there are obvious problems, such as being too vague or not specific enough, limiting its helpfulness in addressing the query. \n",
    "[Utility:2]: Generally, the output still discusses the topic of the query, but it is incorrect or does not actually meet the requirements of the query.\n",
    "[Utility:1]: Generally, the output is completely irrelevant to the query or does not give an answer in the end.\n",
    "\"\"\",\n",
    "        suffix=\n",
    "\"\"\"\n",
    "Query: {query}\n",
    "Answers: {output}\n",
    "Rating:\n",
    " \"\"\",\n",
    "        input_variables=[\"query\", \"output\"],\n",
    ")\n",
    "\n",
    "\n",
    "utility_chain = LLMChain(llm=model, prompt=few_shot_is_useful, verbose=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "self-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
