{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Critic data creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "import os\n",
    "\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"2b219db0d2984f9dae28b651ab8ab3d9\"\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://smsh.openai.azure.com/\"\n",
    "os.environ[\"AZURE_OPENAI_API_VERSION\"] = \"2024-02-01\"\n",
    "os.environ[\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"] = \"gpt-35-turbo\"\n",
    "\n",
    "\n",
    "# os.environ[\"AZURE_OPENAI_API_KEY\"] = \"0c75de50975e4f278b882fe90da47f2f\"\n",
    "# os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://ces.openai.azure.com\"\n",
    "# os.environ[\"AZURE_OPENAI_API_VERSION\"] = \"2024-02-01\"\n",
    "# os.environ[\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"] = \"gpt-35-turbo\"\n",
    "\n",
    "# os.environ[\"AZURE_OPENAI_API_KEY\"] = \"aa183bb914bb4858b15bed161fb47ba5\"\n",
    "# os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://bxcl-prod.openai.azure.com/\"\n",
    "# os.environ[\"AZURE_OPENAI_API_VERSION\"] = \"2024-08-01-preview\"\n",
    "# os.environ[\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"] = \"gpt-4o\"\n",
    "model = AzureChatOpenAI(\n",
    "    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    "    azure_deployment=os.environ[\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"],\n",
    "    temperature=1,\n",
    "    n = 3,\n",
    "    max_retries=5, request_timeout=600\n",
    ")\n",
    "import os\n",
    "from uuid import uuid4\n",
    "\n",
    "# unique_id = uuid4().hex[0:8]\n",
    "# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "# os.environ[\"LANGCHAIN_PROJECT\"] = f\"Tracing Walkthrough - {unique_id}\"\n",
    "# os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "# os.environ[\"LANGCHAIN_API_KEY\"] = \"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\"  \n",
    "\n",
    "# from langsmith import Client\n",
    "\n",
    "# client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"../data/WebQSP.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "\n",
    "multi_graph_three_way = \"\"\"You will be provided with a query, evidence, output sentence, and preceding sentences (optional). Your task is to determine whether the information in the output sentence can be fully verified by the evidence or if it requires further external verification. There are three cases:\\n\n",
    "- If the output sentence can be verified solely with the evidence and the preceding sentences, then respond with [No Retrieval].\n",
    "- If additional information about the tail entity in evidence is needed to verify the output sentence, respond with [Continue to Retrieve Evidence]\n",
    "- If more information unrelated to the evidence is needed, e.g. totally new relationship or new entity, reponse with [New Retrieval]. Please provide explanations for your judgments. \\n\\n\n",
    "###\\nQuery: {query}\\n\n",
    "Preceding sentences: {preceding_sentences}\\n\n",
    "Evidende: {evidence}\n",
    "Output: {target_output}\n",
    "Rating: \n",
    "\"\"\"\n",
    "multi_graph_relevance =  \"\"\"You'll be provided with a query, along with evidence and possibly some preceding sentences. \n",
    "Your job is to determine if the evidence is relevant to the initial query and the preceding context. \n",
    "If the evidence meets this requirement, respond with [Relevant]; otherwise, generate [Irrelevant].\\n\\n\n",
    "###\\nQuery: {query}\\n\n",
    "Preceding sentences: {preceding_sentences}\\n\n",
    "Evidende: {evidence}\n",
    "Rating: \"\"\"\n",
    "\n",
    "multi_graph_groundness =  \"\"\"You'll be provided with a query, along with evidence and possibly some preceding sentences. \n",
    "Your task is to evaluate if the query can be fully supported by the information provided in the evidence, and provide explanations on your judgement.\n",
    "Use the following entailment scale to generate a score:\n",
    "[Fully supported] - All information needed is supported by the evidence. \n",
    "[Partially supported] - The query is supported by the evidence to some extent, but there is major information that is not discussed in the evidence. For example, if the query asks about two concepts and the evidence only discusses either of them, it should be considered a [Partially supported].\n",
    "[No support / Contradictory] - The query is completely unrelated to the evidence, or contradicts the evidence.\n",
    "###\\nQuery: {query}\\n\n",
    "Preceding sentences: {preceding_sentences}\\n\n",
    "Evidende: {evidence}\n",
    "Rating: \n",
    "\"\"\"\n",
    "\n",
    "multi_graph_utility = \"\"\"\n",
    "Given a query and an output, rate whether the response appears to be a helpful and informative answer to the query, from 1 (lowest) - 5 (highest). We call this score perceived utility.\n",
    "\n",
    "The detailed criterion is as follows:\n",
    "5: The response provides a complete, highly detailed, and informative response to the query, fully satisfying the information needs.\n",
    "4: The response mostly fulfills the need in the query, while there can be some minor improvements such as discussing more detailed information, having better structure of the response, or improving coherence.\n",
    "3: The response is acceptable, but some major additions or improvements are needed to satisfy users' needs.\n",
    "2: The response still addresses the main request, but it is not complete or not relevant to the query.\n",
    "1: The response is barely on-topic or completely irrelevant.\n",
    "##\n",
    "\n",
    "Query: Who is the current prime minister of the UK as of 2023?\n",
    "Output: Boris Johnson was the prime minister of the UK from 2019 - 2022.\n",
    "Perceived utility: 2\n",
    "Explanation: While the output provides a factually correct statement about the UK prime minister from 2019 to 2022, this instruction asks who the prime minister is as of 2023, so it doesn't answer the instruction. Therefore, the utility is 2.\n",
    "\n",
    "##\n",
    "\n",
    "Query: Given a description of a travel destination, recommend 10 tourist attractions with detailed explanations of each. The travel destination is Tokyo, Japan.\n",
    "Output: 'Tokyo is a vibrant city full of exciting tourist attractions. Some must-see sights include the Tokyo Skytree, Tokyo Disneyland, Sensoji Temple, Meiji Shrine, Tsukiji Fish Market, Harajuku, and Shinjuku Gyoen.\n",
    "Perceived utility: 3\n",
    "Explanation: This output doesn't provide descriptions of each attraction and the number of the attractions is also less than 10. While this output partially answers the instructions, it doesn't match the instructions strictly.\n",
    "\n",
    "##\n",
    "\n",
    "Query: {query}\n",
    "Output:{output}\n",
    "\"\"\"\n",
    "\n",
    "multi_graph_three_way_prompt = PromptTemplate(input_variables=[\"query\", \"preceding_sentences\", \"evidence\", \"target_output\"], template=\n",
    "multi_graph_three_way)\n",
    "multi_graph_relevance_prmpt = PromptTemplate(input_variables=[\"query\", \"preceding_sentences\", \"evidence\"], template=\n",
    "multi_graph_relevance)\n",
    "multi_graph_groundness_prompt = PromptTemplate(input_variables=[\"query\", \"preceding_sentences\", \"evidence\",], template=\n",
    "multi_graph_groundness)\n",
    "multi_graph_utility_prompt = PromptTemplate(input_variables=[\"query\", \"output\"], template=multi_graph_utility)\n",
    "# prompt_template = FewShotPromptTemplate(\n",
    "#         examples=examples_dict,\n",
    "#         example_prompt=examples_prompt,\n",
    "#         prefix=\n",
    "#         \"\"\"\n",
    "# Based on the Table below, your task is to accurately output columns related to the query or contain useful information about the query. This process involves linking similar words or semantically similar terms to columns in the Table.\n",
    "# Approach this task as follows:\n",
    "# Read the query and extra information thoroughly and list every possible link from query term to column in the Table. \n",
    "# Then based on the column linking, output all useful columns at last. Make sure all columns in the linking step are included and every column is in the Table.\"\"\",\n",
    "#         suffix=\n",
    "#         \"\"\"\n",
    "# Table: {table}\n",
    "# Extra information: {aug}\n",
    "\n",
    "# Query: {claim}\"\"\",\n",
    "#         input_variables=[\"table\", \"claim\", \"aug\"],\n",
    "# )\n",
    "# \"[Continue to Retrieve Evidence]\\n\\nExplanation: The evidence provided only mentions Viggo Mortensen as a film actor, but does not provide any information about his role as Aragorn. To verify if Aragorn influenced Samuel Taylor Coleridge, we would need additional information about Viggo Mortensen's portrayal of Aragorn in a specific context that could have influenced Coleridge.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(llm=model, prompt=multi_graph_relevance_prmpt, verbose=True)\n",
    "batch_pred = llm_chain.batch([{\"query\": data[0]['RawQuestion'], \"preceding_sentences\": \"\", \"evidence\": \" relationship: influence.influence_node.influenced_by\",}], return_only_outputs=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_pred[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(llm=model, prompt=examples_prompt, verbose=True)\n",
    "batch_pred = llm_chain.batch([{\"query\": data[0]['RawQuestion'], \"preceding_sentences\": \"\", \"evidence\": \"(Viggo Mortensen, film.actor.film, Unknown_Name)\", \"target_output\": \"Aragorn\"}], return_only_outputs=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_DICT = {\n",
    "    \"ground_instruction\": (\n",
    "        \"You will be given a query and evidence. Your objective is to assess the extent to which the query is supported by the information presented in the evidence.\\n\"\n",
    "        \"Rate the level of support from [Ignore / Contradictory], [Little support], [Partially supported], [Mostly supported], [Fully supported].\"\n",
    "    ),\n",
    "    \"ground_input\": (\n",
    "        \"Query: {query}\\n\"\n",
    "        \"Evidence: {evidence}\\n\"\n",
    "    ),\n",
    "    \"ground_instruction_multi\": (\n",
    "        \"You will receive a query, evidence, and output, and optional preceding sentences. Your task is to evaluate whether the output filters out valid information from the evidence to answer the given query, and provide explanations on your judgement\\n\"\n",
    "        \"Use the following entailment scale to generate a score:\\n\"\n",
    "        \"[Fully supported] - the output contains sufficient information needed to answer the query, fully satisfying the information needs.\\n\"\n",
    "        \"[Partially supported] - The output contains information to some extent, but there is major information in evidence that is not discussed in the output. For example, if a query asks about two concepts and the output only discusses either of them, it should be considered a [Partially supported].\\n\" \n",
    "        \"[No support / Contradictory] - The output completely ignores evidence, or is unrelated to the query.\\n\\n\"\n",
    "        # \"Make sure to not use any external information/knowledge to judge whether the output is true or not.\\n\\n\"\n",
    "    ),\n",
    "    \"relevance_instruction\": (\n",
    "        \"You will receive a query, topic entity, evidence and optional preceding sentences containing history information. The evidence contains graph relationships or entities may be useful to answer the query. Your task is to filters out 3 valid information from the evidence that contribute to answering the query and provide a relevance score for each output.\\n\"\n",
    "        \"The score of relevance range from [Fully relavant], [Partially relevant] to [Unrelevant].\"\n",
    "    ),\n",
    "    \"relevance_input\": (\n",
    "        \"Query: {query}\\n\"\n",
    "        \"Topic entity: {topic_entity}\\n\"\n",
    "        \"Evidence: {evidence}\"\n",
    "        \"Preceding sentences: {preceding_sentences}\"\n",
    "    ),\n",
    "    \"ground_multi_input\": (\n",
    "        \"Task instruction: {instruction}\\n\"\n",
    "        \"Preceding sentences: {preceding_sentences}\\n\"\n",
    "        \"Output: {target_output}\\n\"\n",
    "        \"Evidence: {evidence}\"\n",
    "    ),\n",
    "    \"ground_multi_input_wo_preceding\": (\n",
    "        \"Task instruction: {instruction}\\n\"\n",
    "        \"Output: {target_output}\\n\"\n",
    "        \"Evidence: {evidence}\"\n",
    "    ),\n",
    "    \"retrieval_multi_instruction\": (\n",
    "        \"You will be provided with an instruction, evidence, output sentence, and preceding sentences (optional). If the preceding sentence is given, the output should be the sentence that follows those preceding sentences.  Your task is to determine whether the information in the output sentence can be fully verified by the evidence or if it requires further external verification. If the output sentence can be verified solely with the evidence or doesn’t require any verification, respond with [No Retrieval]. If additional information is needed to verify the output sentence, respond with [Retrieval]. Please provide explanations for your judgments.\\n\\n\" \n",
    "    ),\n",
    "    \"retrieval_multi_input\": (\n",
    "        \"Task instruction: {instruction}\\n\"\n",
    "        \"Preceding sentences: {preceding_sentences}\\n\"\n",
    "        \"Evidence: {evidence}\\n\"\n",
    "        \"Output: {target_output}\"\n",
    "    ),\n",
    "    \"multi_retrieval_three_way_instruction\": (\n",
    "        \"You will be provided with a query, evidence, output sentence, and preceding sentences (optional). Your task is to determine whether the information in the output sentence can be fully verified by the evidence or if it requires further external verification. There are three cases:\\n\" \n",
    "        \"- If the output sentence can be verified solely with the evidence and the preceding sentences, then respond with [No Retrieval]. \\n\"\n",
    "        \"- If additional information about the tail entity in evidence is needed to verify the output sentence, respond with [Continue to Retrieve Evidence] \\n\"\n",
    "        \"- If more information unrelated to the evidence is needed, e.g. totally new relationship or new entity, reponse with [New Retrieval].\\n\\n\"\n",
    "    ),\n",
    "    \"multi_retrieval_three_way_input\": (\n",
    "        \"Query: {query}\\n\"\n",
    "        \"Preceding sentences: {preceding_sentences}\\n\"\n",
    "        \"Evidence: {evidence}\\n\"\n",
    "        \"Output: {target_output}\"\n",
    "    ),\n",
    "     \"multi_retrieval_three_way_input_wo_preceding\": (\n",
    "        \"Query: {query}\\n\"\n",
    "        \"Evidence: {evidence}\\n\"\n",
    "        \"Output: {target_output}\"\n",
    "    ),\n",
    "    \"utility_instruction\": (\n",
    "        \"Given an instruction and an output, rate whether the response appears to be a helpful and informative answer to the query, from 1 (lowest) - 5 (highest). We call this score perceived utility.\\n\"\n",
    "        \"[Utility:5]: The response provides a complete, highly detailed, and informative response to the query, fully satisfying the information needs.\\n\"\n",
    "        \"[Utility:4]: The response mostly fulfills the need in the query, while there can be some minor improvements such as discussing more detailed information, having better structure of the response, or improving coherence. \\n\"\n",
    "        \"[Utility:3]: The response is acceptable, but some major additions or improvements are needed to satisfy users' needs.\\n\"\n",
    "        \"[Utility:2]: The response still addresses the main request, but it is not complete or not relevant to the query.\\n\"\n",
    "        \"[Utility:1]: The response is barely on-topic or completely irrelevant.\\n\"\n",
    "    ),\n",
    "    \"utility_input\": (\n",
    "        \"Task instruction: {instruction}\\n\"\n",
    "        \"Output: {output}\"\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"../data/WebQSP.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "input_data = []\n",
    "for line in data:\n",
    "    candidate_answer = []\n",
    "    relation_paths = []\n",
    "    continue_paths = []\n",
    "    for p in line['Parses']:\n",
    "        candidate_answer.extend([ans[\"EntityName\"] for ans in p['Answers'] if ans[\"EntityName\"] and ans[\"EntityName\"] not in candidate_answer ])\n",
    "        if p['InferentialChain'] and p['InferentialChain'] not in relation_paths:\n",
    "            relation_paths.append(p['InferentialChain'])\n",
    "    for path in relation_paths:\n",
    "        if len(path) > 1:\n",
    "            continue_paths.append([path[:i] for i in range(len(path))])\n",
    "    input_data.append({\"input\": {\"query\": line['RawQuestion'], \"preceding_sentences\": \"\", \"evidence\": \"\", \"target_output\": ';'.join(candidate_answer)}, \"decision_token\": \"[New Retrieval]\"})\n",
    "    for c in continue_paths:\n",
    "        if len(c) == 1:\n",
    "            input_data.append({\"input\": {\"query\": line['RawQuestion'], \"preceding_sentences\": \"\", \"evidence\": f\"Relations retrieved: {c[-1]}\", \"target_output\": ';'.join(candidate_answer)}, \"decision_token\": \"[Continue to Retrieve Evidence]\"})\n",
    "        else:\n",
    "            input_data.append({\"input\": {\"query\": line['RawQuestion'], \"preceding_sentences\": f\"Relations retrieved: {c[:-1]}\", \"evidence\": f\"Relations retrieved: {c[-1]}\", \"target_output\": ';'.join(candidate_answer)}, \"decision_token\": \"[Continue to Retrieve Evidence]\"})\n",
    "    #生成relation path，根据relation path构造\n",
    "    for p in relation_paths:\n",
    "        if len(p) ==1:\n",
    "            input_data.append({\"input\": {\"query\": line['RawQuestion'], \"preceding_sentences\": \"\", \"evidence\": f\"{p[-1]}\", \"target_output\": ';'.join(candidate_answer)}, \"decision_token\": \"[No Retrieval]\"})\n",
    "        else:\n",
    "            input_data.append({\"input\": {\"query\": line['RawQuestion'], \"preceding_sentences\": f\"Relations retrieved: {p[:-1]}\", \"evidence\": f\"Relations retrieved: {p[-1]}\", \"target_output\": ';'.join(candidate_answer)}, \"decision_token\": \"[No Retrieval]\"})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def create_retrieval_data(input_data, multi_retrieval=False):\n",
    "    print(\"creating multi sentence retrieval data\")\n",
    "    processed_data = []\n",
    "    for item in input_data:\n",
    "        input = item[\"input\"]\n",
    "        output = item[\"decision_token\"]\n",
    "        if len(str(output)) == 0:\n",
    "            continue\n",
    "        if output not in [ \"[New Retrieval]\", \"[Continue to Retrieve Evidence]\", \"[No Retrieval]\"]:\n",
    "            continue\n",
    "\n",
    "        if len(input[\"preceding_sentences\"]) == 0:\n",
    "            processed_data.append({\"instruction\": PROMPT_DICT[\"multi_retrieval_three_way_instruction\"], \"input\": PROMPT_DICT[\"multi_retrieval_three_way_input_wo_preceding\"].format_map(input), \"output\": output, \"task\": \"multi_retrieval\"})\n",
    "        else:\n",
    "            processed_data.append({\"instruction\": PROMPT_DICT[\"multi_retrieval_three_way_instruction\"], \"input\": PROMPT_DICT[\"multi_retrieval_three_way_input\"].format_map(input), \"output\": output, \"task\": \"retrieval\"})\n",
    "            \n",
    "    print(processed_data[-1])\n",
    "    print(\"total data number: {}\".format(len(processed_data)))\n",
    "    print(Counter([item[\"output\"] for item in processed_data ]))\n",
    "    return processed_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "from collections import Counter\n",
    "def create_relevance_data(input_data):\n",
    "    print(\"creating relevance data\")\n",
    "    processed_data = []\n",
    "    for item in input_data:\n",
    "        output = ''\n",
    "        match_dict= item[\"match\"]\n",
    "        for r, s in match_dict.items():\n",
    "            output += f\"{r}:{s}; \"\n",
    "        # if label == \"[Relevant]\" and random.random() > 0.7:\n",
    "        #     continue\n",
    "        processed_data.append({\"instruction\": PROMPT_DICT[\"relevance_instruction\"], \"input\": PROMPT_DICT[\"relevance_input\"].format_map(item['input']), \"output\": output, \"task\": \"relevance\"})\n",
    "    print(processed_data[-1])\n",
    "    print(\"total data number: {}\".format(len(processed_data)))\n",
    "    # print(Counter([item[\"output\"] for item in processed_data ]))\n",
    "    return processed_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_groundness_data(input_data, multi_retrieval=False):\n",
    "    print(\"creating groundness data\")\n",
    "    processed_data = []\n",
    "    for item in input_data:\n",
    "        if multi_retrieval is True:\n",
    "            if \"sent_idx\" not in item or item[\"sent_idx\"] == 0 or len(item[\"preceding_sentences\"]) == 0:\n",
    "                processed_data.append({\"instruction\": PROMPT_DICT[\"ground_multi_instruction\"], \"input\": PROMPT_DICT[\"ground_multi_input_wo_preceding\"].format_map(input), \"output\": item[\"score\"], \"task\": \"groudness\"})\n",
    "            else:\n",
    "                processed_data.append({\"instruction\": PROMPT_DICT[\"ground_multi_instruction\"], \"input\": PROMPT_DICT[\"ground_multi_input\"].format_map(input), \"output\": item[\"score\"], \"task\": \"groudness\"})\n",
    "        else: \n",
    "            processed_data.append({\"instruction\": PROMPT_DICT[\"ground_instruction\"], \"input\": PROMPT_DICT[\"ground_input\"].format_map(item['input']), \"output\": item[\"score\"], \"task\": \"groudness\"})\n",
    "    print(processed_data[-1])\n",
    "    print(\"total data number: {}\".format(len(processed_data)))\n",
    "    print(Counter([item[\"output\"] for item in processed_data ]))\n",
    "    return processed_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### complete chain create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"2b219db0d2984f9dae28b651ab8ab3d9\"\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://smsh.openai.azure.com/\"\n",
    "os.environ[\"AZURE_OPENAI_API_VERSION\"] = \"2024-03-01-preview\"\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",\n",
    ")\n",
    "\n",
    "from langchain.storage import LocalFileStore, RedisStore\n",
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "store = RedisStore(redis_url=\"redis://localhost:6379\")\n",
    "cached_embedder = CacheBackedEmbeddings.from_bytes_store(\n",
    "embeddings, store, namespace=\"openai\"\n",
    ")\n",
    "row_string = []\n",
    "with open('../data/clean_relations', 'r') as f:\n",
    "    data = f.readlines()\n",
    "db = FAISS.from_texts(data, cached_embedder)\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'geography.island.island_group;geography.island.body_of_water;location.country.form_of_government;base.biblioness.bibs_location.country;base.locations.continents.countries_within'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "';'.join([page.page_content.strip() for page in retriever.invoke('what country is the grand bahama island in?')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/media/disk1/chatgpt/zh/graph_data')\n",
    "from src.graph_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import networkx as nx\n",
    "from collections import deque\n",
    "import walker\n",
    "import json\n",
    "def build_Digraph(graph: list) -> nx.Graph:\n",
    "    G = nx.DiGraph()\n",
    "    for triplet in graph:\n",
    "        h, r, t = triplet\n",
    "        G.add_edge(h, t, relation=r.strip())\n",
    "    return G\n",
    "\n",
    "def bfs_with_rule(graph, start_node, target_rule, max_p = 10):\n",
    "    result_paths = []\n",
    "    queue = deque([(start_node, [])])\n",
    "    while queue:\n",
    "        current_node, current_path = queue.popleft()\n",
    "        if len(current_path) == len(target_rule):\n",
    "            result_paths.append(current_path)\n",
    "        if len(current_path) < len(target_rule):\n",
    "            if current_node not in graph:\n",
    "                continue\n",
    "            for neighbor in graph.neighbors(current_node):\n",
    "                rel = graph[current_node][neighbor]['relation']\n",
    "                if rel != target_rule[len(current_path)] or len(current_path) > len(target_rule):\n",
    "                    continue\n",
    "                queue.append((neighbor, current_path + [(current_node, rel,neighbor)]))\n",
    "            for neighbor in graph.predecessors(current_node):\n",
    "                rel = graph[neighbor][current_node]['relation']\n",
    "                if rel != target_rule[len(current_path)] or len(current_path) > len(target_rule):\n",
    "                    continue\n",
    "                queue.append((neighbor, current_path + [(current_node, rel,neighbor)]))\n",
    "    \n",
    "    return result_paths\n",
    "# relation_data_train = datasets.load_dataset('rmanluo/RoG-webqsp', split='train')\n",
    "\n",
    "relation_data_train = datasets.load_dataset('rmanluo/RoG-webqsp', split='test')\n",
    "\n",
    "# llm_chain = LLMChain(llm=model, prompt=multi_graph_groundness_prompt, verbose=True)\n",
    "# batch_pred = llm_chain.batch([{\"query\": data[0]['RawQuestion'], \"preceding_sentences\": \"\", \"evidence\": \" relationship: influence.influence_node.influenced_by\",}], return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "def save_to_json(data: List, data_path='../output/chain_data.json'):\n",
    "    if not os.path.isfile(data_path):\n",
    "        # 文件不存在，创建新列表并写入文件\n",
    "        with open(data_path, 'w', encoding='utf-8') as file:\n",
    "            json.dump(data, file, ensure_ascii=False, indent=4)\n",
    "        return\n",
    "    try:\n",
    "        # 尝试读取现有文件\n",
    "        with open(data_path, 'r', encoding='utf-8') as file:\n",
    "            # 加载现有的JSON数据\n",
    "            existing_data = json.load(file)\n",
    "            existing_data.extend(data)\n",
    "    except json.JSONDecodeError:\n",
    "        # 文件不是有效的JSON，打印错误信息并退出\n",
    "        print(f\"文件 {data_path} 不是有效的JSON格式。\")\n",
    "        return\n",
    "    except ValueError as e:\n",
    "        # 打印错误信息并退出\n",
    "        print(e)\n",
    "        return\n",
    "    # 将更新后的数据写回文件\n",
    "    with open(data_path, 'w', encoding='utf-8') as file:\n",
    "        json.dump(existing_data, file, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = relation_data_train[1283]\n",
    "line = data\n",
    "id = line['id']\n",
    "topic_entity = line['q_entity']\n",
    "answer = line['a_entity']\n",
    "di_graph = build_Digraph(line['graph'])\n",
    "paths = get_truth_paths(topic_entity, answer, di_graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Appalachian Mountains', 'location.location.containedby', 'North America')]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import random\n",
    "import json\n",
    "import multiprocessing as mp\n",
    "def process_relation_data(line):\n",
    "    chain_data = []\n",
    "    id = line['id']\n",
    "    topic_entity = line['q_entity']\n",
    "    answer = line['a_entity']\n",
    "    di_graph = build_Digraph(line['graph'])\n",
    "    paths = get_truth_paths(topic_entity, answer, di_graph)\n",
    "    # every question sample at most 3 paths\n",
    "    paths = random.sample(paths, min(3, len(paths)))\n",
    "    sent_idx = 0\n",
    "    for pid, p in enumerate(paths):\n",
    "        for pidx, step in enumerate(p):\n",
    "            real_relation = step[1]\n",
    "            real_entity = step[2]\n",
    "            candidate_entities = [tail for head, tail in di_graph.out_edges(step[0]) if di_graph[head][tail].get('relation') == step[1]]\n",
    "            candidate_relation = [page.page_content.strip() for page in retriever.invoke(line['question'] + real_relation + real_entity)]\n",
    "            if step[1] in candidate_relation:\n",
    "                chain_data.append({\n",
    "                    \"sent_idx\": sent_idx,\n",
    "                    \"chain_step\": pidx + 1,\n",
    "                    \"candidate_relation\": candidate_relation,\n",
    "                    \"candidate_entity\": candidate_entities,\n",
    "                    \"real_relation\": real_relation,\n",
    "                    \"real_entity\": real_entity,\n",
    "                    \"paths\": p,\n",
    "                    \"effective\": True\n",
    "                })\n",
    "            else:\n",
    "                chain_data.append({\n",
    "                    \"sent_idx\": sent_idx,\n",
    "                    \"chain_step\": pidx + 1,\n",
    "                    \"candidate_relation\": candidate_relation,\n",
    "                    \"candidate_entity\": candidate_entities,\n",
    "                    \"real_relation\": real_relation,\n",
    "                    \"real_entity\": real_entity,\n",
    "                    \"paths\": p,\n",
    "                    \"effective\": False\n",
    "                })\n",
    "            sent_idx += 1\n",
    "    return {\"qid\": id, \"query\": line['question'], \"topic_entity\": topic_entity, \"answer\": answer, \"chains\": chain_data}\n",
    "\n",
    "# processed_data = relation_data_train.shard(num_shards=4, index=2).map(process_relation_data, num_proc=16, remove_columns=relation_data_train.column_names)\n",
    "# processed_data.to_json('../output/cwq_train_chain_top_5_shard_3.json')\n",
    "# import multiprocess\n",
    "# multiprocess.set_start_method(\"spawn\", force=True)\n",
    "processed_data = relation_data_train.shard(num_shards=4, index=3).map(process_relation_data, num_proc=8, remove_columns=relation_data_train.column_names)\n",
    "processed_data.to_json('../output/cwq_train_chain_top_5_shard_4.json')\n",
    "        # with lock:\n",
    "        #     save_to_json(chain_data, f'../output/async_data_top_5_cwp.json')\n",
    "# process_relation_data(relation_data_train.shard(num_shards=4, index=0))\n",
    "# num_proc = 4\n",
    "# with mp.Pool() as pool:\n",
    "#     results = pool.imap_unordered(process_relation_data, [relation_data_train.shard(num_shards=num_proc, index=0)]) \n",
    "# for result in results:\n",
    "#     # 处理结果\n",
    "#     print(len(result))\n",
    "# webqsp_chain = relation_data_train.map(process_relation_data, num_proc=4).remove_columns(['graph'])\n",
    "\n",
    "# # 创建线程池\n",
    "# m = multiprocessing.Manager()\n",
    "# lock = m.Lock()\n",
    "# with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:\n",
    "#     # 提交任务到线程池\n",
    "#     for index, line in enumerate(relation_data):\n",
    "#         executor.submit(process_relation_data, line, lock)\n",
    "\n",
    "# # 等待所有任务完成\n",
    "# executor.shutdown()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../output/chain_data.json', 'r', encoding='utf-8') as file:\n",
    "    chain_data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_relation_data(line):\n",
    "    chains = []\n",
    "    id = line['id']\n",
    "    topic_entity = line['q_entity']\n",
    "    answer = line['a_entity']\n",
    "    di_graph = build_Digraph(line['graph'])\n",
    "    paths = get_truth_paths(topic_entity, answer, di_graph)\n",
    "    # every question sample at most 3 paths\n",
    "    paths = random.sample(paths, min(3, len(paths)))\n",
    "    sent_idx = 0\n",
    "    for pid, p in enumerate(paths):\n",
    "        chain_data = []\n",
    "        for pidx, step in enumerate(p):\n",
    "            real_relation = step[1]\n",
    "            real_entity = step[2]\n",
    "            candidate_entities = [tail for head, tail in di_graph.out_edges(step[0]) if di_graph[head][tail].get('relation') == step[1]]\n",
    "            candidate_relation = [page.page_content.strip() for page in retriever.invoke(line['question'] + real_relation + real_entity)]\n",
    "            if step[1] in candidate_relation:\n",
    "                chain_data.append({\n",
    "                    \"p_id\": pid,\n",
    "                    \"chain_step\": pidx + 1,\n",
    "                    \"candidate_relation\": candidate_relation,\n",
    "                    \"candidate_entity\": candidate_entities,\n",
    "                    \"real_relation\": real_relation,\n",
    "                    \"real_entity\": real_entity,\n",
    "                    \"paths\": p,\n",
    "                    \"effective\": True\n",
    "                })\n",
    "            else:\n",
    "                chain_data.append({\n",
    "                    \"p_id\": pid,\n",
    "                    \"chain_step\": pidx + 1, \n",
    "                    \"candidate_relation\": candidate_relation,\n",
    "                    \"candidate_entity\": candidate_entities,\n",
    "                    \"real_relation\": real_relation,\n",
    "                    \"real_entity\": real_entity,\n",
    "                    \"paths\": p,\n",
    "                    \"effective\": False\n",
    "                })\n",
    "    chains.append(chain_data)\n",
    "    return {\"qid\": id, \"query\": line['question'], \"topic_entity\": topic_entity, \"answer\": answer, \"chains\": chains}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create Retrieval data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "input_data = []\n",
    "\n",
    "for s, line in enumerate(chain_data):\n",
    "    random.seed(s + 1)\n",
    "    rand_num = random.random()\n",
    "    if line['sent_idx'] == 0:\n",
    "        if rand_num < 0.5:\n",
    "            input_data.append({\"input\": {\"query\": line['query'], \"preceding_sentences\": \"\", \"evidence\": \"\", \"target_output\": ';'.join(line['answer'])}, \"decision_token\": \"[New Retrieval]\"})\n",
    "            continue\n",
    "    step = line['chain_step']\n",
    "    if len(line['paths']) == 1:\n",
    "        assert line['chain_step'] == 1\n",
    "        if rand_num < 0.6:\n",
    "            input_data.append({\"input\": {\"query\": line['query'], \"preceding_sentences\": \"\", \"evidence\": f\"Relations retrieved: {line['real_relation']}\\n Entities retrieved: {line['real_entity']}\", \"target_output\": ';'.join(line['answer'])}, \"decision_token\": \"[No Retrieval]\"})\n",
    "    else:\n",
    "        if step == 1:\n",
    "            if rand_num < 0.9:\n",
    "                input_data.append({\"input\": {\"query\": line['query'], \"preceding_sentences\": \"\", \"evidence\": f\"Relations retrieved: {line['real_relation']}\\n Entities retrieved: {line['real_entity']}\", \"target_output\": ';'.join(line['answer'])}, \"decision_token\": \"[Continue to Retrieve Evidence]\"})\n",
    "        elif step < len(line['paths']):\n",
    "            if rand_num < 0.8:\n",
    "                input_data.append({\"input\": {\"query\": line['query'], \"preceding_sentences\": f\"Information retrieved: {line['paths'][:step]}\", \"evidence\": f\"Relations retrieved: {line['real_relation']}\\n Entities retrieved: {line['real_entity']}\", \"target_output\": ';'.join(line['answer'])}, \"decision_token\": \"[Continue to Retrieve Evidence]\"})\n",
    "        else:\n",
    "            if rand_num < 0.4:\n",
    "                input_data.append({\"input\": {\"query\": line['query'], \"preceding_sentences\": f\"Information retrieved: {line['paths'][:step]}\", \"evidence\": f\"Relations retrieved: {line['real_relation']}\\n Entities retrieved: {line['real_entity']}\", \"target_output\": ';'.join(line['answer'])}, \"decision_token\": \"[No Retrieval]\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating multi sentence retrieval data\n",
      "{'instruction': 'You will be provided with a query, evidence, output sentence, and preceding sentences (optional). Your task is to determine whether the information in the output sentence can be fully verified by the evidence or if it requires further external verification. There are three cases:\\n- If the output sentence can be verified solely with the evidence and the preceding sentences, then respond with [No Retrieval]. \\n- If additional information about the tail entity in evidence is needed to verify the output sentence, respond with [Continue to Retrieve Evidence] \\n- If more information unrelated to the evidence is needed, e.g. totally new relationship or new entity, reponse with [New Retrieval].\\n\\n', 'input': 'Query: who played alf on the tv show\\nEvidence: \\nOutput: Paul Fusco', 'output': '[New Retrieval]', 'task': 'multi_retrieval'}\n",
      "total data number: 6498\n",
      "Counter({'[Continue to Retrieve Evidence]': 3241, '[No Retrieval]': 2946, '[New Retrieval]': 311})\n"
     ]
    }
   ],
   "source": [
    "retrival_critic = create_retrieval_data(input_data=input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create Relevant data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_intepretable =  \"\"\"Based on the reasoning paths, please answer the given question and explain why.\n",
    "Question: {query}\\n\n",
    "Reasoning Paths: {path}\\n\n",
    "\"\"\"\n",
    "# The name of Justin Bieber's brother is Jaxon Bieber. This is based on the reasoning path that connects Justin Bieber to Jaxon Bieber through the relationship of sibling. The other paths that connect Justin Bieber to Jazmyn Bieber or back to Justin Bieber himself are incorrect in this context.\n",
    "few_shot_intepretable_prompt = FewShotPromptTemplate(\n",
    "        examples=[{\n",
    "            \"query\": \"what is the name of justin bieber brother?\",\n",
    "            \"path\": \"[[('Justin Bieber', 'people.person.sibling_s', 'm.0gxnnwc'), ('m.0gxnnwc', 'people.sibling_relationship.sibling', 'Jazmyn Bieber')], [('Justin Bieber', 'people.person.sibling_s', 'm.0gxnnwc'), ('m.0gxnnwc', 'people.sibling_relationship.sibling', 'Justin Bieber')], [('Justin Bieber', 'people.person.sibling_s', 'm.0gxnnwp'), ('m.0gxnnwp', 'people.sibling_relationship.sibling', 'Jaxon Bieber')], [('Justin Bieber', 'people.person.sibling_s', 'm.0gxnnwp'), ('m.0gxnnwp', 'people.sibling_relationship.sibling', 'Justin Bieber')]]\",\n",
    "            \"answer\": \"Jaxon Bieber\",\n",
    "            \"explanation\": \"This is based on the reasoning path that connects Justin Bieber to Jaxon Bieber through the relationship of sibling. The other paths that connect Justin Bieber to Jazmyn Bieber or back to Justin Bieber himself are incorrect in this context.Therefore, the name of Justin Bieber's brother is Jaxon Bieber\"\n",
    "        }],\n",
    "        example_prompt=PromptTemplate.from_template(\"\"\"\n",
    "Question: {query}\\n\n",
    "Reasoning Paths: {path}\\n\n",
    "Answer: {answer}\n",
    "Explanation: {explanation}\n",
    "\"\"\"),\n",
    "        prefix=\n",
    "        \"\"\"Based on the reasoning paths, please answer the given question and explain why.\"\"\",\n",
    "        suffix=\n",
    "        \"\"\"\n",
    "Question: {query}\\n\n",
    "Reasoning Paths: {path}\\n\"\"\",\n",
    "        input_variables=[\"query\", \"path\"],\n",
    ")\n",
    "graph_intepretable_prompt = PromptTemplate(input_variables=[\"query\", \"path\"], template=\n",
    "graph_intepretable)\n",
    "llm_chain = LLMChain(llm=model, prompt=few_shot_intepretable_prompt, verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = []\n",
    "for line in chain_data:\n",
    "    if line['effective'] == True:\n",
    "        input_data.append({\"input\": {\"query\": line['query'], \"evidence\": f\"Relations retrieved: {';'.join(line['candidate_relation'])}\"}, \"decision_token\": '[Relevant]'})\n",
    "    else:\n",
    "        input_data.append({\"input\": {\"query\": line['query'], \"evidence\": f\"Relations retrieved: {';'.join(line['candidate_relation'])}\"}, \"decision_token\": '[Irrelevant]'})\n",
    "create_relevance_data(input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create Support groundness data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_intepretable =  \"\"\"You will receive a query, evidence and optional preceding sentences containing history information. The evidence contains graph relationships or entities may be useful to answer the query. Your task is to filters out valid information from the evidence to answer the given query, evaluate your output and provide explanations on your result.\n",
    "\n",
    "###\n",
    "Query: Name the president of the country whose main spoken language was Brahui in 1980?\n",
    "Topic Entity: Brahui Language\n",
    "Evidence: language.human_language.main_country; language.human_language.language_family; language.human_language.iso_639_3_code; base.rosetta.languoid.parent; language.human_language.writing_system; base.rosetta.languoid.languoid_class; language.human_language.countries_spoken_in; kg.object_profile.prominent_type; base.rosetta.languoid.document; base.ontologies.ontology_instance.equivalent_instances; base.rosetta.languoid.local_name; language.human_language.region\n",
    "Preceding sentences: \n",
    "Output: \n",
    "1. {{language.human_language.main_country (Score: Fully relavant))}}: This relation is highly relevant as it directly relates to the country whose president is being asked for, and the main country where Brahui language is spoken in 1980.\n",
    "2. {{language.human_language.countries_spoken_in (Score: Fully relavant)}}: This relation is also relevant as it provides information on the countries where Brahui language is spoken, which could help narrow down the search for the president.\n",
    "3. {{base.rosetta.languoid.parent (Score: Partially relevant)}}: This relation is less relevant but still provides some context on the language family to which Brahui belongs, which could be useful in understanding the linguistic and cultural background of the country in question.\n",
    "\n",
    "###\n",
    "Query: {query}\n",
    "Topic Entity: {topic}\n",
    "Evidence: {evidence}\n",
    "Preceding sentences: {preceding_sentences}\n",
    "Output: \n",
    "\"\"\"\n",
    "# The name of Justin Bieber's brother is Jaxon Bieber. This is based on the reasoning path that connects Justin Bieber to Jaxon Bieber through the relationship of sibling. The other paths that connect Justin Bieber to Jazmyn Bieber or back to Justin Bieber himself are incorrect in this context.\n",
    "few_shot_intepretable_prompt = FewShotPromptTemplate(\n",
    "        examples=[{\n",
    "            \"query\": \"Name the president of the country whose main spoken language was Brahui in 1980?\",\n",
    "            \"topic\": \"Brahui Language\",\n",
    "            \"evidence\": \"language.human_language.main_country; language.human_language.language_family; language.human_language.iso_639_3_code; base.rosetta.languoid.parent; language.human_language.writing_system; base.rosetta.languoid.languoid_class; language.human_language.countries_spoken_in; kg.object_profile.prominent_type; base.rosetta.languoid.document; base.ontologies.ontology_instance.equivalent_instances; base.rosetta.languoid.local_name; language.human_language.region\",\n",
    "            \"preceding_sentences\": \"\",\n",
    "            \"output\": \"\"\"1. {{language.human_language.main_country (Score: [Fully Relavant])}}: This relation is highly relevant as it directly relates to the country whose president is being asked for, and the main country where Brahui language is spoken in 1980.\n",
    "2. {{language.human_language.countries_spoken_in (Score: [Fully Relavant])}}: This relation is also relevant as it provides information on the countries where Brahui language is spoken, which could help narrow down the search for the president.\n",
    "3. {{base.rosetta.languoid.parent (Score: [Partially Relevant])}}: This relation is less relevant but still provides some context on the language family to which Brahui belongs, which could be useful in understanding the linguistic and cultural background of the country in question.\"\"\"\n",
    "        }],\n",
    "        example_prompt=PromptTemplate.from_template(\"\"\"###\n",
    "Query: {query}\n",
    "Topic Entity: {topic}\n",
    "Evidence: {evidence}\n",
    "Preceding sentences: {preceding_sentences}\n",
    "Output: {output}\n",
    "\"\"\"),\n",
    "        prefix=\n",
    "        \"\"\"You will receive a query, topic entity, evidence and optional preceding sentences containing history information. The evidence contains graph relationships or entities may be useful to answer the query. Your task is to filters out 3 valid information from the evidence that contribute to answering the query and provide a relevance score for each output, output your explanations for the score.\n",
    "The score of relevance range from [Fully Relavant], [Partially Relevant] to [Unrelevant].\"\"\",\n",
    "        suffix=\n",
    "        \"\"\"###\n",
    "Query: {query}\n",
    "Topic Entity: {topic}\n",
    "Evidence: {evidence}\n",
    "Preceding sentences: {preceding_sentences}\n",
    "Output: \"\"\",\n",
    "        input_variables=[\"query\", \"evidence\", \"preceding_sentences\", \"topic\"],\n",
    ")\n",
    "graph_intepretable_prompt = PromptTemplate(input_variables=[\"query\", \"evidence\", \"preceding_sentences\", \"topic\"], template=\n",
    "graph_intepretable)\n",
    "llm_chain = LLMChain(llm=model, prompt=few_shot_intepretable_prompt, verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def extract_relationship_and_score(s):\n",
    "    match_dict = dict()\n",
    "    pattern = r'{(.+?) \\(Score: (.+?)\\)}'\n",
    "    for match in re.findall(pattern, s):\n",
    "        match_dict[match[0]] = match[1]\n",
    "    return match_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'qid': 'WebQTrn-0',\n",
       " 'sent_idx': 0,\n",
       " 'query': 'what is the name of justin bieber brother',\n",
       " 'chain_step': 1,\n",
       " 'candidate_relation': ['people.sibling_relationship.sibling',\n",
       "  'fictional_universe.fictional_character.siblings',\n",
       "  'fictional_universe.sibling_relationship_of_fictional_characters.siblings'],\n",
       " 'candidate_entity': [['Pattie Mallette', 'Jeremy Bieber']],\n",
       " 'real_relation': 'people.person.parents',\n",
       " 'real_entity': 'Jeremy Bieber',\n",
       " 'paths': [['Justin Bieber', 'people.person.parents', 'Jeremy Bieber'],\n",
       "  ['Jeremy Bieber', 'people.person.children', 'Jaxon Bieber']],\n",
       " 'answer': ['Jaxon Bieber'],\n",
       " 'effective': False}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = []\n",
    "random.seed(42)\n",
    "from openai import BadRequestError\n",
    "qids = []\n",
    "for line in random.sample(chain_data, 2000):\n",
    "    if line['qid'] in qids:\n",
    "        continue\n",
    "    qids.append(line['qid'])\n",
    "    try:\n",
    "        batch_pred = llm_chain.batch([{\"query\": line['query'], \"evidence\": ';'.join(line['candidate_relation']), \"preceding_sentences\": '', 'topic': line['paths'][line['chain_step'] - 1][0]}], return_only_outputs=True)\n",
    "    except BadRequestError as e:\n",
    "        print('*************************Bad Request**************')\n",
    "    except ValueError as e:\n",
    "        print(f'******************Value Error****************************')\n",
    "    match_dict = extract_relationship_and_score(batch_pred[0]['text'])\n",
    "    input_data.append({\"input\": {\"query\": line['query'], \"evidence\": ';'.join(line['candidate_relation']), \"preceding_sentences\": '', 'topic': line['paths'][line['chain_step'] - 1][0]}, \"match\":match_dict, 'real_relation': line['real_relation']})\n",
    "    if len(input_data) >= 3:\n",
    "        print('Saving')\n",
    "        save_to_json(input_data, f'../output/relevance_critic_data_top_3.json')\n",
    "        input_data = []\n",
    "# create_relevance_data(input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create confidence data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "few_shot_intepretable_prompt = FewShotPromptTemplate(\n",
    "        examples=[{\n",
    "            \"query\": \"what is the name of justin bieber brother?\",\n",
    "            \"output\": \"[Retreive New Relationship]<paragraph>people.sibling_relationship.sibling;fictional_universe.fictional_character.siblings;fictional_universe.sibling_relationship_of_fictional_characters.siblings;people.person.sibling_s;people.family.members;people.person.parents</paragraph>Retrieved relationship: people.person.parents[Fully Relevant][Retrieve Entity]<paragraph>(Justin Bieber, people.person.parents, Pattie Mallette);(Justin Bieber, people.person.parents, Jeremy Bieber);(Justin Bieber, people.person.parents, Jeremy Bieber)</paragraph>Retrieved triplet: (Justin Bieber, people.person.parents, Jeremy Bieber)[Fully Relevant][Continue to Retrieve Evidence]<paragraph>people.sibling_relationship.sibling;people.person.sibling_s;people.person.parents;fictional_universe.fictional_character.siblings;people.family.members;people.person.children</paragraph>Retrieved relationship: people.person.children[Fully Relevant][Retrieve Entity]<paragraph>(Jeremy Bieber, people.person.children, Jazmyn Bieber);(Jeremy Bieber, people.person.children, Justin Bieber);(Jeremy Bieber, people.person.children, Jaxon Bieber);(Jeremy Bieber, people.person.children, Jaxon Bieber)</paragraph>Retrieved triplet: (Jeremy Bieber, people.person.children, Jaxon Bieber)[Fully Relevant][No Retrieval] Answer: Jaxon Bieber\",\n",
    "            \"explanation\": \"The output provides the name of justin bieber's brother. This is based on the reasoning path that connects Justin Bieber to Jaxon Bieber through the relationship of sibling. The other paths that connect Justin Bieber to Jazmyn Bieber or back to Justin Bieber himself are incorrect in this context.\",\n",
    "            \"rating\": \"[Confidence:5]\"\n",
    "        }],\n",
    "        example_prompt=PromptTemplate.from_template(\"\"\"\n",
    "Query: {query}\\n\n",
    "Output: {output}\n",
    "Explanation: {explanation}\n",
    "Rating: {rating}\n",
    "\"\"\"),\n",
    "        prefix=\"\"\"Given a query and an output, rate whether the response and the thought process appears to be a helpful and informative answer to the query, from 1 (lowest) - 5 (highest). We call this confidence score.\n",
    "[Confidence:5]: The response provides a complete and correct reasoning chain to the query, and the final answer is complete and logically correct.\n",
    "[Confidence:4]: The response mostly fulfills the need in the query and provides correct answers, while there can be some minor improvements such as shorter reasoning chain or less repetition.\n",
    "[Confidence:3]: The response is acceptable, but the answers may be not complete or needs minor improvement.\n",
    "[Confidence:2]: The reasoning process still addresses the main request, but the answers are not correct or not relevant to the query.\n",
    "[Confidence:1]: The reasoning is barely irrelevant or does not give an answer in the end.\"\"\",\n",
    "        suffix=\n",
    "        \"\"\"\n",
    "Query: {query}\\n\n",
    "Output: {output}\\n\"\"\",\n",
    "        input_variables=[\"query\", \"output\"],\n",
    ")\n",
    "# confidence_prompt = PromptTemplate(input_variables=[\"query\", \"output\"], template=\n",
    "# graph_intepretable)\n",
    "llm_chain = LLMChain(llm=model, prompt=few_shot_intepretable_prompt, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven a query and an output, rate whether the response and the thought process appears to be a helpful and informative answer to the query, from 1 (lowest) - 5 (highest). We call this confidence score.\n",
      "[Confidence:5]: The response provides a complete and correct reasoning chain to the query, and the final answer is complete and logically correct.\n",
      "[Confidence:4]: The response mostly fulfills the need in the query and provides correct answers, while there can be some minor improvements such as shorter reasoning chain or less repetition.\n",
      "[Confidence:3]: The response is acceptable, but the answers may be not complete or needs minor improvement.\n",
      "[Confidence:2]: The reasoning process still addresses the main request, but the answers are not correct or not relevant to the query.\n",
      "[Confidence:1]: The reasoning is barely irrelevant or does not give an answer in the end.\n",
      "\n",
      "\n",
      "Query: what is the name of justin bieber brother?\n",
      "\n",
      "Output: [Retreive New Relationship]<paragraph>people.sibling_relationship.sibling;fictional_universe.fictional_character.siblings;fictional_universe.sibling_relationship_of_fictional_characters.siblings;people.person.sibling_s;people.family.members;people.person.parents</paragraph>Retrieved relationship: people.person.parents[Fully Relevant][Retrieve Entity]<paragraph>(Justin Bieber, people.person.parents, Pattie Mallette);(Justin Bieber, people.person.parents, Jeremy Bieber);(Justin Bieber, people.person.parents, Jeremy Bieber)</paragraph>Retrieved triplet: (Justin Bieber, people.person.parents, Jeremy Bieber)[Fully Relevant][Continue to Retrieve Evidence]<paragraph>people.sibling_relationship.sibling;people.person.sibling_s;people.person.parents;fictional_universe.fictional_character.siblings;people.family.members;people.person.children</paragraph>Retrieved relationship: people.person.children[Fully Relevant][Retrieve Entity]<paragraph>(Jeremy Bieber, people.person.children, Jazmyn Bieber);(Jeremy Bieber, people.person.children, Justin Bieber);(Jeremy Bieber, people.person.children, Jaxon Bieber);(Jeremy Bieber, people.person.children, Jaxon Bieber)</paragraph>Retrieved triplet: (Jeremy Bieber, people.person.children, Jaxon Bieber)[Fully Relevant][No Retrieval] Answer: Jaxon Bieber\n",
      "Explanation: The output provides the name of justin bieber's brother. This is based on the reasoning path that connects Justin Bieber to Jaxon Bieber through the relationship of sibling. The other paths that connect Justin Bieber to Jazmyn Bieber or back to Justin Bieber himself are incorrect in this context.\n",
      "Rating: [Confidence:5]\n",
      "\n",
      "\n",
      "\n",
      "Query: what is the money called in peru\n",
      "\n",
      "Output: [Retreive New Relationship]<paragraph>base.coinsdaily.denomination.money_value;finance.currency.currency_code;measurement_unit.money_value.amount;location.country.currency_used;location.country.currency_formerly_used</paragraph>Retrieved relationship: location.country.currency_used[Fully Relevant][Retrieve Entity] Justin\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'what is the money called in peru',\n",
       " 'output': '[Retreive New Relationship]<paragraph>base.coinsdaily.denomination.money_value;finance.currency.currency_code;measurement_unit.money_value.amount;location.country.currency_used;location.country.currency_formerly_used</paragraph>Retrieved relationship: location.country.currency_used[Fully Relevant][Retrieve Entity] Justin',\n",
       " 'text': 'Rating: [Confidence:2]\\nExplanation: The response does not provide the name of the currency in Peru, which is the main request of the query. Therefore, the reasoning process is not relevant to the query.'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.invoke({'query': 'what is the money called in peru',\"output\": '[Retreive New Relationship]<paragraph>base.coinsdaily.denomination.money_value;finance.currency.currency_code;measurement_unit.money_value.amount;location.country.currency_used;location.country.currency_formerly_used</paragraph>Retrieved relationship: location.country.currency_used[Fully Relevant][Retrieve Entity] Justin' })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/WebQSP.train.json\", \"r\") as f:\n",
    "    train_webqsp = json.load(f)\n",
    "id2chain = dict()\n",
    "for line in train_webqsp['Questions']:\n",
    "    unique_chain = []\n",
    "    for p in line['Parses']:\n",
    "        if p['InferentialChain'] not in unique_chain:\n",
    "            unique_chain.append(p['InferentialChain'])\n",
    "    id2chain[line['QuestionId']] = unique_chain\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = []\n",
    "#添加constraint\n",
    "line = relation_data_train[1]\n",
    "topic_entity = line['q_entity']\n",
    "answer = line['a_entity']\n",
    "di_graph = build_Digraph(line['graph'])\n",
    "for chain in id2chain[line['id']]:\n",
    "    paths = bfs_with_rule(di_graph, topic_entity[0], chain)\n",
    "    # for p in paths:\n",
    "        # if p[-1][-1] == answer[0]:\n",
    "    input_data.append({\"query\": line['question'], \"path\": paths})\n",
    "batch_pred = llm_chain.batch(input_data, return_only_outputs=True)\n",
    "\n",
    "                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = []\n",
    "output = []\n",
    "#添加constraint\n",
    "from tqdm.notebook import tqdm\n",
    "from openai import BadRequestError\n",
    "ind = 1\n",
    "with tqdm(total=len(relation_data_train), desc=f\"Processing\",ncols=1500) as pbar:\n",
    "    while ind < len(relation_data_train):\n",
    "        line = relation_data_train[ind]\n",
    "        topic_entity = line['q_entity']\n",
    "        answer = line['a_entity']\n",
    "        di_graph = build_Digraph(line['graph'])\n",
    "        for chain in id2chain[line['id']]:\n",
    "            if chain:\n",
    "                paths = bfs_with_rule(di_graph, topic_entity[0], chain)\n",
    "            # for p in paths:\n",
    "                # if p[-1][-1] == answer[0]:\n",
    "            input_data.append({\"query\": line['question'], \"path\": paths})\n",
    "        if len(input_data) > 8:\n",
    "            try:\n",
    "                batch_pred = llm_chain.batch(input_data, return_only_outputs=True)\n",
    "            except BadRequestError as e:\n",
    "                print('*************************Bad Request**************')\n",
    "            except ValueError as e:\n",
    "                print(f'******************Value Error {ind}****************************')\n",
    "            output.extend(batch_pred)\n",
    "            input_data = []\n",
    "\n",
    "        ind += 1\n",
    "        pbar.update(1)\n",
    "        \n",
    "                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_utility_data(input_data, multi_retrieval=False):\n",
    "    print(\"creating groundness data\")\n",
    "    processed_data = []\n",
    "    for item in input_data:\n",
    "        if multi_retrieval is True:\n",
    "            if \"sent_idx\" not in item or item[\"sent_idx\"] == 0 or len(item[\"preceding_sentences\"]) == 0:\n",
    "                processed_data.append({\"instruction\": PROMPT_DICT[\"ground_multi_instruction\"], \"input\": PROMPT_DICT[\"ground_multi_input_wo_preceding\"].format_map(input), \"output\": item[\"score\"], \"task\": \"groudness\"})\n",
    "            else:\n",
    "                processed_data.append({\"instruction\": PROMPT_DICT[\"ground_multi_instruction\"], \"input\": PROMPT_DICT[\"ground_multi_input\"].format_map(input), \"output\": item[\"score\"], \"task\": \"groudness\"})\n",
    "        else: \n",
    "            processed_data.append({\"instruction\": PROMPT_DICT[\"ground_instruction\"], \"input\": PROMPT_DICT[\"ground_input\"].format_map(item['input']), \"output\": item[\"score\"], \"task\": \"groudness\"})\n",
    "    print(processed_data[-1])\n",
    "    print(\"total data number: {}\".format(len(processed_data)))\n",
    "    print(Counter([item[\"output\"] for item in processed_data ]))\n",
    "    return processed_data\n",
    "process_data = create_groundness_data(input_data, multi_retrieval=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_422534/2392507187.py:32: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  llm_chain = LLMChain(llm=model, prompt=few_shot_intepretable_prompt, verbose=False)\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "graph_intepretable =  \"\"\"Based on the reasoning paths, please answer the given question and explain why.\n",
    "Question: {query}\\n\n",
    "Reasoning Paths: {path}\\n\n",
    "\"\"\"\n",
    "# The name of Justin Bieber's brother is Jaxon Bieber. This is based on the reasoning path that connects Justin Bieber to Jaxon Bieber through the relationship of sibling. The other paths that connect Justin Bieber to Jazmyn Bieber or back to Justin Bieber himself are incorrect in this context.\n",
    "few_shot_intepretable_prompt = FewShotPromptTemplate(\n",
    "        examples=[{\n",
    "            \"query\": \"what is the name of justin bieber brother?\",\n",
    "            \"path\": \"[[('Justin Bieber', 'people.person.sibling_s', 'm.0gxnnwc'), ('m.0gxnnwc', 'people.sibling_relationship.sibling', 'Jazmyn Bieber')], [('Justin Bieber', 'people.person.sibling_s', 'm.0gxnnwc'), ('m.0gxnnwc', 'people.sibling_relationship.sibling', 'Justin Bieber')], [('Justin Bieber', 'people.person.sibling_s', 'm.0gxnnwp'), ('m.0gxnnwp', 'people.sibling_relationship.sibling', 'Jaxon Bieber')], [('Justin Bieber', 'people.person.sibling_s', 'm.0gxnnwp'), ('m.0gxnnwp', 'people.sibling_relationship.sibling', 'Justin Bieber')]]\",\n",
    "            \"answer\": \"Jaxon Bieber\",\n",
    "            \"explanation\": \"This is based on the reasoning path that connects Justin Bieber to Jaxon Bieber through the relationship of sibling. The other paths that connect Justin Bieber to Jazmyn Bieber or back to Justin Bieber himself are incorrect in this context.Therefore, the name of Justin Bieber's brother is Jaxon Bieber\"\n",
    "        }],\n",
    "        example_prompt=PromptTemplate.from_template(\"\"\"\n",
    "Question: {query}\\n\n",
    "Reasoning Paths: {path}\\n\n",
    "Answer: {answer}\n",
    "Explanation: {explanation}\n",
    "\"\"\"),\n",
    "        prefix=\n",
    "        \"\"\"Based on the reasoning paths, please answer the given question and explain why.\"\"\",\n",
    "        suffix=\n",
    "        \"\"\"\n",
    "Question: {query}\\n\n",
    "Reasoning Paths: {path}\\n\"\"\",\n",
    "        input_variables=[\"query\", \"path\"],\n",
    ")\n",
    "graph_intepretable_prompt = PromptTemplate(input_variables=[\"query\", \"path\"], template=\n",
    "graph_intepretable)\n",
    "llm_chain = LLMChain(llm=model, prompt=few_shot_intepretable_prompt, verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "本质是reflection的能力，reflection即对于思维和检索的内容进行打分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../output/groudness_webqsp.json\", \"w\") as outfile:\n",
    "        json.dump(process_data, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "random.shuffle(new_data)\n",
    "output_file_name = '../output/testtime1_1105'\n",
    "train_data = new_data[500:]\n",
    "dev_data = new_data[:500]\n",
    "\n",
    "with open(output_file_name + \"_train.json\", \"w\") as outfile:\n",
    "    json.dump(train_data, outfile)\n",
    "with open(output_file_name + \"_dev.json\", \"w\") as outfile:\n",
    "    json.dump(dev_data, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "new_data = []\n",
    "for d in data:\n",
    "    if d['output'] == '[Fully supported]':\n",
    "        if random.random() < 0.2:\n",
    "            new_data.append(d)\n",
    "    else:\n",
    "        new_data.append(d)\n",
    "Counter([item[\"output\"] for item in new_data ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../output/intepretable_answer.json\", \"r\") as f:\n",
    "    data= json.load(f)\n",
    "input_data = []\n",
    "ind = 1\n",
    "count = 0\n",
    "llm_chain = LLMChain(llm=model, prompt=multi_graph_utility_prompt, verbose=True)   \n",
    "while ind < len(relation_data_train):\n",
    "        line = relation_data_train[ind]\n",
    "        topic_entity = line['q_entity']\n",
    "        answer = line['a_entity']\n",
    "        di_graph = build_Digraph(line['graph'])\n",
    "        for chain in id2chain[line['id']]:\n",
    "            if chain:\n",
    "                paths = bfs_with_rule(di_graph, topic_entity[0], chain)\n",
    "                input_data.append({\"query\": line['question'], \"output\": '.'.join(data[count]['text'].split('\\nExplanation: '))})\n",
    "                count += 1\n",
    "        if len(input_data) > 8:\n",
    "            try:\n",
    "                batch_pred = llm_chain.batch(input_data, return_only_outputs=True)\n",
    "            except BadRequestError as e:\n",
    "                print('*************************Bad Request**************')\n",
    "            except ValueError as e:\n",
    "                print(f'******************Value Error {ind}****************************')\n",
    "            input_data = []\n",
    "            break\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference data creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../output/chain_data_top_5.json', 'r') as f:\n",
    "    chain_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7867"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chain_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_start = True\n",
    "generate_data = []\n",
    "for line in chain_data:\n",
    "    if new_start:\n",
    "        starter = '[Retreive New Relationship]'\n",
    "    if line['effective'] == False:\n",
    "        processed_relationship = ';'.join(line['candidate_relation']  + [line['real_relation']])\n",
    "    else:\n",
    "        processed_relationship = ';'.join(line['candidate_relation'])\n",
    "    candidate_entities = line['candidate_entity'][:5] if line['real_entity'] not in line['candidate_entity'] else line['candidate_entity'][:4] + [line['real_entity']]\n",
    "    precessed_triplet = [f\"({line['paths'][line['chain_step']-1][0]}, {line['paths'][line['chain_step']-1][1]}, {entity})\" for entity in candidate_entities]\n",
    "    starter += \"<paragraph>{}</paragraph>\".format(processed_relationship)\n",
    "    starter += 'Retrieved relationship: {}[Fully Relevant][Retrieve Entity]'.format(line['real_relation'])\n",
    "    starter += \"<paragraph>{}</paragraph>\".format(';'.join(precessed_triplet))\n",
    "    starter +=f\"Retrieved triplet: ({line['paths'][line['chain_step']-1][0]}, {line['paths'][line['chain_step']-1][1]}, {line['paths'][line['chain_step']-1][-1]})[Fully Relevant]\"\n",
    "    if line['chain_step'] == len(line['paths']):\n",
    "        starter += f\"[No Retrieval] {';'.join(line['answer'])} [Confidence: 5]\"\n",
    "        generate_data.append({\"instruction\": line['query'], \"output\": starter})\n",
    "        new_start = True\n",
    "        # if len(generate_data) == 10:\n",
    "        #     break\n",
    "    else:\n",
    "        starter += f\"[Continue to Retrieve Evidence]\"\n",
    "        new_start = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../output/test_generate_data_2000.json', 'w') as f:\n",
    "    json.dump(generate_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[New Retrieval] <paragraph> 'people.sibling_relationship.sibling',\n",
    "'people.person.sibling_s',\n",
    "'people.person.parents'</paragraph> Retrieved relationship: people.person.sibling_s[Fully Relevant][Retrieve entity]<paragraph> 'Jazmyn Bieber', 'Justin Bieber', 'Jaxon Bieber'</paragraph> Retrieved triplet: ('Jazmyn Bieber', 'sibling_s', 'Justin Bieber')[Fully Relevant][No Retrieval] Jaxon Bieber [Confidence: 5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:8000/v1\",\n",
    "    api_key=\"token-abc123\",\n",
    "    # skip_special_tokens=False,\n",
    ")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"/media/disk1/chatgpt/LLaMA-Factory/saves/llama3-8b/full/sft/\", \n",
    "  messages=[\n",
    "    {\"role\": \"user\", \"content\": \"You will be provided with a query, evidence, output sentence, and preceding sentences (optional). Your task is to determine whether the information in the output sentence can be fully verified by the evidence or if it requires further external verification. There are three cases:\\n- If the output sentence can be verified solely with the evidence and the preceding sentences, then respond with [No Retrieval]. \\n- If additional information about the tail entity in evidence is needed to verify the output sentence, respond with [Continue to Retrieve Evidence] \\n- If more information unrelated to the evidence is needed, e.g. totally new relationship or new entity, reponse with [New Retrieval].\\n\\nQuery: what city was michael jackson born in?\\nEvidence: \\nOutput: Gary\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "step = 0\n",
    "prediction = ''\n",
    "preceding_sentences = ''\n",
    "evidence = ''\n",
    "i = 0\n",
    "query = train_webqsp['Questions'][i]['RawQuestion']\n",
    "candidate_entity = train_webqsp['Questions'][i]['Parses'][0]['TopicEntityName']\n",
    "while prediction != '[No Retrieval]' and step < 3:\n",
    "    step += 1\n",
    "\n",
    "    current_input = PROMPT_DICT['multi_retrieval_three_way_instruction'] + PROMPT_DICT['multi_retrieval_three_way_input'].format(query=query, evidence=evidence, output=prediction, preceding_sentences=preceding_sentences)\n",
    "    print(current_input)\n",
    "    completion = client.chat.completions.create(\n",
    "      model=\"/media/disk1/chatgpt/LLaMA-Factory/saves/llama3-8b/full/sft/\", \n",
    "    \n",
    "      messages=[\n",
    "        {\"role\": \"user\", \"content\": current_input}\n",
    "      ]\n",
    "    )   \n",
    "    prediction = completion.choices[0].message.content\n",
    "        \n",
    "    if prediction == '[Continue to Retrieve Evidence]':\n",
    "      completion = client.chat.completions.create(\n",
    "        model=\"/media/disk1/chatgpt/LLaMA-Factory/saves/llama3-8b/full/sft/\", \n",
    "      \n",
    "        messages=[\n",
    "          {\"role\": \"user\", \"content\": }\n",
    "        ]\n",
    "      )\n",
    "    prediction = completion.choices[0].message.content\n",
    "      \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ground_instruction', 'ground_input', 'ground_instruction_multi', 'relevance_instruction', 'relevance_input', 'ground_multi_input', 'ground_multi_input_wo_preceding', 'retrieval_instruction', 'retrieval_input', 'retrieval_multi_instruction', 'retrieval_multi_input', 'multi_retrieval_three_way_instruction', 'multi_retrieval_three_way_input', 'multi_retrieval_three_way_input_wo_preceding', 'utility_instruction', 'utility_input'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PROMPT_DICT.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Query: {query}\\nPreceding sentences: {preceding_sentences}\\nEvidence: {evidence}\\nOutput: {target_output}'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PROMPT_DICT['multi_retrieval_three_way_input']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "raptor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
