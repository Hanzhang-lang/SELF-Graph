{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"2b219db0d2984f9dae28b651ab8ab3d9\"\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://smsh.openai.azure.com/\"\n",
    "os.environ[\"AZURE_OPENAI_API_VERSION\"] = \"2024-03-01-preview\"\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",\n",
    ")\n",
    "\n",
    "from langchain.storage import LocalFileStore, RedisStore\n",
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "store = RedisStore(redis_url=\"redis://localhost:6379\")\n",
    "cached_embedder = CacheBackedEmbeddings.from_bytes_store(\n",
    "embeddings, store, namespace=\"openai\"\n",
    ")\n",
    "row_string = []\n",
    "with open('./data/clean_relations', 'r') as f:\n",
    "    data = [line.strip() for line in f]\n",
    "db = FAISS.from_texts(data, cached_embedder)\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create with GPT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-09 08:25:14 multiproc_worker_utils.py:133] Terminating local vLLM worker processes\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "import os\n",
    "\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"2b219db0d2984f9dae28b651ab8ab3d9\"\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://smsh.openai.azure.com/\"\n",
    "os.environ[\"AZURE_OPENAI_API_VERSION\"] = \"2024-02-01\"\n",
    "os.environ[\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"] = \"gpt-35-turbo\"\n",
    "\n",
    "model = AzureChatOpenAI(\n",
    "    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    "    azure_deployment=os.environ[\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"],\n",
    "    temperature=1,\n",
    "    n = 3,\n",
    "    max_retries=5, request_timeout=600\n",
    ")\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_701401/3707533165.py:28: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  llm_chain = LLMChain(llm=model, prompt=few_shot_intepretable_prompt, verbose=True)\n"
     ]
    }
   ],
   "source": [
    "# The name of Justin Bieber's brother is Jaxon Bieber. This is based on the reasoning path that connects Justin Bieber to Jaxon Bieber through the relationship of sibling. The other paths that connect Justin Bieber to Jazmyn Bieber or back to Justin Bieber himself are incorrect in this context.\n",
    "few_shot_intepretable_prompt = FewShotPromptTemplate(\n",
    "        examples=[{\n",
    "            \"query\": \"Who was the prime minister of Japan in 2011, that served in the New Party Sakigake?\",\n",
    "            \"evidence\": \"Relations retrieved: language.human_language.countries_spoken_in\\n Entities retrieved: Japan\",\n",
    "            \"preceding_sentences\": \"\",\n",
    "            \"output\": \"Naoto Kan\",\n",
    "            \"rating\": \"[Continue to Retrieve Evidence]\"\n",
    "        }],\n",
    "        example_prompt=PromptTemplate.from_template(\"\"\"Query: {query}\n",
    "Preceding sentences: {preceding_sentences}\n",
    "Evidence: {evidence}\n",
    "Output: {target_output}\n",
    "Rating: {rating}\"\"\"),\n",
    "        prefix=\n",
    "        \"You will be provided with a query, evidence, output sentence, and preceding sentences (optional). Your task is to determine whether the information in the output sentence can be fully verified by the evidence or if it requires further external verification. There are three cases:\\n\" \n",
    "        \"- If the output sentence can be verified solely with the evidence and the preceding sentences, then respond with [No Retrieval]. \\n\"\n",
    "        \"- If additional information about the tail entity in evidence is needed to verify the output sentence, respond with [Continue to Retrieve Evidence] \\n\"\n",
    "        \"- If more information unrelated to the evidence is needed, e.g. totally new relationship or new entity, reponse with [New Retrieval].\\n\\n\",\n",
    "        suffix=\n",
    "        \"Query: {query}\\n\"\n",
    "        \"Preceding sentences: {preceding_sentences}\\n\"\n",
    "        \"Evidence: {evidence}\\n\"\n",
    "        \"Output: {target_output}\",\n",
    "        input_variables=[\"query\", \"evidence\", \"preceding_sentences\", \"topic\"],\n",
    ")\n",
    "\n",
    "llm_chain = LLMChain(llm=model, prompt=few_shot_intepretable_prompt, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relationship relevance\n",
    "graph_intepretable =  \"\"\"You will receive a query, evidence and optional preceding sentences containing history information. The evidence contains graph relationships or entities may be useful to answer the query. Your task is to filters out valid information from the evidence to answer the given query, evaluate your output and provide explanations on your result.\n",
    "\n",
    "###\n",
    "Query: Name the president of the country whose main spoken language was Brahui in 1980?\n",
    "Topic Entity: Brahui Language\n",
    "Evidence: language.human_language.main_country; language.human_language.language_family; language.human_language.iso_639_3_code; base.rosetta.languoid.parent; language.human_language.writing_system; base.rosetta.languoid.languoid_class; language.human_language.countries_spoken_in; kg.object_profile.prominent_type; base.rosetta.languoid.document; base.ontologies.ontology_instance.equivalent_instances; base.rosetta.languoid.local_name; language.human_language.region\n",
    "Preceding sentences: \n",
    "Output: \n",
    "1. {{language.human_language.main_country (Score: Fully relavant))}}: This relation is highly relevant as it directly relates to the country whose president is being asked for, and the main country where Brahui language is spoken in 1980.\n",
    "2. {{language.human_language.countries_spoken_in (Score: Fully relavant)}}: This relation is also relevant as it provides information on the countries where Brahui language is spoken, which could help narrow down the search for the president.\n",
    "3. {{base.rosetta.languoid.parent (Score: Partially relevant)}}: This relation is less relevant but still provides some context on the language family to which Brahui belongs, which could be useful in understanding the linguistic and cultural background of the country in question.\n",
    "\n",
    "###\n",
    "Query: {query}\n",
    "Topic Entity: {topic}\n",
    "Evidence: {evidence}\n",
    "Preceding sentences: {preceding_sentences}\n",
    "Output: \n",
    "\"\"\"\n",
    "# The name of Justin Bieber's brother is Jaxon Bieber. This is based on the reasoning path that connects Justin Bieber to Jaxon Bieber through the relationship of sibling. The other paths that connect Justin Bieber to Jazmyn Bieber or back to Justin Bieber himself are incorrect in this context.\n",
    "few_shot_intepretable_prompt = FewShotPromptTemplate(\n",
    "        examples=[{\n",
    "            \"query\": \"Name the president of the country whose main spoken language was Brahui in 1980?\",\n",
    "            \"topic\": \"Brahui Language\",\n",
    "            \"evidence\": \"language.human_language.main_country; language.human_language.language_family; language.human_language.iso_639_3_code; base.rosetta.languoid.parent; language.human_language.writing_system; base.rosetta.languoid.languoid_class; language.human_language.countries_spoken_in; kg.object_profile.prominent_type; base.rosetta.languoid.document; base.ontologies.ontology_instance.equivalent_instances; base.rosetta.languoid.local_name; language.human_language.region\",\n",
    "            \"preceding_sentences\": \"\",\n",
    "            \"output\": \"\"\"1. {{language.human_language.main_country (Score: [Fully Relavant])}}: This relation is highly relevant as it directly relates to the country whose president is being asked for, and the main country where Brahui language is spoken in 1980.\n",
    "2. {{language.human_language.countries_spoken_in (Score: [Fully Relavant])}}: This relation is also relevant as it provides information on the countries where Brahui language is spoken, which could help narrow down the search for the president.\n",
    "3. {{base.rosetta.languoid.parent (Score: [Partially Relevant])}}: This relation is less relevant but still provides some context on the language family to which Brahui belongs, which could be useful in understanding the linguistic and cultural background of the country in question.\"\"\"\n",
    "        }],\n",
    "        example_prompt=PromptTemplate.from_template(\"\"\"###\n",
    "Query: {query}\n",
    "Topic Entity: {topic}\n",
    "Evidence: {evidence}\n",
    "Preceding sentences: {preceding_sentences}\n",
    "Output: {output}\n",
    "\"\"\"),\n",
    "        prefix=\n",
    "        \"\"\"You will receive a query, topic entity, evidence and optional preceding sentences containing history information. The evidence contains graph relationships or entities may be useful to answer the query. Your task is to filters out 3 valid information from the evidence that contribute to answering the query and provide a relevance score for each output, output your explanations for the score.\n",
    "The score of relevance range from [Fully Relavant], [Partially Relevant] to [Unrelevant].\"\"\",\n",
    "        suffix=\n",
    "        \"\"\"###\n",
    "Query: {query}\n",
    "Topic Entity: {topic}\n",
    "Evidence: {evidence}\n",
    "Preceding sentences: {preceding_sentences}\n",
    "Output: \"\"\",\n",
    "        input_variables=[\"query\", \"evidence\", \"preceding_sentences\", \"topic\"],\n",
    ")\n",
    "graph_intepretable_prompt = PromptTemplate(input_variables=[\"query\", \"evidence\", \"preceding_sentences\", \"topic\"], template=\n",
    "graph_intepretable)\n",
    "llm_chain = LLMChain(llm=model, prompt=few_shot_intepretable_prompt, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "few_shot_intepretable_prompt = FewShotPromptTemplate(\n",
    "        examples=[{\n",
    "            \"query\": \"what is the name of justin bieber brother?\",\n",
    "            \"output\": \"[Retreive New Relationship]<paragraph>people.sibling_relationship.sibling;fictional_universe.fictional_character.siblings;fictional_universe.sibling_relationship_of_fictional_characters.siblings;people.person.sibling_s;people.family.members;people.person.parents</paragraph>Retrieved relationship: people.person.parents[Fully Relevant][Retrieve Entity]<paragraph>(Justin Bieber, people.person.parents, Pattie Mallette);(Justin Bieber, people.person.parents, Jeremy Bieber);(Justin Bieber, people.person.parents, Jeremy Bieber)</paragraph>Retrieved triplet: (Justin Bieber, people.person.parents, Jeremy Bieber)[Fully Relevant][Continue to Retrieve Evidence]<paragraph>people.sibling_relationship.sibling;people.person.sibling_s;people.person.parents;fictional_universe.fictional_character.siblings;people.family.members;people.person.children</paragraph>Retrieved relationship: people.person.children[Fully Relevant][Retrieve Entity]<paragraph>(Jeremy Bieber, people.person.children, Jazmyn Bieber);(Jeremy Bieber, people.person.children, Justin Bieber);(Jeremy Bieber, people.person.children, Jaxon Bieber);(Jeremy Bieber, people.person.children, Jaxon Bieber)</paragraph>Retrieved triplet: (Jeremy Bieber, people.person.children, Jaxon Bieber)[Fully Relevant][No Retrieval] Answer: Jaxon Bieber\",\n",
    "            \"explanation\": \"The output provides the name of justin bieber's brother. This is based on the reasoning path that connects Justin Bieber to Jaxon Bieber through the relationship of sibling. The other paths that connect Justin Bieber to Jazmyn Bieber or back to Justin Bieber himself are incorrect in this context.\",\n",
    "            \"rating\": \"[Confidence:5]\"\n",
    "        }],\n",
    "        example_prompt=PromptTemplate.from_template(\"\"\"\n",
    "Query: {query}\\n\n",
    "Output: {output}\n",
    "Explanation: {explanation}\n",
    "Rating: {rating}\n",
    "\"\"\"),\n",
    "        prefix=\"\"\"Given a query and an output, rate whether the response and the thought process appears to be a helpful and informative answer to the query, from 1 (lowest) - 5 (highest). We call this confidence score.\n",
    "[Confidence:5]: The response provides a complete and correct reasoning chain to the query, and the final answer is complete and logically correct.\n",
    "[Confidence:4]: The response mostly fulfills the need in the query and provides correct answers, while there can be some minor improvements such as shorter reasoning chain or less repetition.\n",
    "[Confidence:3]: The response is acceptable, but the answers may be not complete or needs minor improvement.\n",
    "[Confidence:2]: The reasoning process still addresses the main request, but the answers are not correct or not relevant to the query.\n",
    "[Confidence:1]: The reasoning is barely irrelevant or does not give an answer in the end.\"\"\",\n",
    "        suffix=\n",
    "        \"\"\"\n",
    "Query: {query}\\n\n",
    "Output: {output}\\n\"\"\",\n",
    "        input_variables=[\"query\", \"output\"],\n",
    ")\n",
    "# confidence_prompt = PromptTemplate(input_variables=[\"query\", \"output\"], template=\n",
    "# graph_intepretable)\n",
    "llm_chain = LLMChain(llm=model, prompt=few_shot_intepretable_prompt, verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run_long_form answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/disk1/chatgpt/miniconda3/envs/self-rag/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 12-09 08:29:25 cuda.py:22] You are using a deprecated `pynvml` package. Please install `nvidia-ml-py` instead, and make sure to uninstall `pynvml`. When both of them are installed, `pynvml` will take precedence and cause errors. See https://pypi.org/project/pynvml for more information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-09 08:29:26,550\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-09 08:29:30 config.py:905] Defaulting to use mp for distributed inference\n",
      "INFO 12-09 08:29:30 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='/media/disk2/llama_factory/generation_1206_no_mask', speculative_config=None, tokenizer='/media/disk2/llama_factory/generation_1206_no_mask', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/media/disk2/llama_factory/generation_1206_no_mask, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "WARNING 12-09 08:29:31 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 12-09 08:29:31 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "\u001b[1;36m(VllmWorkerProcess pid=724396)\u001b[0;0m INFO 12-09 08:29:31 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=724398)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=724397)\u001b[0;0m INFO 12-09 08:29:31 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "INFO 12-09 08:29:31 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "INFO 12-09 08:29:33 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "INFO 12-09 08:29:33 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=724397)\u001b[0;0m INFO 12-09 08:29:33 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=724398)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=724396)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=724397)\u001b[0;0m INFO 12-09 08:29:33 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "INFO 12-09 08:29:33 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 12-09 08:29:33 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=724396)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=724398)\u001b[0;0m INFO 12-09 08:29:33 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 12-09 08:29:33 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "WARNING 12-09 08:29:34 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=724398)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=724396)\u001b[0;0m WARNING 12-09 08:29:34 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 12-09 08:29:34 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=724397)\u001b[0;0m WARNING 12-09 08:29:34 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "INFO 12-09 08:29:34 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7f15562a3c40>, local_subscribe_port=55795, remote_subscribe_port=None)\n",
      "INFO 12-09 08:29:34 model_runner.py:1056] Starting to load model /media/disk2/llama_factory/generation_1206_no_mask...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=724396)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=724398)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=724397)\u001b[0;0m INFO 12-09 08:29:34 model_runner.py:1056] Starting to load model /media/disk2/llama_factory/generation_1206_no_mask...\n",
      "INFO 12-09 08:29:34 model_runner.py:1056] Starting to load model /media/disk2/llama_factory/generation_1206_no_mask...\n",
      "INFO 12-09 08:29:34 model_runner.py:1056] Starting to load model /media/disk2/llama_factory/generation_1206_no_mask...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:00<00:00,  4.75it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:00<00:00,  3.45it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:01<00:00,  2.94it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:01<00:00,  3.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=724397)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=724398)\u001b[0;0m INFO 12-09 08:29:35 model_runner.py:1067] Loading model weights took 3.7420 GB\n",
      "INFO 12-09 08:29:35 model_runner.py:1067] Loading model weights took 3.7420 GB\n",
      "INFO 12-09 08:29:35 model_runner.py:1067] Loading model weights took 3.7420 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=724396)\u001b[0;0m "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-09 08:29:35 model_runner.py:1067] Loading model weights took 3.7420 GB\n",
      "INFO 12-09 08:29:37 distributed_gpu_executor.py:57] # GPU blocks: 61323, # CPU blocks: 8192\n",
      "INFO 12-09 08:29:37 distributed_gpu_executor.py:61] Maximum concurrency for 8192 tokens per request: 119.77x\n",
      "INFO 12-09 08:29:39 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 12-09 08:29:39 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=724398)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=724396)\u001b[0;0m INFO 12-09 08:29:39 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 12-09 08:29:39 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=724396)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=724397)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=724398)\u001b[0;0m INFO 12-09 08:29:39 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 12-09 08:29:39 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 12-09 08:29:39 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=724397)\u001b[0;0m INFO 12-09 08:29:39 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 12-09 08:29:52 model_runner.py:1523] Graph capturing finished in 13 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=724398)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=724396)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=724397)\u001b[0;0m INFO 12-09 08:29:52 model_runner.py:1523] Graph capturing finished in 13 secs.\n",
      "INFO 12-09 08:29:52 model_runner.py:1523] Graph capturing finished in 13 secs.\n",
      "INFO 12-09 08:29:52 model_runner.py:1523] Graph capturing finished in 13 secs.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "# model = LLM(model='/media/disk2/llama_factory/generation_1124_special', trust_remote_code=True, tensor_parallel_size=4)\n",
    "model = LLM(model='/media/disk2/llama_factory/generation_1206_no_mask', trust_remote_code=True, tensor_parallel_size=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.09s/it, est. speed input: 15.65 toks/s, output: 92.06 toks/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletionOutput(index=0, text='[Retrieve Entity]<paragraph>law.invention.inventor;law.inventor.inventions;law.invention.date_of_invention;business.consumer_company.products;organization.organization.founders</paragraph>business.consumer_company.products[Fully Relevant]law.invention.inventor[Partially Relevant]organization.organization.founders[Partially Relevant][Retrieve Entity]<paragraph>(Google Search, law.invention.inventor, Sergey Brin);(Google Search, law.invention.inventor, Larry Page)</paragraph>Sergey Brin[Partially Relevant]Larry Page[Partially Relevant][Continue to Retrieve Evidence]<paragraph>people.person.profession', token_ids=(128259, 128263, 20510, 1896, 7611, 1896, 688, 269, 26, 20510, 1896, 688, 269, 1896, 31759, 26, 20510, 1896, 7611, 10108, 3659, 1265, 7611, 26, 27243, 69866, 34503, 37262, 26, 24844, 70324, 840, 801, 388, 128264, 27243, 69866, 34503, 37262, 128261, 20510, 1896, 7611, 1896, 688, 269, 128262, 24844, 70324, 840, 801, 388, 128262, 128259, 128263, 6838, 2738, 7694, 11, 2383, 1896, 7611, 1896, 688, 269, 11, 74529, 3320, 258, 1237, 7, 14783, 7694, 11, 2383, 1896, 7611, 1896, 688, 269, 11, 30390, 5874, 8, 128264, 50, 10286, 88, 3320, 258, 128262, 89595, 5874, 128262, 128258, 128263, 16455, 31970, 93605, 1362), cumulative_logprob=-3.814689989667386e-06, logprobs=[{128259: Logprob(logprob=0.0, rank=1, decoded_token='[Retrieve Entity]'), 128256: Logprob(logprob=-137.3658447265625, rank=2, decoded_token='[No Retrieval]'), 128258: Logprob(logprob=-268.48779296875, rank=3, decoded_token='[Continue to Retrieve Evidence]'), 14783: Logprob(logprob=-387.1219482421875, rank=4, decoded_token='Google'), 58: Logprob(logprob=-487.0244140625, rank=5, decoded_token='[')}, {128263: Logprob(logprob=0.0, rank=1, decoded_token='<paragraph>'), 128256: Logprob(logprob=-2110.43896484375, rank=2, decoded_token='[No Retrieval]'), 128258: Logprob(logprob=-2141.658447265625, rank=3, decoded_token='[Continue to Retrieve Evidence]'), 953: Logprob(logprob=-2172.8779296875, rank=4, decoded_token='ix'), 85514: Logprob(logprob=-2338.34130859375, rank=5, decoded_token='yw')}, {20510: Logprob(logprob=0.0, rank=1, decoded_token='law'), 24844: Logprob(logprob=-199.8048095703125, rank=2, decoded_token='organization'), 27243: Logprob(logprob=-237.2681884765625, rank=3, decoded_token='business'), 3231: Logprob(logprob=-324.682861328125, rank=4, decoded_token='base'), 5581: Logprob(logprob=-349.658447265625, rank=5, decoded_token='common')}, {1896: Logprob(logprob=0.0, rank=1, decoded_token='.in'), 522: Logprob(logprob=-836.6829833984375, rank=2, decoded_token='.c'), 37858: Logprob(logprob=-1142.6341552734375, rank=3, decoded_token='.business'), 28020: Logprob(logprob=-1186.3414306640625, rank=4, decoded_token='.company'), 7929: Logprob(logprob=-1223.804931640625, rank=5, decoded_token='.dis')}, {7611: Logprob(logprob=0.0, rank=1, decoded_token='vention'), 688: Logprob(logprob=-74.927001953125, rank=2, decoded_token='vent'), 31759: Logprob(logprob=-711.804931640625, rank=3, decoded_token='ventions'), 46043: Logprob(logprob=-761.7562255859375, rank=4, decoded_token='venture'), 39142: Logprob(logprob=-774.2440185546875, rank=5, decoded_token='nov')}, {1896: Logprob(logprob=0.0, rank=1, decoded_token='.in'), 10108: Logprob(logprob=-524.4879150390625, rank=2, decoded_token='.date'), 28020: Logprob(logprob=-799.2196044921875, rank=3, decoded_token='.company'), 70324: Logprob(logprob=-955.317138671875, rank=4, decoded_token='.organization'), 916: Logprob(logprob=-974.048828125, rank=5, decoded_token='.com')}, {688: Logprob(logprob=0.0, rank=1, decoded_token='vent'), 7611: Logprob(logprob=-1198.8292236328125, rank=2, decoded_token='vention'), 31759: Logprob(logprob=-1273.7559814453125, rank=3, decoded_token='ventions'), 39142: Logprob(logprob=-1348.682861328125, rank=4, decoded_token='nov'), 46043: Logprob(logprob=-1511.0242919921875, rank=5, decoded_token='venture')}, {269: Logprob(logprob=0.0, rank=1, decoded_token='or'), 1105: Logprob(logprob=-1273.756103515625, rank=2, decoded_token='ors'), 3093: Logprob(logprob=-1298.731689453125, rank=3, decoded_token='ored'), 261: Logprob(logprob=-1304.9755859375, rank=4, decoded_token='er'), 291: Logprob(logprob=-1342.43896484375, rank=5, decoded_token='ed')}, {26: Logprob(logprob=0.0, rank=1, decoded_token=';'), 40514: Logprob(logprob=-536.9755859375, rank=2, decoded_token=';s'), 82960: Logprob(logprob=-1142.63427734375, rank=3, decoded_token=';base'), 1161: Logprob(logprob=-1211.317138671875, rank=4, decoded_token='(s'), 11: Logprob(logprob=-1286.2440185546875, rank=5, decoded_token=',')}, {20510: Logprob(logprob=0.0, rank=1, decoded_token='law'), 24844: Logprob(logprob=-936.585205078125, rank=2, decoded_token='organization'), 258: Logprob(logprob=-986.5364990234375, rank=3, decoded_token='in'), 27243: Logprob(logprob=-999.0242919921875, rank=4, decoded_token='business'), 5581: Logprob(logprob=-1148.8779296875, rank=5, decoded_token='common')}, {1896: Logprob(logprob=0.0, rank=1, decoded_token='.in'), 1351: Logprob(logprob=-1536.0001220703125, rank=2, decoded_token='.re'), 6537: Logprob(logprob=-1642.1463623046875, rank=3, decoded_token='.int'), 37858: Logprob(logprob=-1642.1463623046875, rank=4, decoded_token='.business'), 929: Logprob(logprob=-1654.63427734375, rank=5, decoded_token='.l')}, {688: Logprob(logprob=0.0, rank=1, decoded_token='vent'), 7611: Logprob(logprob=-1423.60986328125, rank=2, decoded_token='vention'), 39142: Logprob(logprob=-1785.7562255859375, rank=3, decoded_token='nov'), 82920: Logprob(logprob=-1885.65869140625, rank=4, decoded_token='venting'), 93258: Logprob(logprob=-1948.09765625, rank=5, decoded_token=' Invent')}, {269: Logprob(logprob=0.0, rank=1, decoded_token='or'), 261: Logprob(logprob=-1498.53662109375, rank=2, decoded_token='er'), 45807: Logprob(logprob=-1816.9757080078125, rank=3, decoded_token='nor'), 93258: Logprob(logprob=-1866.9268798828125, rank=4, decoded_token=' Invent'), 461: Logprob(logprob=-1885.6585693359375, rank=5, decoded_token='ore')}, {1896: Logprob(logprob=0.0, rank=1, decoded_token='.in'), 37262: Logprob(logprob=-1311.2196044921875, rank=2, decoded_token='.products'), 36025: Logprob(logprob=-1361.1707763671875, rank=3, decoded_token='.original'), 13: Logprob(logprob=-1411.1219482421875, rank=4, decoded_token='.'), 85149: Logprob(logprob=-1429.853759765625, rank=5, decoded_token=' inventions')}, {31759: Logprob(logprob=0.0, rank=1, decoded_token='ventions'), 7611: Logprob(logprob=-2035.51220703125, rank=2, decoded_token='vention'), 85149: Logprob(logprob=-2110.43896484375, rank=3, decoded_token=' inventions'), 74228: Logprob(logprob=-2272.7802734375, rank=4, decoded_token='vented'), 688: Logprob(logprob=-2285.26806640625, rank=5, decoded_token='vent')}, {26: Logprob(logprob=0.0, rank=1, decoded_token=';'), 82960: Logprob(logprob=-1073.951171875, rank=2, decoded_token=';base'), 40514: Logprob(logprob=-1273.756103515625, rank=3, decoded_token=';s'), 79732: Logprob(logprob=-1536.0, rank=4, decoded_token=';c'), 68336: Logprob(logprob=-1604.6829833984375, rank=5, decoded_token=';m')}, {20510: Logprob(logprob=0.0, rank=1, decoded_token='law'), 27243: Logprob(logprob=-686.8292236328125, rank=2, decoded_token='business'), 5581: Logprob(logprob=-711.8048095703125, rank=3, decoded_token='common'), 24844: Logprob(logprob=-836.682861328125, rank=4, decoded_token='organization'), 16455: Logprob(logprob=-1036.4876708984375, rank=5, decoded_token='people')}, {1896: Logprob(logprob=0.0, rank=1, decoded_token='.in'), 70324: Logprob(logprob=-1517.268310546875, rank=2, decoded_token='.organization'), 1351: Logprob(logprob=-1585.9512939453125, rank=3, decoded_token='.re'), 22680: Logprob(logprob=-1592.1951904296875, rank=4, decoded_token='.us'), 28020: Logprob(logprob=-1685.853759765625, rank=5, decoded_token='.company')}, {7611: Logprob(logprob=0.0, rank=1, decoded_token='vention'), 688: Logprob(logprob=-1461.072998046875, rank=2, decoded_token='vent'), 31759: Logprob(logprob=-1511.024169921875, rank=3, decoded_token='ventions'), 10663: Logprob(logprob=-1948.097412109375, rank=4, decoded_token='stitution'), 46043: Logprob(logprob=-2072.9755859375, rank=5, decoded_token='venture')}, {10108: Logprob(logprob=0.0, rank=1, decoded_token='.date'), 4257: Logprob(logprob=-1186.3414306640625, rank=2, decoded_token='_date'), 962: Logprob(logprob=-1286.243896484375, rank=3, decoded_token='.d'), 25418: Logprob(logprob=-1417.3658447265625, rank=4, decoded_token='.year'), 25463: Logprob(logprob=-1486.0487060546875, rank=5, decoded_token='.place')}, {3659: Logprob(logprob=0.0, rank=1, decoded_token='_of'), 315: Logprob(logprob=-1136.39013671875, rank=2, decoded_token=' of'), 5046: Logprob(logprob=-1511.0242919921875, rank=3, decoded_token=' Of'), 28388: Logprob(logprob=-1585.951171875, rank=4, decoded_token='_created'), 2173: Logprob(logprob=-1585.951171875, rank=5, decoded_token='Of')}, {1265: Logprob(logprob=0.0, rank=1, decoded_token='_in'), 1896: Logprob(logprob=-1280.0, rank=2, decoded_token='.in'), 644: Logprob(logprob=-1386.146240234375, rank=3, decoded_token='In'), 28173: Logprob(logprob=-1423.6097412109375, rank=4, decoded_token='_public'), 10135: Logprob(logprob=-1461.0731201171875, rank=5, decoded_token='_dis')}, {7611: Logprob(logprob=0.0, rank=1, decoded_token='vention'), 31759: Logprob(logprob=-1473.56103515625, rank=2, decoded_token='ventions'), 688: Logprob(logprob=-1823.2197265625, rank=3, decoded_token='vent'), 99848: Logprob(logprob=-2197.853759765625, rank=4, decoded_token='novation'), 82920: Logprob(logprob=-2360.1953125, rank=5, decoded_token='venting')}, {26: Logprob(logprob=0.0, rank=1, decoded_token=';'), 82960: Logprob(logprob=-649.365966796875, rank=2, decoded_token=';base'), 40514: Logprob(logprob=-974.048828125, rank=3, decoded_token=';s'), 79732: Logprob(logprob=-1073.9512939453125, rank=4, decoded_token=';c'), 100249: Logprob(logprob=-1223.804931640625, rank=5, decoded_token=';d')}, {27243: Logprob(logprob=-3.814689989667386e-06, rank=1, decoded_token='business'), 24844: Logprob(logprob=-12.487796783447266, rank=2, decoded_token='organization'), 32261: Logprob(logprob=-299.707275390625, rank=3, decoded_token='music'), 44211: Logprob(logprob=-362.1463623046875, rank=4, decoded_token='computer'), 86843: Logprob(logprob=-387.1219482421875, rank=5, decoded_token='internet')}, {69866: Logprob(logprob=0.0, rank=1, decoded_token='.consumer'), 13: Logprob(logprob=-112.3902587890625, rank=2, decoded_token='.'), 37858: Logprob(logprob=-212.292724609375, rank=3, decoded_token='.business'), 516: Logprob(logprob=-262.2440185546875, rank=4, decoded_token='.s'), 28020: Logprob(logprob=-287.2196044921875, rank=5, decoded_token='.company')}, {34503: Logprob(logprob=0.0, rank=1, decoded_token='_company'), 8351: Logprob(logprob=-1123.90234375, rank=2, decoded_token=' Company'), 28020: Logprob(logprob=-1136.39013671875, rank=3, decoded_token='.company'), 10041: Logprob(logprob=-1136.39013671875, rank=4, decoded_token='_product'), 14831: Logprob(logprob=-1211.3170166015625, rank=5, decoded_token='Company')}, {37262: Logprob(logprob=0.0, rank=1, decoded_token='.products'), 21610: Logprob(logprob=-487.0244140625, rank=2, decoded_token='.br'), 12377: Logprob(logprob=-861.6585693359375, rank=3, decoded_token='.product'), 37858: Logprob(logprob=-942.8292236328125, rank=4, decoded_token='.business'), 13: Logprob(logprob=-961.5609130859375, rank=5, decoded_token='.')}, {26: Logprob(logprob=0.0, rank=1, decoded_token=';'), 82960: Logprob(logprob=-162.341552734375, rank=2, decoded_token=';base'), 40514: Logprob(logprob=-911.6097412109375, rank=3, decoded_token=';s'), 56033: Logprob(logprob=-1111.4146728515625, rank=4, decoded_token=';b'), 79732: Logprob(logprob=-1123.9024658203125, rank=5, decoded_token=';c')}, {24844: Logprob(logprob=0.0, rank=1, decoded_token='organization'), 27243: Logprob(logprob=-162.34130859375, rank=2, decoded_token='business'), 86843: Logprob(logprob=-599.41455078125, rank=3, decoded_token='internet'), 44211: Logprob(logprob=-649.3658447265625, rank=4, decoded_token='computer'), 21100: Logprob(logprob=-749.2681884765625, rank=5, decoded_token='astr')}, {70324: Logprob(logprob=0.0, rank=1, decoded_token='.organization'), 31602: Logprob(logprob=-1048.9755859375, rank=2, decoded_token='.le'), 45939: Logprob(logprob=-1573.46337890625, rank=3, decoded_token='.members'), 24844: Logprob(logprob=-1642.146240234375, rank=4, decoded_token='organization'), 83452: Logprob(logprob=-1698.34130859375, rank=5, decoded_token='_organization')}, {840: Logprob(logprob=0.0, rank=1, decoded_token='.f'), 22200: Logprob(logprob=-237.268310546875, rank=2, decoded_token='_found'), 10108: Logprob(logprob=-743.0244140625, rank=3, decoded_token='.date'), 49065: Logprob(logprob=-780.4879150390625, rank=4, decoded_token='_spin'), 31602: Logprob(logprob=-786.7318115234375, rank=5, decoded_token='.le')}, {801: Logprob(logprob=0.0, rank=1, decoded_token='ound'), 13900: Logprob(logprob=-1473.56103515625, rank=2, decoded_token='ounding'), 13382: Logprob(logprob=-1610.9267578125, rank=3, decoded_token='ounded'), 13891: Logprob(logprob=-1723.3170166015625, rank=4, decoded_token='OUND'), 3171: Logprob(logprob=-1785.7559814453125, rank=5, decoded_token='ounds')}, {388: Logprob(logprob=0.0, rank=1, decoded_token='ers'), 4419: Logprob(logprob=-1873.170654296875, rank=2, decoded_token='ERS'), 261: Logprob(logprob=-1948.0975341796875, rank=3, decoded_token='er'), 1697: Logprob(logprob=-1979.3170166015625, rank=4, decoded_token='ational'), 811: Logprob(logprob=-2023.0244140625, rank=5, decoded_token='ations')}, {128264: Logprob(logprob=0.0, rank=1, decoded_token='</paragraph>'), 26: Logprob(logprob=-62.43896484375, rank=2, decoded_token=';'), 82960: Logprob(logprob=-880.3902587890625, rank=3, decoded_token=';base'), 13: Logprob(logprob=-911.6097412109375, rank=4, decoded_token='.'), 128262: Logprob(logprob=-999.0244140625, rank=5, decoded_token='[Partially Relevant]')}, {27243: Logprob(logprob=0.0, rank=1, decoded_token='business'), 20510: Logprob(logprob=-249.756103515625, rank=2, decoded_token='law'), 24844: Logprob(logprob=-536.9755859375, rank=3, decoded_token='organization'), 5581: Logprob(logprob=-1017.756103515625, rank=4, decoded_token='common'), 23562: Logprob(logprob=-1024.0, rank=5, decoded_token='Business')}, {69866: Logprob(logprob=0.0, rank=1, decoded_token='.consumer'), 47864: Logprob(logprob=-1198.8292236328125, rank=2, decoded_token='consumer'), 2932: Logprob(logprob=-1248.7803955078125, rank=3, decoded_token='.con'), 11761: Logprob(logprob=-1423.609619140625, rank=4, decoded_token=' consumer'), 75069: Logprob(logprob=-1473.5609130859375, rank=5, decoded_token='_consumer')}, {34503: Logprob(logprob=0.0, rank=1, decoded_token='_company'), 8351: Logprob(logprob=-1411.121826171875, rank=2, decoded_token=' Company'), 28020: Logprob(logprob=-1473.5609130859375, rank=3, decoded_token='.company'), 14831: Logprob(logprob=-1573.4632568359375, rank=4, decoded_token='Company'), 92133: Logprob(logprob=-1635.90234375, rank=5, decoded_token='-company')}, {37262: Logprob(logprob=0.0, rank=1, decoded_token='.products'), 3956: Logprob(logprob=-1323.7073974609375, rank=2, decoded_token=' products'), 10354: Logprob(logprob=-1398.6341552734375, rank=3, decoded_token='products'), 72055: Logprob(logprob=-1411.1219482421875, rank=4, decoded_token='(products'), 34580: Logprob(logprob=-1423.6097412109375, rank=5, decoded_token='/products')}, {128261: Logprob(logprob=0.0, rank=1, decoded_token='[Fully Relevant]'), 128262: Logprob(logprob=-524.48779296875, rank=2, decoded_token='[Partially Relevant]'), 128260: Logprob(logprob=-1198.829345703125, rank=3, decoded_token='[Unrelevant]'), 128264: Logprob(logprob=-1629.6585693359375, rank=4, decoded_token='</paragraph>'), 58: Logprob(logprob=-1642.1463623046875, rank=5, decoded_token='[')}, {20510: Logprob(logprob=0.0, rank=1, decoded_token='law'), 24844: Logprob(logprob=-99.90234375, rank=2, decoded_token='organization'), 27243: Logprob(logprob=-861.658447265625, rank=3, decoded_token='business'), 70324: Logprob(logprob=-999.0242919921875, rank=4, decoded_token='.organization'), 2588: Logprob(logprob=-1017.756103515625, rank=5, decoded_token='location')}, {1896: Logprob(logprob=0.0, rank=1, decoded_token='.in'), 37858: Logprob(logprob=-1311.2193603515625, rank=2, decoded_token='.business'), 69866: Logprob(logprob=-1523.5120849609375, rank=3, decoded_token='.consumer'), 1265: Logprob(logprob=-1604.682861328125, rank=4, decoded_token='_in'), 70324: Logprob(logprob=-1648.39013671875, rank=5, decoded_token='.organization')}, {7611: Logprob(logprob=0.0, rank=1, decoded_token='vention'), 688: Logprob(logprob=-237.268310546875, rank=2, decoded_token='vent'), 31759: Logprob(logprob=-855.414794921875, rank=3, decoded_token='ventions'), 46043: Logprob(logprob=-917.853759765625, rank=4, decoded_token='venture'), 82920: Logprob(logprob=-1042.7318115234375, rank=5, decoded_token='venting')}, {1896: Logprob(logprob=0.0, rank=1, decoded_token='.in'), 10108: Logprob(logprob=-649.365966796875, rank=2, decoded_token='.date'), 1265: Logprob(logprob=-1304.9757080078125, rank=3, decoded_token='_in'), 28020: Logprob(logprob=-1442.341552734375, rank=4, decoded_token='.company'), 26270: Logprob(logprob=-1461.0732421875, rank=5, decoded_token='.board')}, {688: Logprob(logprob=0.0, rank=1, decoded_token='vent'), 7611: Logprob(logprob=-536.9757080078125, rank=2, decoded_token='vention'), 31759: Logprob(logprob=-942.829345703125, rank=3, decoded_token='ventions'), 46043: Logprob(logprob=-1105.1707763671875, rank=4, decoded_token='venture'), 82920: Logprob(logprob=-1117.65869140625, rank=5, decoded_token='venting')}, {269: Logprob(logprob=0.0, rank=1, decoded_token='or'), 261: Logprob(logprob=-1373.6585693359375, rank=2, decoded_token='er'), 324: Logprob(logprob=-1479.804931640625, rank=3, decoded_token='ur'), 1105: Logprob(logprob=-1542.243896484375, rank=4, decoded_token='ors'), 859: Logprob(logprob=-1679.60986328125, rank=5, decoded_token='ator')}, {128262: Logprob(logprob=0.0, rank=1, decoded_token='[Partially Relevant]'), 128260: Logprob(logprob=-661.8536376953125, rank=2, decoded_token='[Unrelevant]'), 128261: Logprob(logprob=-1180.09765625, rank=3, decoded_token='[Fully Relevant]'), 58: Logprob(logprob=-1398.6341552734375, rank=4, decoded_token='['), 26: Logprob(logprob=-1635.9024658203125, rank=5, decoded_token=';')}, {24844: Logprob(logprob=0.0, rank=1, decoded_token='organization'), 20510: Logprob(logprob=-174.829345703125, rank=2, decoded_token='law'), 128259: Logprob(logprob=-955.317138671875, rank=3, decoded_token='[Retrieve Entity]'), 70324: Logprob(logprob=-961.56103515625, rank=4, decoded_token='.organization'), 2588: Logprob(logprob=-1036.48779296875, rank=5, decoded_token='location')}, {70324: Logprob(logprob=0.0, rank=1, decoded_token='.organization'), 83452: Logprob(logprob=-1423.60986328125, rank=2, decoded_token='_organization'), 24844: Logprob(logprob=-1436.09765625, rank=3, decoded_token='organization'), 51272: Logprob(logprob=-1486.048828125, rank=4, decoded_token='.organ'), 7471: Logprob(logprob=-1560.9757080078125, rank=5, decoded_token=' organization')}, {840: Logprob(logprob=0.0, rank=1, decoded_token='.f'), 22200: Logprob(logprob=-1423.6097412109375, rank=2, decoded_token='_found'), 10108: Logprob(logprob=-1710.829345703125, rank=3, decoded_token='.date'), 48727: Logprob(logprob=-1835.7073974609375, rank=4, decoded_token=' founders'), 1896: Logprob(logprob=-1860.6829833984375, rank=5, decoded_token='.in')}, {801: Logprob(logprob=0.0, rank=1, decoded_token='ound'), 13900: Logprob(logprob=-1398.63427734375, rank=2, decoded_token='ounding'), 3171: Logprob(logprob=-1635.9024658203125, rank=3, decoded_token='ounds'), 3023: Logprob(logprob=-1673.3658447265625, rank=4, decoded_token='oud'), 1263: Logprob(logprob=-1798.243896484375, rank=5, decoded_token='und')}, {388: Logprob(logprob=0.0, rank=1, decoded_token='ers'), 1105: Logprob(logprob=-2010.5364990234375, rank=2, decoded_token='ors'), 5079: Logprob(logprob=-2072.9755859375, rank=3, decoded_token='ners'), 4419: Logprob(logprob=-2197.853515625, rank=4, decoded_token='ERS'), 826: Logprob(logprob=-2247.8046875, rank=5, decoded_token='ings')}, {128262: Logprob(logprob=0.0, rank=1, decoded_token='[Partially Relevant]'), 128260: Logprob(logprob=-124.8779296875, rank=2, decoded_token='[Unrelevant]'), 128261: Logprob(logprob=-1173.8536376953125, rank=3, decoded_token='[Fully Relevant]'), 58: Logprob(logprob=-1267.51220703125, rank=4, decoded_token='['), 128259: Logprob(logprob=-1557.8536376953125, rank=5, decoded_token='[Retrieve Entity]')}, {128259: Logprob(logprob=0.0, rank=1, decoded_token='[Retrieve Entity]'), 128256: Logprob(logprob=-1423.6097412109375, rank=2, decoded_token='[No Retrieval]'), 128258: Logprob(logprob=-1767.0244140625, rank=3, decoded_token='[Continue to Retrieve Evidence]'), 20510: Logprob(logprob=-1860.6829833984375, rank=4, decoded_token='law'), 128264: Logprob(logprob=-2016.780517578125, rank=5, decoded_token='</paragraph>')}, {128263: Logprob(logprob=0.0, rank=1, decoded_token='<paragraph>'), 128256: Logprob(logprob=-2197.853515625, rank=2, decoded_token='[No Retrieval]'), 128259: Logprob(logprob=-2372.68310546875, rank=3, decoded_token='[Retrieve Entity]'), 128258: Logprob(logprob=-2378.9267578125, rank=4, decoded_token='[Continue to Retrieve Evidence]'), 7235: Logprob(logprob=-2422.63427734375, rank=5, decoded_token='icial')}, {6838: Logprob(logprob=0.0, rank=1, decoded_token='(G'), 5063: Logprob(logprob=-1348.6829833984375, rank=2, decoded_token='(L'), 3269: Logprob(logprob=-1560.9757080078125, rank=3, decoded_token='(M'), 3348: Logprob(logprob=-1585.9512939453125, rank=4, decoded_token='(g'), 7: Logprob(logprob=-1623.4146728515625, rank=5, decoded_token='(')}, {2738: Logprob(logprob=0.0, rank=1, decoded_token='oogle'), 52031: Logprob(logprob=-1323.7073974609375, rank=2, decoded_token='adget'), 68: Logprob(logprob=-1348.6829833984375, rank=3, decoded_token='e'), 329: Logprob(logprob=-1436.09765625, rank=4, decoded_token='ad'), 686: Logprob(logprob=-1448.58544921875, rank=5, decoded_token='ear')}, {7694: Logprob(logprob=0.0, rank=1, decoded_token=' Search'), 11: Logprob(logprob=-337.1707763671875, rank=2, decoded_token=','), 34120: Logprob(logprob=-549.46337890625, rank=3, decoded_token=' Apps'), 2467: Logprob(logprob=-674.3414306640625, rank=4, decoded_token=' Ad'), 29030: Logprob(logprob=-699.3170166015625, rank=5, decoded_token=' Voice')}, {11: Logprob(logprob=0.0, rank=1, decoded_token=','), 369: Logprob(logprob=-955.3170166015625, rank=2, decoded_token=' for'), 8364: Logprob(logprob=-1080.195068359375, rank=3, decoded_token=' Engine'), 555: Logprob(logprob=-1167.609619140625, rank=4, decoded_token=' by'), 4817: Logprob(logprob=-1192.585205078125, rank=5, decoded_token=' engine')}, {2383: Logprob(logprob=0.0, rank=1, decoded_token=' law'), 20510: Logprob(logprob=-1173.853515625, rank=2, decoded_token='law'), 28229: Logprob(logprob=-1448.5853271484375, rank=3, decoded_token=' invention'), 2626: Logprob(logprob=-1492.2926025390625, rank=4, decoded_token=' business'), 5195: Logprob(logprob=-1573.46337890625, rank=5, decoded_token=' Google')}, {1896: Logprob(logprob=0.0, rank=1, decoded_token='.in'), 94470: Logprob(logprob=-1336.195068359375, rank=2, decoded_token='.instrument'), 1265: Logprob(logprob=-1373.658447265625, rank=3, decoded_token='_in'), 22680: Logprob(logprob=-1448.5853271484375, rank=4, decoded_token='.us'), 7611: Logprob(logprob=-1473.5609130859375, rank=5, decoded_token='vention')}, {7611: Logprob(logprob=0.0, rank=1, decoded_token='vention'), 688: Logprob(logprob=-1248.780517578125, rank=2, decoded_token='vent'), 31759: Logprob(logprob=-1498.53662109375, rank=3, decoded_token='ventions'), 46043: Logprob(logprob=-1735.804931640625, rank=4, decoded_token='venture'), 59093: Logprob(logprob=-1935.60986328125, rank=5, decoded_token='vasion')}, {1896: Logprob(logprob=0.0, rank=1, decoded_token='.in'), 10108: Logprob(logprob=-1223.804931640625, rank=2, decoded_token='.date'), 1265: Logprob(logprob=-1461.0732421875, rank=3, decoded_token='_in'), 3502: Logprob(logprob=-1710.829345703125, rank=4, decoded_token='-in'), 44320: Logprob(logprob=-1729.56103515625, rank=5, decoded_token='.inv')}, {688: Logprob(logprob=0.0, rank=1, decoded_token='vent'), 7611: Logprob(logprob=-1198.829345703125, rank=2, decoded_token='vention'), 31759: Logprob(logprob=-1660.878173828125, rank=3, decoded_token='ventions'), 77282: Logprob(logprob=-1773.2684326171875, rank=4, decoded_token='vant'), 82920: Logprob(logprob=-1785.7562255859375, rank=5, decoded_token='venting')}, {269: Logprob(logprob=0.0, rank=1, decoded_token='or'), 1105: Logprob(logprob=-1735.8048095703125, rank=2, decoded_token='ors'), 261: Logprob(logprob=-1748.292724609375, rank=3, decoded_token='er'), 3093: Logprob(logprob=-1885.6585693359375, rank=4, decoded_token='ored'), 324: Logprob(logprob=-1923.1219482421875, rank=5, decoded_token='ur')}, {11: Logprob(logprob=0.0, rank=1, decoded_token=','), 74529: Logprob(logprob=-1923.1220703125, rank=2, decoded_token=' Sergey'), 5195: Logprob(logprob=-2004.292724609375, rank=3, decoded_token=' Google'), 1161: Logprob(logprob=-2004.292724609375, rank=4, decoded_token='(s'), 14783: Logprob(logprob=-2010.5367431640625, rank=5, decoded_token='Google')}, {74529: Logprob(logprob=0.0, rank=1, decoded_token=' Sergey'), 85098: Logprob(logprob=-1223.8048095703125, rank=2, decoded_token=' Sergei'), 30390: Logprob(logprob=-1336.1949462890625, rank=3, decoded_token=' Larry'), 66294: Logprob(logprob=-1392.39013671875, rank=4, decoded_token=' Sergio'), 34297: Logprob(logprob=-1498.5364990234375, rank=5, decoded_token=' Luis')}, {3320: Logprob(logprob=0.0, rank=1, decoded_token=' Br'), 5874: Logprob(logprob=-899.121826171875, rank=2, decoded_token=' Page'), 74529: Logprob(logprob=-911.609619140625, rank=3, decoded_token=' Sergey'), 5195: Logprob(logprob=-1055.2193603515625, rank=4, decoded_token=' Google'), 88920: Logprob(logprob=-1123.90234375, rank=5, decoded_token=' Bray')}, {258: Logprob(logprob=0.0, rank=1, decoded_token='in'), 77: Logprob(logprob=-1011.51220703125, rank=2, decoded_token='n'), 318: Logprob(logprob=-1180.0975341796875, rank=3, decoded_token='im'), 17851: Logprob(logprob=-1192.5853271484375, rank=4, decoded_token='dn'), 1354: Logprob(logprob=-1223.804931640625, rank=5, decoded_token='ins')}, {1237: Logprob(logprob=0.0, rank=1, decoded_token=');'), 8: Logprob(logprob=-1348.68310546875, rank=2, decoded_token=')'), 6030: Logprob(logprob=-1598.4390869140625, rank=3, decoded_token='));'), 4772: Logprob(logprob=-1604.68310546875, rank=4, decoded_token=\"');\"), 5378: Logprob(logprob=-1635.902587890625, rank=5, decoded_token='];')}, {7: Logprob(logprob=0.0, rank=1, decoded_token='('), 14783: Logprob(logprob=-1835.7073974609375, rank=2, decoded_token='Google'), 320: Logprob(logprob=-1848.1951904296875, rank=3, decoded_token=' ('), 1021: Logprob(logprob=-2048.0, rank=4, decoded_token='(\\n'), 6838: Logprob(logprob=-2054.243896484375, rank=5, decoded_token='(G')}, {14783: Logprob(logprob=0.0, rank=1, decoded_token='Google'), 17943: Logprob(logprob=-1573.46337890625, rank=2, decoded_token='google'), 48255: Logprob(logprob=-1748.292724609375, rank=3, decoded_token='_google'), 38: Logprob(logprob=-1785.756103515625, rank=4, decoded_token='G'), 5195: Logprob(logprob=-1885.6585693359375, rank=5, decoded_token=' Google')}, {7694: Logprob(logprob=0.0, rank=1, decoded_token=' Search'), 6014: Logprob(logprob=-1486.048828125, rank=2, decoded_token='Search'), 2778: Logprob(logprob=-1548.4879150390625, rank=3, decoded_token=' search'), 8409: Logprob(logprob=-1679.60986328125, rank=4, decoded_token=' Ser'), 10947: Logprob(logprob=-1685.853759765625, rank=5, decoded_token='_search')}, {11: Logprob(logprob=0.0, rank=1, decoded_token=','), 13: Logprob(logprob=-1879.4146728515625, rank=2, decoded_token='.'), 128260: Logprob(logprob=-2060.48779296875, rank=3, decoded_token='[Unrelevant]'), 2637: Logprob(logprob=-2085.46337890625, rank=4, decoded_token='.,'), 1896: Logprob(logprob=-2166.63427734375, rank=5, decoded_token='.in')}, {2383: Logprob(logprob=0.0, rank=1, decoded_token=' law'), 20510: Logprob(logprob=-1673.3658447265625, rank=2, decoded_token='law'), 31412: Logprob(logprob=-2060.48779296875, rank=3, decoded_token='-law'), 25333: Logprob(logprob=-2072.9755859375, rank=4, decoded_token='法'), 7658: Logprob(logprob=-2110.43896484375, rank=5, decoded_token=' Law')}, {1896: Logprob(logprob=0.0, rank=1, decoded_token='.in'), 1265: Logprob(logprob=-1548.4879150390625, rank=2, decoded_token='_in'), 3502: Logprob(logprob=-1623.4146728515625, rank=3, decoded_token='-in'), 28229: Logprob(logprob=-1785.7562255859375, rank=4, decoded_token=' invention'), 17886: Logprob(logprob=-1785.7562255859375, rank=5, decoded_token='\\tin')}, {7611: Logprob(logprob=0.0, rank=1, decoded_token='vention'), 688: Logprob(logprob=-1323.707275390625, rank=2, decoded_token='vent'), 31759: Logprob(logprob=-1873.1708984375, rank=3, decoded_token='ventions'), 46043: Logprob(logprob=-1873.1708984375, rank=4, decoded_token='venture'), 4464: Logprob(logprob=-1998.048828125, rank=5, decoded_token='version')}, {1896: Logprob(logprob=0.0, rank=1, decoded_token='.in'), 1265: Logprob(logprob=-1610.9267578125, rank=2, decoded_token='_in'), 3502: Logprob(logprob=-1748.2926025390625, rank=3, decoded_token='-in'), 44320: Logprob(logprob=-1948.097412109375, rank=4, decoded_token='.inv'), 17886: Logprob(logprob=-1985.5609130859375, rank=5, decoded_token='\\tin')}, {688: Logprob(logprob=0.0, rank=1, decoded_token='vent'), 7611: Logprob(logprob=-1723.317138671875, rank=2, decoded_token='vention'), 77282: Logprob(logprob=-2010.53662109375, rank=3, decoded_token='vant'), 1653: Logprob(logprob=-2035.51220703125, rank=4, decoded_token='vert'), 1055: Logprob(logprob=-2060.48779296875, rank=5, decoded_token='ven')}, {269: Logprob(logprob=0.0, rank=1, decoded_token='or'), 261: Logprob(logprob=-2172.8779296875, rank=2, decoded_token='er'), 324: Logprob(logprob=-2222.8291015625, rank=3, decoded_token='ur'), 859: Logprob(logprob=-2222.8291015625, rank=4, decoded_token='ator'), 878: Logprob(logprob=-2247.804931640625, rank=5, decoded_token='OR')}, {11: Logprob(logprob=0.0, rank=1, decoded_token=','), 13: Logprob(logprob=-2185.36572265625, rank=2, decoded_token='.'), 2637: Logprob(logprob=-2254.048828125, rank=3, decoded_token='.,'), 17974: Logprob(logprob=-2535.0244140625, rank=4, decoded_token=',.'), 345: Logprob(logprob=-2591.21923828125, rank=5, decoded_token=',\\n')}, {30390: Logprob(logprob=0.0, rank=1, decoded_token=' Larry'), 89595: Logprob(logprob=-1073.9512939453125, rank=2, decoded_token='Larry'), 28574: Logprob(logprob=-1148.8780517578125, rank=3, decoded_token=' Lawrence'), 445: Logprob(logprob=-1186.341552734375, rank=4, decoded_token=' L'), 5874: Logprob(logprob=-1286.243896484375, rank=5, decoded_token=' Page')}, {5874: Logprob(logprob=0.0, rank=1, decoded_token=' Page'), 2732: Logprob(logprob=-1273.7562255859375, rank=2, decoded_token='Page'), 43455: Logprob(logprob=-1523.5123291015625, rank=3, decoded_token=' Pag'), 22521: Logprob(logprob=-1548.4879150390625, rank=4, decoded_token=' Pages'), 52640: Logprob(logprob=-1585.9512939453125, rank=5, decoded_token='_Page')}, {8: Logprob(logprob=0.0, rank=1, decoded_token=')'), 1237: Logprob(logprob=-1311.219482421875, rank=2, decoded_token=');'), 60: Logprob(logprob=-1398.634033203125, rank=3, decoded_token=']'), 16726: Logprob(logprob=-1473.5609130859375, rank=4, decoded_token='_)'), 340: Logprob(logprob=-1511.0242919921875, rank=5, decoded_token=')\\n')}, {128264: Logprob(logprob=0.0, rank=1, decoded_token='</paragraph>'), 74529: Logprob(logprob=-1286.243896484375, rank=2, decoded_token=' Sergey'), 82960: Logprob(logprob=-1411.1219482421875, rank=3, decoded_token=';base'), 40514: Logprob(logprob=-1504.7803955078125, rank=4, decoded_token=';s'), 75262: Logprob(logprob=-1585.951171875, rank=5, decoded_token=';element')}, {50: Logprob(logprob=0.0, rank=1, decoded_token='S'), 14783: Logprob(logprob=-374.63427734375, rank=2, decoded_token='Google'), 6014: Logprob(logprob=-986.53662109375, rank=3, decoded_token='Search'), 32845: Logprob(logprob=-999.0244140625, rank=4, decoded_token='Ser'), 74529: Logprob(logprob=-1024.0001220703125, rank=5, decoded_token=' Sergey')}, {10286: Logprob(logprob=0.0, rank=1, decoded_token='erge'), 1216: Logprob(logprob=-1061.463623046875, rank=2, decoded_token='ey'), 2431: Logprob(logprob=-1211.317138671875, rank=3, decoded_token='erg'), 74529: Logprob(logprob=-1660.878173828125, rank=4, decoded_token=' Sergey'), 43043: Logprob(logprob=-1685.853759765625, rank=5, decoded_token='ergy')}, {88: Logprob(logprob=0.0, rank=1, decoded_token='y'), 72: Logprob(logprob=-1123.9024658203125, rank=2, decoded_token='i'), 16618: Logprob(logprob=-1585.9512939453125, rank=3, decoded_token='iy'), 7911: Logprob(logprob=-1592.1951904296875, rank=4, decoded_token='ya'), 14029: Logprob(logprob=-1604.6829833984375, rank=5, decoded_token='vy')}, {3320: Logprob(logprob=0.0, rank=1, decoded_token=' Br'), 30444: Logprob(logprob=-1398.6341552734375, rank=2, decoded_token=' Bin'), 6971: Logprob(logprob=-1473.56103515625, rank=3, decoded_token='Br'), 426: Logprob(logprob=-1698.341552734375, rank=4, decoded_token=' B'), 2563: Logprob(logprob=-1710.829345703125, rank=5, decoded_token=' Bl')}, {258: Logprob(logprob=0.0, rank=1, decoded_token='in'), 318: Logprob(logprob=-1386.1463623046875, rank=2, decoded_token='im'), 2259: Logprob(logprob=-1548.48779296875, rank=3, decoded_token='ina'), 6258: Logprob(logprob=-1548.48779296875, rank=4, decoded_token='inn'), 1354: Logprob(logprob=-1610.9267578125, rank=5, decoded_token='ins')}, {128262: Logprob(logprob=0.0, rank=1, decoded_token='[Partially Relevant]'), 128260: Logprob(logprob=-224.7803955078125, rank=2, decoded_token='[Unrelevant]'), 58: Logprob(logprob=-262.2437744140625, rank=3, decoded_token='['), 128261: Logprob(logprob=-736.7803955078125, rank=4, decoded_token='[Fully Relevant]'), 11: Logprob(logprob=-1155.121826171875, rank=5, decoded_token=',')}, {89595: Logprob(logprob=0.0, rank=1, decoded_token='Larry'), 30390: Logprob(logprob=-1123.90234375, rank=2, decoded_token=' Larry'), 43: Logprob(logprob=-1348.682861328125, rank=3, decoded_token='L'), 39166: Logprob(logprob=-1798.243896484375, rank=4, decoded_token='Law'), 8921: Logprob(logprob=-1998.0487060546875, rank=5, decoded_token='La')}, {5874: Logprob(logprob=0.0, rank=1, decoded_token=' Page'), 2732: Logprob(logprob=-1523.51220703125, rank=2, decoded_token='Page'), 2199: Logprob(logprob=-1823.219482421875, rank=3, decoded_token=' page'), 6257: Logprob(logprob=-1885.65869140625, rank=4, decoded_token='_page'), 66539: Logprob(logprob=-1898.146484375, rank=5, decoded_token='\\tPage')}, {128262: Logprob(logprob=0.0, rank=1, decoded_token='[Partially Relevant]'), 128260: Logprob(logprob=-1086.43896484375, rank=2, decoded_token='[Unrelevant]'), 58: Logprob(logprob=-1098.9267578125, rank=3, decoded_token='['), 128261: Logprob(logprob=-1223.8048095703125, rank=4, decoded_token='[Fully Relevant]'), 38173: Logprob(logprob=-2023.0244140625, rank=5, decoded_token='[F')}, {128258: Logprob(logprob=0.0, rank=1, decoded_token='[Continue to Retrieve Evidence]'), 128256: Logprob(logprob=-112.39013671875, rank=2, decoded_token='[No Retrieval]'), 14783: Logprob(logprob=-374.634033203125, rank=3, decoded_token='Google'), 5195: Logprob(logprob=-936.5853271484375, rank=4, decoded_token=' Google'), 14109: Logprob(logprob=-1036.48779296875, rank=5, decoded_token='Unknown')}, {128263: Logprob(logprob=0.0, rank=1, decoded_token='<paragraph>'), 128258: Logprob(logprob=-2416.390380859375, rank=2, decoded_token='[Continue to Retrieve Evidence]'), 9355: Logprob(logprob=-2541.268310546875, rank=3, decoded_token='ema'), 370: Logprob(logprob=-2553.75634765625, rank=4, decoded_token='ab'), 797: Logprob(logprob=-2628.68310546875, rank=5, decoded_token='eg')}, {16455: Logprob(logprob=0.0, rank=1, decoded_token='people'), 20510: Logprob(logprob=-62.439208984375, rank=2, decoded_token='law'), 27243: Logprob(logprob=-362.146484375, rank=3, decoded_token='business'), 5581: Logprob(logprob=-586.927001953125, rank=4, decoded_token='common'), 3231: Logprob(logprob=-586.927001953125, rank=5, decoded_token='base')}, {31970: Logprob(logprob=0.0, rank=1, decoded_token='.person'), 2337: Logprob(logprob=-986.53662109375, rank=2, decoded_token='.de'), 69047: Logprob(logprob=-1386.1463623046875, rank=3, decoded_token='.family'), 93605: Logprob(logprob=-1523.51220703125, rank=4, decoded_token='.prof'), 49190: Logprob(logprob=-1536.0, rank=5, decoded_token='.eth')}, {93605: Logprob(logprob=0.0, rank=1, decoded_token='.prof'), 1276: Logprob(logprob=-674.3414306640625, rank=2, decoded_token='.n'), 49190: Logprob(logprob=-786.731689453125, rank=3, decoded_token='.eth'), 13: Logprob(logprob=-836.682861328125, rank=4, decoded_token='.'), 10265: Logprob(logprob=-849.170654296875, rank=5, decoded_token='.ed')}, {1362: Logprob(logprob=0.0, rank=1, decoded_token='ession'), 16151: Logprob(logprob=-1685.853515625, rank=2, decoded_token='essional'), 8719: Logprob(logprob=-1798.243896484375, rank=3, decoded_token='essions'), 434: Logprob(logprob=-1898.146240234375, rank=4, decoded_token='ess'), 10603: Logprob(logprob=-2360.195068359375, rank=5, decoded_token='iciency')}], finish_reason=length, stop_reason=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#是否带special token 分开传入，用于检索和生成\n",
    "sampling_params = SamplingParams(\n",
    "            temperature=0.01, top_p=1.0,max_tokens=100, logprobs=5, skip_special_tokens=False, include_stop_str_in_output=True)\n",
    "PROMPT_DICT = {\"llama3\": '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n{input}<|eot_id|><|start_header_id|>assistant<|end_header_id|>'}\n",
    "model.generate([PROMPT_DICT[\"llama3\"].format(input=\"what all does google now do?\")], sampling_params)[0].outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('/media/disk2/llama_factory/generation_1124_special')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_special_tokens(tokenizer, use_grounding=False, use_utility=False):\n",
    "    rel_tokens = {}\n",
    "    for token in ['[Unrelevant]','[Partially Relevant]','[Fully Relevant]']:\n",
    "        rel_tokens[token] = tokenizer.convert_tokens_to_ids(token)\n",
    "\n",
    "    # ut_tokens = None\n",
    "    # if use_utility is True:\n",
    "    #     ut_tokens = {}\n",
    "    #     for token in utility_tokens_names:\n",
    "    #         ut_tokens[token] = tokenizer.convert_tokens_to_ids(token)\n",
    "\n",
    "    return rel_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_tokens = load_special_tokens(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from src.sparql_utils import *\n",
    "def run_step_generation_batch(model, prompt, topic_entity,new_retrieval, beam_width=3):\n",
    "    pattern = r'(.*?)\\[(.*?)\\]'\n",
    "    rel_score_dict = {}\n",
    "    return_entities = []\n",
    "    final_preds = []\n",
    "    overall_scores = {}\n",
    "    paragraph = ';'.join([page.page_content.strip() for page in retriever.invoke(prompt.split('\\n\\n')[1].split('<|eot_id|>')[0])])\n",
    "    print(paragraph)\n",
    "    if new_retrieval:\n",
    "        retrieval_token = \"[New Retrieval]\"\n",
    "        aug_prompts =  [\"<paragraph>{}</paragraph>\".format(paragraph)]\n",
    "    else:\n",
    "        retrieval_token = \"[Continue to Retrieve Evidence]\"\n",
    "        aug_prompts =  [\"<paragraph>{}</paragraph>\".format(paragraph)]\n",
    "    \n",
    "    pred = model.generate(prompt + retrieval_token + aug_prompts[0], sampling_params)[0]\n",
    "    pred_token_ids = pred.outputs[0].token_ids\n",
    "    pred_text_1 = pred.outputs[0].text\n",
    "    pred_log_probs = pred.outputs[0].logprobs\n",
    "    seq_score = pred.outputs[0].cumulative_logprob / \\\n",
    "        max(len(pred.outputs[0].token_ids), 1)\n",
    "    relevance_indices = []\n",
    "    for tok_idx, tok in enumerate(pred_token_ids):\n",
    "        if tok in rel_tokens.values():\n",
    "            relevance_indices.append(tok_idx)\n",
    "    if len(relevance_indices) > 0:\n",
    "        for idx in relevance_indices:\n",
    "            for token, token_id in rel_tokens.items():\n",
    "                prob = pred_log_probs[idx][token_id].logprob if token_id in pred_log_probs[idx] else -100\n",
    "                rel_score_dict[token] = np.exp(prob)\n",
    "    relevance_score = rel_score_dict['[Fully Relevant]']+ rel_score_dict['[Partially Relevant]'] / np.sum(list(rel_score_dict.values()))\n",
    "    processed_pred = pred_text_1.split('[Retrieve Entity]')[0]\n",
    "    matches =  dict(re.findall(pattern,processed_pred))\n",
    "    \n",
    "    name2id = dict()\n",
    "    entity_prompts = []\n",
    "    for _, entity in enumerate(topic_entity):\n",
    "        entities = []\n",
    "        for k, v in matches.items():\n",
    "            if v in ['Fully Relevant', 'Partially Relevant']:\n",
    "                another_entities = get_another_entity(entity, k, return_label=True)\n",
    "                # print(another_entities)\n",
    "                name2id.update(another_entities)\n",
    "                entities.extend([f'({get_label(entity)}, {k}, {e})' for e in another_entities.values()])\n",
    "        entity_prompts.append(aug_prompts[0] + processed_pred +  '[Retrieve Entity]' + \"<paragraph>{}</paragraph>\".format(';'.join(entities[:10])))\n",
    "    # print(aug_prompts)\n",
    "    preds = model.generate([prompt + retrieval_token+ entity_prompts[i] for i in range(len(entity_prompts))], sampling_params)\n",
    "    \n",
    "    for p_idx, pred in enumerate(preds):\n",
    "        pred_token_ids = pred.outputs[0].token_ids\n",
    "        pred_text_2 = pred.outputs[0].text\n",
    "        pred_log_probs = pred.outputs[0].logprobs\n",
    "        rel_score_dict = {}\n",
    "        relevance_indices = []\n",
    "        for tok_idx, tok in enumerate(pred_token_ids):\n",
    "            if tok in rel_tokens.values():\n",
    "                relevance_indices.append(tok_idx)\n",
    "        if len(relevance_indices) > 0:\n",
    "            # print(relevance_indices)\n",
    "            for idx in relevance_indices:\n",
    "                for token, token_id in rel_tokens.items():\n",
    "                    prob = pred_log_probs[idx][token_id].logprob if token_id in pred_log_probs[idx] else -100\n",
    "                    rel_score_dict[token] = np.exp(prob)\n",
    "        overall_scores[p_idx] = relevance_score + rel_score_dict['[Fully Relevant]'] + rel_score_dict['[Partially Relevant]']/ np.sum(list(rel_score_dict.values()))\n",
    "        if '[Continue to Retrieve Evidence]' in pred_text_2:\n",
    "            processed_pred = pred_text_2.split('[Continue to Retrieve Evidence]')[0]\n",
    "            matches =  dict(re.findall(pattern, processed_pred))\n",
    "            for k, v in matches.items():\n",
    "                if v in ['Fully Relevant', 'Partially Relevant']:\n",
    "                    if k in name2id:\n",
    "                        return_entities.append(name2id[k])\n",
    "            processed_pred += '[Continue to Retrieve Evidence]'\n",
    "        else:\n",
    "            processed_pred = pred_text_2\n",
    "        final_preds.append(aug_prompts[0] + processed_pred)\n",
    "    return final_preds, [overall_scores[p_idx] for p_idx in overall_scores], return_entities\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1639\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('./data/merged/WebQSP_test.json', 'r',encoding='utf-8') as f:\n",
    "    test_data = json.load(f)\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 4\n",
      "location.us_state.capital;people.family.country;people.ethnicity.geographic_distribution;fictional_universe.fictional_character.place_of_birth;people.marriage.from\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 55.59 toks/s, output: 63.68 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  2.54it/s, est. speed input: 193.99 toks/s, output: 63.81 toks/s]\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "# for i in range(len(test_data)):\n",
    "i=6\n",
    "print(f'Process {i}')\n",
    "data_input = test_data[i]['question']\n",
    "prompt = PROMPT_DICT['llama3'].format(input= data_input)\n",
    "max_depth = 3\n",
    "topic_entity = list(test_data[i]['gold_entity_map'].keys())\n",
    "# pred = model.generate([prompt], sampling_params)[0]\n",
    "# pred_text = pred.outputs[0].text\n",
    "# if '[New Retrieval]' in pred_text:\n",
    "curr_depth = 1\n",
    "terminated = False\n",
    "node_id = 0\n",
    "prediction_tree = {}\n",
    "levels = {}\n",
    "prediction_tree[node_id] = {\"prompt\": prompt, \"pred\": \"[New Retrieval]\",\n",
    "                            \"processed_pred\": \"\", \"score\": None, \"topic_entity\": topic_entity, \"parent\": None}\n",
    "levels[0] = [0]\n",
    "while curr_depth < max_depth:\n",
    "    levels[curr_depth] = []\n",
    "    if curr_depth-1 in levels and terminated is False:\n",
    "        for node in levels[curr_depth-1]:\n",
    "            pred = prediction_tree[node][\"pred\"]\n",
    "            if \"<|eot_id|>\" in pred:\n",
    "                terminated = True\n",
    "                continue\n",
    "            prompt = prediction_tree[node][\"prompt\"]\n",
    "            prev_generation = prediction_tree[node][\"processed_pred\"]\n",
    "            score = prediction_tree[node][\"score\"]\n",
    "            topic_entity = prediction_tree[node][\"topic_entity\"]\n",
    "            if \"[New Retrieval]\" in pred or \"[Continue to Retrieve Evidence]\" in pred:\n",
    "                retrieval_results = {}\n",
    "                preds, scores, next_entities = run_step_generation_batch(\n",
    "                    model, prompt + prev_generation, topic_entity, new_retrieval=True if (\"[New Retrieval]\" in pred) else False)\n",
    "                for i, (pred, p_score) in enumerate(zip(preds, scores)):\n",
    "                    retrieval_results[i] = {\n",
    "                        \"pred\": pred, \"score\": p_score}\n",
    "\n",
    "                for i, result in retrieval_results.items():\n",
    "                    node_id += 1\n",
    "                    node_score = result[\"score\"] * \\\n",
    "                        score if score is not None else result[\"score\"]\n",
    "                    pred = result[\"pred\"]\n",
    "                    if len(next_entities) == 0:\n",
    "                        next_entities = [topic_entity]\n",
    "                    prediction_tree[node_id] = {\"prompt\": prompt + prev_generation, \"pred\": pred,\n",
    "                                                \"score\": node_score, \"parent\": node,\n",
    "                                                \"topic_entity\": next_entities[0]}\n",
    "                    if \"[Continue to Retrieve Evidence]\" in pred:\n",
    "                        gen_result_index = pred.index(\"[Continue to Retrieve Evidence]\")\n",
    "                        prev_generation = pred[:gen_result_index]\n",
    "                    else:\n",
    "                        prev_generation = pred\n",
    "                    prediction_tree[node_id][\"processed_pred\"] = prev_generation\n",
    "                    levels[curr_depth].append(node_id)\n",
    "            current_rank = levels[curr_depth]\n",
    "            node2score = {\n",
    "                node_id: prediction_tree[node_id][\"score\"] for node_id in current_rank}\n",
    "            top_nodes = sorted(node2score.items(), key=lambda x: x[1], reverse=True)[\n",
    "                :2]\n",
    "            levels[curr_depth] = [node[0] for node in top_nodes]\n",
    "            curr_depth += 1\n",
    "    else:\n",
    "        break\n",
    "labels = [get_label(ans) for ans in test_data[i]['answer']]\n",
    "for label in labels:\n",
    "    if label in prediction_tree[len(prediction_tree)-1]['processed_pred']:\n",
    "        print(f'correct {i}')\n",
    "        count += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['m.03_r3']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(test_data[i]['gold_entity_map'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'prompt': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nwhere was george washington carver from<|eot_id|><|start_header_id|>assistant<|end_header_id|>',\n",
       "  'pred': '[New Retrieval]',\n",
       "  'processed_pred': '',\n",
       "  'score': None,\n",
       "  'topic_entity': ['m.03djm'],\n",
       "  'parent': None},\n",
       " 1: {'prompt': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nwhere was george washington carver from<|eot_id|><|start_header_id|>assistant<|end_header_id|>',\n",
       "  'pred': '<paragraph>location.us_state.capital;people.family.country;people.ethnicity.geographic_distribution;fictional_universe.fictional_character.place_of_birth;people.marriage.from</paragraph>people.family.country[Fully Relevant]people.ethnicity.geographic_distribution[Partially Relevant]location.us_state.capital[Unrelevant][Retrieve Entity]<paragraph>[No Retrieval]Answer: Minneapolis<|eot_id|>',\n",
       "  'score': 2.578040019318527e-07,\n",
       "  'parent': 0,\n",
       "  'topic_entity': ['m.03djm'],\n",
       "  'processed_pred': '<paragraph>location.us_state.capital;people.family.country;people.ethnicity.geographic_distribution;fictional_universe.fictional_character.place_of_birth;people.marriage.from</paragraph>people.family.country[Fully Relevant]people.ethnicity.geographic_distribution[Partially Relevant]location.us_state.capital[Unrelevant][Retrieve Entity]<paragraph>[No Retrieval]Answer: Minneapolis<|eot_id|>'}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtracking_prediction_tree(levels: dict[int,list[int]], curr_depth: int, prediction_tree: dict[int, dict]) -> dict[int,list[int]]:\n",
    "    '''\n",
    "    get best tracking from prediction_tree base on levels\n",
    "    '''\n",
    "    parent = 0 \n",
    "    best_selections = {}\n",
    "    # Traverse from the bottom \n",
    "    levels = {k: v for k, v in levels.items() if len(v) > 0 and k != 0} # remove empty list in levels\n",
    "    for path_i, node in enumerate(levels[len(levels)]): # beam search \n",
    "        if node == 0:\n",
    "            break\n",
    "        best_selections[path_i] = [node] \n",
    "        current_node = node \n",
    "        current_level = curr_depth \n",
    "        if current_node is None:\n",
    "            continue\n",
    "        while current_level > 0 and current_node is not None:\n",
    "            parent = prediction_tree[current_node][\"parent\"]\n",
    "            best_selections[path_i] = [parent] + best_selections[path_i] \n",
    "            current_node = parent \n",
    "            current_level -= 1\n",
    "    return best_selections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_selections = backtracking_prediction_tree(levels, curr_depth, prediction_tree)\n",
    "final_prediction = {}\n",
    "splitted_sentences = {}\n",
    "original_splitted_sentences = {}\n",
    "ctxs = {}\n",
    "for path_i, nodes in best_selections.items():\n",
    "    final_prediction[path_i] = \" \".join([prediction_tree[node][\"processed_pred\"] for node in nodes if node is not None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: ' <paragraph>government.us_vice_president.to_president;government.us_president.vice_president;government.politician.government_positions_held;government.political_appointer.appointees;base.obamabase.cabinet_member.cabinet_position</paragraph>None[Fully Relevant]George M. Dallas[Unrelevant][No Retrieval]Answer: Speaker of the House of Representatives;President of the House of Representatives<|eot_id|>'}\n"
     ]
    }
   ],
   "source": [
    "print(final_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from src.sparql_utils import *\n",
    "def run_relation_generation_batch(model, prompt, new_retrieval, context):\n",
    "    rel_score_dict = {}\n",
    "    final_preds = []\n",
    "    overall_scores = {}\n",
    "    paragraph = ';'.join([page.page_content.strip() for page in retriever.invoke(prompt.split('\\n\\n')[1].split('<|eot_id|>')[0])])\n",
    "    if new_retrieval:\n",
    "        retrieval_token = \"[New Retrieval]\"\n",
    "        aug_prompts =  [\"<paragraph>{}</paragraph>\".format(paragraph)]\n",
    "    else:\n",
    "        retrieval_token = \"[Continue to Retrieve Evidence]\"\n",
    "        aug_prompts =  [\"<paragraph>{}</paragraph>\".format(paragraph)]\n",
    "    \n",
    "    pred = model.generate(prompt + retrieval_token + aug_prompts[0], sampling_params)[0]\n",
    "    pred_token_ids = pred.outputs[0].token_ids\n",
    "    pred_text_1 = pred.outputs[0].text\n",
    "    pred_log_probs = pred.outputs[0].logprobs\n",
    "    seq_score = pred.outputs[0].cumulative_logprob / \\\n",
    "        max(len(pred.outputs[0].token_ids), 1)\n",
    "    relevance_indices = []\n",
    "    for tok_idx, tok in enumerate(pred_token_ids):\n",
    "        if tok in rel_tokens.values():\n",
    "            relevance_indices.append(tok_idx)\n",
    "    if len(relevance_indices) > 0:\n",
    "        for idx in relevance_indices:\n",
    "            for token, token_id in rel_tokens.items():\n",
    "                prob = pred_log_probs[idx][token_id].logprob if token_id in pred_log_probs[idx] else -100\n",
    "                rel_score_dict[token] = np.exp(prob)\n",
    "    relevance_score = rel_score_dict['[Fully Relevant]']+ rel_score_dict['[Partially Relevant]'] / np.sum(list(rel_score_dict.values()))\n",
    "    assert '[Retrieve Entity]' in pred_text_1\n",
    "    processed_pred = pred_text_1.split('[Retrieve Entity]')[0] + '[Retrieve Entity]'\n",
    "    return [retrieval_token + aug_prompts[0] + processed_pred], [relevance_score], pred_text_1.split('[Retrieve Entity]')[0]\n",
    "\n",
    "def run_entity_generation_batch(model, prompt, topic_entity, context):\n",
    "    final_preds = []\n",
    "    overall_scores = {}\n",
    "    return_entities = dict()\n",
    "    pattern = r'(.*?)\\[(.*?)\\]'\n",
    "    matches =  dict(re.findall(pattern,context))\n",
    "    name2id = dict()\n",
    "    entity_prompts = []\n",
    "    for _, entity in enumerate(topic_entity):\n",
    "        entities = []\n",
    "        for k, v in matches.items():\n",
    "            if v in ['Fully Relevant', 'Partially Relevant']:\n",
    "                another_entities = get_another_entity(entity, k, return_label=True)\n",
    "                # if len(another_entities) > 10:\n",
    "                #     another_entities = another_entities[:5]\n",
    "                # print(another_entities)\n",
    "                name2id.update(another_entities)\n",
    "                entities.extend([f'({get_label(entity)}, {k}, {e})' for e in another_entities.values()])\n",
    "                entity_prompts.append(prompt+  '[Retrieve Entity]' + \"<paragraph>{}</paragraph>\".format(';'.join(entities[:10]) if len(entities) else 'No triplets Received'))\n",
    "    # print(aug_prompts)\n",
    "    preds = model.generate([prompt + entity_prompts[i] for i in range(len(entity_prompts))], sampling_params)\n",
    "    \n",
    "    for p_idx, pred in enumerate(preds):\n",
    "        pred_token_ids = pred.outputs[0].token_ids\n",
    "        pred_text_2 = pred.outputs[0].text\n",
    "        pred_log_probs = pred.outputs[0].logprobs\n",
    "        rel_score_dict = {}\n",
    "        relevance_indices = []\n",
    "        for tok_idx, tok in enumerate(pred_token_ids):\n",
    "            if tok in rel_tokens.values():\n",
    "                relevance_indices.append(tok_idx)\n",
    "        if len(relevance_indices) > 0:\n",
    "            # print(relevance_indices)\n",
    "            for idx in relevance_indices:\n",
    "                for token, token_id in rel_tokens.items():\n",
    "                    prob = pred_log_probs[idx][token_id].logprob if token_id in pred_log_probs[idx] else -100\n",
    "                    rel_score_dict[token] = np.exp(prob)\n",
    "        overall_scores[p_idx] = rel_score_dict['[Fully Relevant]'] + rel_score_dict['[Partially Relevant]']/ np.sum(list(rel_score_dict.values()))\n",
    "        if '[Continue to Retrieve Evidence]' in pred_text_2:\n",
    "            processed_pred = pred_text_2.split('[Continue to Retrieve Evidence]')[0]\n",
    "            matches =  dict(re.findall(pattern, processed_pred))\n",
    "            for k, v in matches.items():\n",
    "                if v in ['Fully Relevant', 'Partially Relevant']:\n",
    "                    if k in name2id:\n",
    "                        return_entities[k] = name2id[k]\n",
    "            processed_pred += '[Continue to Retrieve Evidence]'\n",
    "        else:\n",
    "            processed_pred = pred_text_2\n",
    "        final_preds.append(processed_pred)\n",
    "    return final_preds, [overall_scores[p_idx] for p_idx in overall_scores], return_entities.values(), ' '.join(return_entities.keys()) \n",
    " \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.01s/it, est. speed input: 57.49 toks/s, output: 91.18 toks/s]\n",
      "Processed prompts: 100%|██████████| 3/3 [00:00<00:00,  3.07it/s, est. speed input: 603.88 toks/s, output: 151.99 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.02s/it, est. speed input: 70.75 toks/s, output: 98.25 toks/s]\n",
      "Processed prompts: 100%|██████████| 6/6 [00:01<00:00,  4.94it/s, est. speed input: 1306.21 toks/s, output: 308.21 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.02s/it, est. speed input: 172.19 toks/s, output: 97.83 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.03s/it, est. speed input: 55.33 toks/s, output: 97.07 toks/s]\n",
      "Processed prompts: 100%|██████████| 4/4 [00:01<00:00,  3.73it/s, est. speed input: 603.71 toks/s, output: 372.65 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.06it/s, est. speed input: 128.21 toks/s, output: 105.96 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.00s/it, est. speed input: 66.83 toks/s, output: 99.74 toks/s]\n",
      "Processed prompts: 100%|██████████| 6/6 [00:01<00:00,  4.72it/s, est. speed input: 859.64 toks/s, output: 472.32 toks/s]\n",
      "Processed prompts: 100%|██████████| 4/4 [00:01<00:00,  3.56it/s, est. speed input: 1032.34 toks/s, output: 352.41 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.05it/s, est. speed input: 65.30 toks/s, output: 105.33 toks/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00,  9.91it/s, est. speed input: 3312.51 toks/s, output: 109.08 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.88it/s, est. speed input: 103.63 toks/s, output: 99.85 toks/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00,  3.49it/s, est. speed input: 552.32 toks/s, output: 185.26 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.04it/s, est. speed input: 57.53 toks/s, output: 104.60 toks/s]\n",
      "Processed prompts: 100%|██████████| 3/3 [00:00<00:00,  6.57it/s, est. speed input: 1715.99 toks/s, output: 236.68 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.05it/s, est. speed input: 63.19 toks/s, output: 105.32 toks/s]\n",
      "Processed prompts: 100%|██████████| 6/6 [00:01<00:00,  5.25it/s, est. speed input: 884.97 toks/s, output: 326.50 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.79it/s, est. speed input: 118.32 toks/s, output: 102.18 toks/s]\n",
      "Processed prompts: 100%|██████████| 4/4 [00:01<00:00,  3.65it/s, est. speed input: 701.36 toks/s, output: 365.29 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.52it/s, est. speed input: 222.07 toks/s, output: 101.90 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.54it/s, est. speed input: 117.53 toks/s, output: 103.61 toks/s]\n",
      "Processed prompts: 100%|██████████| 4/4 [00:01<00:00,  3.46it/s, est. speed input: 1216.85 toks/s, output: 345.69 toks/s]\n",
      "Processed prompts: 100%|██████████| 4/4 [00:01<00:00,  3.59it/s, est. speed input: 761.88 toks/s, output: 359.37 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.38it/s, est. speed input: 107.70 toks/s, output: 103.55 toks/s]\n",
      "Processed prompts: 100%|██████████| 3/3 [00:00<00:00,  8.31it/s, est. speed input: 2347.89 toks/s, output: 169.08 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.00s/it, est. speed input: 72.83 toks/s, output: 99.76 toks/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:01<00:00,  2.00it/s, est. speed input: 435.41 toks/s, output: 199.73 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.22it/s, est. speed input: 284.43 toks/s, output: 104.21 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.22it/s, est. speed input: 270.45 toks/s, output: 104.02 toks/s]\n",
      "Processed prompts: 100%|██████████| 3/3 [00:00<00:00, 10.10it/s, est. speed input: 5416.51 toks/s, output: 101.05 toks/s]\n",
      "Processed prompts: 100%|██████████| 3/3 [00:00<00:00, 10.68it/s, est. speed input: 5494.43 toks/s, output: 106.89 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.11it/s, est. speed input: 72.45 toks/s, output: 104.77 toks/s]\n",
      "Processed prompts: 100%|██████████| 6/6 [00:01<00:00,  4.69it/s, est. speed input: 946.87 toks/s, output: 469.13 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.05it/s, est. speed input: 148.24 toks/s, output: 105.13 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.10it/s, est. speed input: 101.94 toks/s, output: 104.15 toks/s]\n",
      "Processed prompts: 100%|██████████| 6/6 [00:01<00:00,  4.38it/s, est. speed input: 1548.76 toks/s, output: 437.70 toks/s]\n",
      "Processed prompts: 100%|██████████| 6/6 [00:01<00:00,  4.60it/s, est. speed input: 1176.61 toks/s, output: 459.91 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  2.10it/s, est. speed input: 113.60 toks/s, output: 98.87 toks/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00,  8.48it/s, est. speed input: 1455.96 toks/s, output: 127.33 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.06it/s, est. speed input: 62.59 toks/s, output: 106.08 toks/s]\n",
      "Processed prompts: 100%|██████████| 3/3 [00:01<00:00,  2.86it/s, est. speed input: 515.03 toks/s, output: 286.12 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.04it/s, est. speed input: 183.88 toks/s, output: 104.47 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.06it/s, est. speed input: 160.11 toks/s, output: 106.02 toks/s]\n",
      "Processed prompts: 100%|██████████| 3/3 [00:01<00:00,  2.65it/s, est. speed input: 1097.79 toks/s, output: 265.16 toks/s]\n",
      "Processed prompts: 100%|██████████| 3/3 [00:00<00:00,  3.28it/s, est. speed input: 1193.70 toks/s, output: 259.07 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.05it/s, est. speed input: 79.02 toks/s, output: 105.36 toks/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00,  2.02it/s, est. speed input: 462.18 toks/s, output: 202.26 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.05it/s, est. speed input: 86.17 toks/s, output: 105.09 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.35it/s, est. speed input: 231.30 toks/s, output: 104.15 toks/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00,  9.41it/s, est. speed input: 3960.62 toks/s, output: 84.76 toks/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00,  2.43it/s, est. speed input: 591.96 toks/s, output: 109.84 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.25it/s, est. speed input: 84.94 toks/s, output: 103.67 toks/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00,  6.91it/s, est. speed input: 1528.73 toks/s, output: 166.00 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.04it/s, est. speed input: 96.52 toks/s, output: 103.78 toks/s]\n",
      "Processed prompts: 100%|██████████| 3/3 [00:01<00:00,  2.91it/s, est. speed input: 817.04 toks/s, output: 112.56 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.89it/s, est. speed input: 149.99 toks/s, output: 102.52 toks/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00,  6.40it/s, est. speed input: 1556.23 toks/s, output: 109.31 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.05it/s, est. speed input: 76.63 toks/s, output: 104.97 toks/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:01<00:00,  1.84it/s, est. speed input: 696.23 toks/s, output: 183.70 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.00it/s, est. speed input: 70.18 toks/s, output: 100.25 toks/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:01<00:00,  1.88it/s, est. speed input: 390.63 toks/s, output: 187.80 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.54it/s, est. speed input: 332.34 toks/s, output: 103.56 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.56it/s, est. speed input: 302.84 toks/s, output: 103.03 toks/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00,  2.66it/s, est. speed input: 1324.29 toks/s, output: 178.16 toks/s]\n",
      "Processed prompts: 100%|██████████| 3/3 [00:00<00:00,  3.84it/s, est. speed input: 1780.02 toks/s, output: 180.30 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.35it/s, est. speed input: 99.02 toks/s, output: 97.66 toks/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00,  2.02it/s, est. speed input: 427.99 toks/s, output: 201.87 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.41it/s, est. speed input: 305.41 toks/s, output: 104.62 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.40it/s, est. speed input: 268.35 toks/s, output: 103.96 toks/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00,  2.44it/s, est. speed input: 1214.41 toks/s, output: 180.45 toks/s]\n",
      "Processed prompts: 100%|██████████| 3/3 [00:00<00:00,  3.50it/s, est. speed input: 1579.46 toks/s, output: 180.67 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  2.45it/s, est. speed input: 128.40 toks/s, output: 96.27 toks/s]\n",
      "Processed prompts: 100%|██████████| 3/3 [00:00<00:00,  5.53it/s, est. speed input: 1412.97 toks/s, output: 239.79 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  2.04it/s, est. speed input: 112.67 toks/s, output: 102.42 toks/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00,  7.36it/s, est. speed input: 1285.40 toks/s, output: 132.58 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.59it/s, est. speed input: 101.80 toks/s, output: 101.79 toks/s]\n",
      "Processed prompts: 100%|██████████| 3/3 [00:00<00:00,  4.21it/s, est. speed input: 773.99 toks/s, output: 269.21 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.58it/s, est. speed input: 99.79 toks/s, output: 95.03 toks/s]\n",
      "Processed prompts: 100%|██████████| 3/3 [00:00<00:00, 12.92it/s, est. speed input: 2754.49 toks/s, output: 176.72 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.01s/it, est. speed input: 65.49 toks/s, output: 99.22 toks/s]\n",
      "Processed prompts: 100%|██████████| 4/4 [00:01<00:00,  3.76it/s, est. speed input: 790.59 toks/s, output: 229.10 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  2.03it/s, est. speed input: 114.30 toks/s, output: 93.88 toks/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00,  8.57it/s, est. speed input: 1496.75 toks/s, output: 128.64 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.01s/it, est. speed input: 73.10 toks/s, output: 98.78 toks/s]\n",
      "Processed prompts: 100%|██████████| 3/3 [00:01<00:00,  2.77it/s, est. speed input: 770.58 toks/s, output: 276.85 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.05it/s, est. speed input: 118.87 toks/s, output: 105.20 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.05it/s, est. speed input: 140.88 toks/s, output: 105.12 toks/s]\n",
      "Processed prompts: 100%|██████████| 3/3 [00:01<00:00,  2.69it/s, est. speed input: 959.27 toks/s, output: 269.20 toks/s]\n",
      "Processed prompts: 100%|██████████| 3/3 [00:01<00:00,  2.69it/s, est. speed input: 1071.40 toks/s, output: 268.97 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.06it/s, est. speed input: 79.76 toks/s, output: 106.34 toks/s]\n",
      "Processed prompts: 100%|██████████| 6/6 [00:01<00:00,  5.28it/s, est. speed input: 1221.59 toks/s, output: 289.34 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.44it/s, est. speed input: 86.26 toks/s, output: 103.51 toks/s]\n",
      "Processed prompts: 100%|██████████| 3/3 [00:00<00:00,  3.88it/s, est. speed input: 1139.20 toks/s, output: 249.82 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.06it/s, est. speed input: 52.90 toks/s, output: 105.79 toks/s]\n",
      "Processed prompts: 100%|██████████| 3/3 [00:01<00:00,  2.83it/s, est. speed input: 744.01 toks/s, output: 225.65 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s, est. speed input: 105.45 toks/s, output: 103.93 toks/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:01<00:00,  1.90it/s, est. speed input: 379.16 toks/s, output: 189.58 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.44it/s, est. speed input: 300.42 toks/s, output: 98.21 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.45it/s, est. speed input: 276.87 toks/s, output: 99.09 toks/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00,  2.56it/s, est. speed input: 1220.90 toks/s, output: 166.71 toks/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00,  2.61it/s, est. speed input: 1149.70 toks/s, output: 185.51 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.07it/s, est. speed input: 71.88 toks/s, output: 107.29 toks/s]\n",
      "Processed prompts: 100%|██████████| 4/4 [00:01<00:00,  3.48it/s, est. speed input: 1059.47 toks/s, output: 348.50 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.59it/s, est. speed input: 325.35 toks/s, output: 98.87 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.41it/s, est. speed input: 262.04 toks/s, output: 100.56 toks/s]\n",
      "Processed prompts: 100%|██████████| 4/4 [00:01<00:00,  3.11it/s, est. speed input: 1796.43 toks/s, output: 281.26 toks/s]\n",
      "Processed prompts: 100%|██████████| 4/4 [00:01<00:00,  3.24it/s, est. speed input: 1750.01 toks/s, output: 324.07 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s, est. speed input: 90.90 toks/s, output: 105.80 toks/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 10.14it/s, est. speed input: 1932.63 toks/s, output: 131.87 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.05it/s, est. speed input: 78.00 toks/s, output: 105.41 toks/s]\n",
      "Processed prompts: 100%|██████████| 6/6 [00:01<00:00,  4.71it/s, est. speed input: 1169.62 toks/s, output: 419.96 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.06it/s, est. speed input: 89.63 toks/s, output: 106.70 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.06it/s, est. speed input: 86.89 toks/s, output: 105.95 toks/s]\n",
      "Processed prompts: 100%|██████████| 6/6 [00:01<00:00,  5.00it/s, est. speed input: 1265.12 toks/s, output: 366.45 toks/s]\n",
      "Processed prompts: 100%|██████████| 4/4 [00:01<00:00,  3.66it/s, est. speed input: 867.68 toks/s, output: 289.22 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  2.13it/s, est. speed input: 123.98 toks/s, output: 96.19 toks/s]\n",
      "Processed prompts: 100%|██████████| 3/3 [00:00<00:00, 12.38it/s, est. speed input: 2321.28 toks/s, output: 152.81 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.98it/s, est. speed input: 111.17 toks/s, output: 97.27 toks/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 15.98it/s, est. speed input: 2703.99 toks/s, output: 111.98 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.03it/s, est. speed input: 61.70 toks/s, output: 102.83 toks/s]\n",
      "Processed prompts: 100%|██████████| 3/3 [00:01<00:00,  2.69it/s, est. speed input: 862.11 toks/s, output: 269.38 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.01s/it, est. speed input: 50.61 toks/s, output: 99.23 toks/s]\n",
      "Processed prompts: 100%|██████████| 3/3 [00:01<00:00,  2.92it/s, est. speed input: 464.02 toks/s, output: 113.08 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.71it/s, est. speed input: 132.55 toks/s, output: 94.66 toks/s]\n",
      "Processed prompts: 100%|██████████| 3/3 [00:00<00:00, 10.79it/s, est. speed input: 2559.72 toks/s, output: 154.79 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.76it/s, est. speed input: 113.35 toks/s, output: 95.63 toks/s]\n",
      "Processed prompts: 100%|██████████| 3/3 [00:01<00:00,  2.75it/s, est. speed input: 791.41 toks/s, output: 226.24 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.71it/s, est. speed input: 197.91 toks/s, output: 94.63 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.01s/it, est. speed input: 60.78 toks/s, output: 99.63 toks/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00,  2.00it/s, est. speed input: 355.09 toks/s, output: 107.33 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.06it/s, est. speed input: 140.78 toks/s, output: 105.84 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.90it/s, est. speed input: 118.31 toks/s, output: 103.04 toks/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00,  9.26it/s, est. speed input: 1831.12 toks/s, output: 134.42 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 70.36 toks/s, output: 101.97 toks/s]\n",
      "Processed prompts: 100%|██████████| 3/3 [00:01<00:00,  2.80it/s, est. speed input: 747.55 toks/s, output: 131.14 toks/s]\n",
      "Processed prompts: 0it [00:00, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  2.06it/s, est. speed input: 103.54 toks/s, output: 97.32 toks/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00,  3.93it/s, est. speed input: 574.18 toks/s, output: 184.80 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.86it/s, est. speed input: 130.89 toks/s, output: 95.35 toks/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00,  8.24it/s, est. speed input: 1819.66 toks/s, output: 107.23 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.01s/it, est. speed input: 63.77 toks/s, output: 99.63 toks/s]\n",
      "Processed prompts: 100%|██████████| 3/3 [00:01<00:00,  2.85it/s, est. speed input: 547.42 toks/s, output: 285.11 toks/s]\n",
      "Processed prompts: 0it [00:00, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n",
      "Processed prompts: 0it [00:00, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.16it/s, est. speed input: 107.29 toks/s, output: 96.79 toks/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00,  8.32it/s, est. speed input: 2633.45 toks/s, output: 108.29 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.08it/s, est. speed input: 76.77 toks/s, output: 98.39 toks/s]\n",
      "Processed prompts: 100%|██████████| 3/3 [00:00<00:00,  5.88it/s, est. speed input: 1686.60 toks/s, output: 159.21 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.01s/it, est. speed input: 56.33 toks/s, output: 98.82 toks/s]\n",
      "Processed prompts: 100%|██████████| 3/3 [00:01<00:00,  2.56it/s, est. speed input: 924.41 toks/s, output: 256.06 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.02s/it, est. speed input: 214.59 toks/s, output: 98.89 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.00it/s, est. speed input: 255.77 toks/s, output: 100.30 toks/s]\n",
      "Processed prompts: 100%|██████████| 3/3 [00:01<00:00,  2.37it/s, est. speed input: 1616.25 toks/s, output: 237.33 toks/s]\n",
      "Processed prompts: 100%|██████████| 3/3 [00:01<00:00,  2.36it/s, est. speed input: 1785.29 toks/s, output: 235.83 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.07s/it, est. speed input: 72.05 toks/s, output: 93.57 toks/s]\n",
      "Processed prompts: 100%|██████████| 3/3 [00:01<00:00,  2.80it/s, est. speed input: 633.78 toks/s, output: 280.43 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.00it/s, est. speed input: 172.15 toks/s, output: 100.67 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.01s/it, est. speed input: 86.20 toks/s, output: 99.08 toks/s]\n",
      "Processed prompts: 100%|██████████| 3/3 [00:01<00:00,  2.66it/s, est. speed input: 1102.11 toks/s, output: 266.20 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.05it/s, est. speed input: 252.35 toks/s, output: 105.14 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  2.28it/s, est. speed input: 118.82 toks/s, output: 100.54 toks/s]\n",
      "Processed prompts: 100%|██████████| 3/3 [00:00<00:00, 13.46it/s, est. speed input: 2115.30 toks/s, output: 166.15 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.30it/s, est. speed input: 91.42 toks/s, output: 104.48 toks/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00,  2.50it/s, est. speed input: 536.22 toks/s, output: 120.27 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 59.49 toks/s, output: 100.83 toks/s]\n",
      "Processed prompts: 100%|██████████| 6/6 [00:01<00:00,  4.80it/s, est. speed input: 824.79 toks/s, output: 470.39 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.07it/s, est. speed input: 74.15 toks/s, output: 107.46 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.05it/s, est. speed input: 72.28 toks/s, output: 104.75 toks/s]\n",
      "Processed prompts: 100%|██████████| 6/6 [00:01<00:00,  4.69it/s, est. speed input: 898.88 toks/s, output: 460.76 toks/s]\n",
      "Processed prompts: 100%|██████████| 6/6 [00:01<00:00,  4.74it/s, est. speed input: 908.80 toks/s, output: 465.84 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.83it/s, est. speed input: 131.92 toks/s, output: 102.60 toks/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 10.02it/s, est. speed input: 2247.00 toks/s, output: 120.36 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.04it/s, est. speed input: 67.95 toks/s, output: 104.52 toks/s]\n",
      "Processed prompts: 100%|██████████| 6/6 [00:01<00:00,  4.68it/s, est. speed input: 902.73 toks/s, output: 395.91 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.07it/s, est. speed input: 151.18 toks/s, output: 107.22 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.06it/s, est. speed input: 79.65 toks/s, output: 106.20 toks/s]\n",
      "Processed prompts: 100%|██████████| 6/6 [00:01<00:00,  4.45it/s, est. speed input: 1534.17 toks/s, output: 376.12 toks/s]\n",
      "Processed prompts: 100%|██████████| 6/6 [00:01<00:00,  5.23it/s, est. speed input: 1111.81 toks/s, output: 293.63 toks/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'[Fully Relevant]'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 37\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrieve Entity\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m curr_pred\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m     36\u001b[0m     retrieval_results \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m---> 37\u001b[0m     preds, scores, next_entities,context \u001b[38;5;241m=\u001b[39m \u001b[43mrun_entity_generation_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mprev_generation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopic_entity\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, (pred, p_score) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(preds, scores)):\n\u001b[1;32m     40\u001b[0m         retrieval_results[i] \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     41\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpred\u001b[39m\u001b[38;5;124m\"\u001b[39m: pred, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m\"\u001b[39m: p_score}\n",
      "Cell \u001b[0;32mIn[45], line 73\u001b[0m, in \u001b[0;36mrun_entity_generation_batch\u001b[0;34m(model, prompt, topic_entity, context)\u001b[0m\n\u001b[1;32m     71\u001b[0m             prob \u001b[38;5;241m=\u001b[39m pred_log_probs[idx][token_id]\u001b[38;5;241m.\u001b[39mlogprob \u001b[38;5;28;01mif\u001b[39;00m token_id \u001b[38;5;129;01min\u001b[39;00m pred_log_probs[idx] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m\n\u001b[1;32m     72\u001b[0m             rel_score_dict[token] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexp(prob)\n\u001b[0;32m---> 73\u001b[0m overall_scores[p_idx] \u001b[38;5;241m=\u001b[39m \u001b[43mrel_score_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m[Fully Relevant]\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m+\u001b[39m rel_score_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[Partially Relevant]\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;28mlist\u001b[39m(rel_score_dict\u001b[38;5;241m.\u001b[39mvalues()))\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[Continue to Retrieve Evidence]\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m pred_text_2:\n\u001b[1;32m     75\u001b[0m     processed_pred \u001b[38;5;241m=\u001b[39m pred_text_2\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[Continue to Retrieve Evidence]\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mKeyError\u001b[0m: '[Fully Relevant]'"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "correct_ids = []\n",
    "for index in range(0, 200):\n",
    "# index = 42\n",
    "    hit = 0\n",
    "    print(f'Process {index}')\n",
    "    data_input = test_data[index]['question']\n",
    "    prompt = PROMPT_DICT['llama3'].format(input= data_input)\n",
    "    max_depth = 5\n",
    "    topic_entity = list(test_data[index]['gold_entity_map'].keys())\n",
    "    # pred = model.generate([prompt], sampling_params)[0]\n",
    "    # pred_text = pred.outputs[0].text\n",
    "    # if '[New Retrieval]' in pred_text:\n",
    "    curr_depth = 1\n",
    "    terminated = False\n",
    "    node_id = 0\n",
    "    prediction_tree = {}\n",
    "    levels = {}\n",
    "    prediction_tree[node_id] = {\"prompt\": prompt, \"pred\": \"[New Retrieval]\",\n",
    "                                \"processed_pred\": \"\", \"score\": None, \"topic_entity\": topic_entity, \"parent\": None, \"context\": None}\n",
    "    levels[0] = [0]\n",
    "    while curr_depth < max_depth:\n",
    "        levels[curr_depth] = []\n",
    "        if curr_depth-1 in levels and terminated is False:\n",
    "            for node in levels[curr_depth-1]:\n",
    "                curr_pred = prediction_tree[node][\"pred\"]\n",
    "                if \"<|eot_id|>\" in curr_pred:\n",
    "                    terminated = True\n",
    "                    continue\n",
    "                prompt = prediction_tree[node][\"prompt\"]\n",
    "                prev_generation = prediction_tree[node][\"processed_pred\"]\n",
    "                score = prediction_tree[node][\"score\"]\n",
    "                topic_entity = prediction_tree[node][\"topic_entity\"]\n",
    "                context = prediction_tree[node]['context']\n",
    "                if \"Retrieve Entity\" in curr_pred.split('[')[-1]:\n",
    "                    retrieval_results = {}\n",
    "                    preds, scores, next_entities,context = run_entity_generation_batch(\n",
    "                        model, prompt + prev_generation, topic_entity, context)\n",
    "                    for i, (pred, p_score) in enumerate(zip(preds, scores)):\n",
    "                        retrieval_results[i] = {\n",
    "                            \"pred\": pred, \"score\": p_score}\n",
    "\n",
    "                    for i, result in retrieval_results.items():\n",
    "                        node_id += 1\n",
    "                        node_score = result[\"score\"] * \\\n",
    "                            score if score is not None else result[\"score\"]\n",
    "                        pred = result[\"pred\"]\n",
    "                        if len(next_entities) == 0:\n",
    "                            next_entities = topic_entity\n",
    "                        prediction_tree[node_id] = {\"prompt\": prompt + prev_generation, \"pred\": pred, \"context\": context,\n",
    "                                                    \"score\": node_score, \"parent\": node,\n",
    "                                                    \"topic_entity\": next_entities}\n",
    "                        if \"[Continue to Retrieve Evidence]\" in pred:\n",
    "                            gen_result_index = pred.index(\"[Continue to Retrieve Evidence]\")\n",
    "                            prev_generation = pred[:gen_result_index]\n",
    "                        else:\n",
    "                            prev_generation = pred\n",
    "                        prediction_tree[node_id][\"processed_pred\"] = prev_generation\n",
    "                        levels[curr_depth].append(node_id)\n",
    "                #存在前后逻辑粘连   \n",
    "                if \"New Retrieval\" in curr_pred.split('[')[-1] or \"Continue to Retrieve Evidence\" in curr_pred.split('[')[-1]:\n",
    "                    retrieval_results = {}\n",
    "                    preds, scores, context = run_relation_generation_batch(\n",
    "                        model, prompt + prev_generation, new_retrieval=True if (\"[New Retrieval]\" in curr_pred) else False, context=context)\n",
    "                    for i, (pred, p_score) in enumerate(zip(preds, scores)):\n",
    "                        retrieval_results[i] = {\n",
    "                            \"pred\": pred, \"score\": p_score}\n",
    "\n",
    "                    for i, result in retrieval_results.items():\n",
    "                        node_id += 1\n",
    "                        node_score = result[\"score\"] * \\\n",
    "                            score if score is not None else result[\"score\"]\n",
    "                        pred = result[\"pred\"]\n",
    "                        prediction_tree[node_id] = {\"prompt\": prompt + prev_generation, \"pred\": pred,\n",
    "                                                    \"score\": node_score, \"parent\": node,\n",
    "                                                    \"topic_entity\": topic_entity, \"context\": context}\n",
    "                        if \"[Retrieve Entity]\" in pred:\n",
    "                            gen_result_index = pred.index(\"[Retrieve Entity]\")\n",
    "                            prev_generation = pred[:gen_result_index]\n",
    "                        else:\n",
    "                            prev_generation = pred\n",
    "                        prediction_tree[node_id][\"processed_pred\"] = prev_generation\n",
    "                        levels[curr_depth].append(node_id)\n",
    "            current_rank = levels[curr_depth]\n",
    "            node2score = {\n",
    "                node_id: prediction_tree[node_id][\"score\"] for node_id in current_rank}\n",
    "            top_nodes = sorted(node2score.items(), key=lambda x: x[1], reverse=True)[\n",
    "                :2]\n",
    "            levels[curr_depth] = [node[0] for node in top_nodes]\n",
    "            curr_depth += 1\n",
    "        else:\n",
    "            break\n",
    "    labels = [get_label(ans) for ans in test_data[index]['answer']]\n",
    "    for tree_node in prediction_tree.values():\n",
    "        if 'Answer' in tree_node['processed_pred']:\n",
    "            answer = tree_node['processed_pred'].split('Answer:')[-1]\n",
    "            for label in labels:\n",
    "                if label and label in answer:\n",
    "                    hit = 1\n",
    "    if hit == 1:\n",
    "        # print('Correct')\n",
    "        count += 1\n",
    "        correct_ids.append(index)\n",
    "\n",
    "    #注意有value标签, e.g. WebQTest-31\n",
    "    # for label in labels:\n",
    "    #     if label and label in prediction_tree[len(prediction_tree)-1]['processed_pred']:\n",
    "    #         count += 1\n",
    "    #         break\n",
    "    # except:\n",
    "    #     print(f'{index} Error')\n",
    "    #     continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a b'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = dict({\"a\": 1, \"b\": 2})\n",
    "' '.join(a.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_another_entity('m.045c7b', 'location.location.contains')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'prompt': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nwhat countries are part of the uk<|eot_id|><|start_header_id|>assistant<|end_header_id|>',\n",
       "  'pred': '[New Retrieval]',\n",
       "  'processed_pred': '',\n",
       "  'score': None,\n",
       "  'topic_entity': ['m.07ssc', 'm.0hzjlmp'],\n",
       "  'parent': None,\n",
       "  'context': None},\n",
       " 1: {'prompt': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nwhat countries are part of the uk<|eot_id|><|start_header_id|>assistant<|end_header_id|>',\n",
       "  'pred': '[New Retrieval]<paragraph>base.locations.continents.countries_within;finance.currency.countries_used;royalty.monarch.kingdom;base.unitednations.united_nations_body_membership.member;base.unitednations.united_nations_body.members</paragraph>base.locations.continents.countries_within[Fully Relevant]finance.currency.countries_used[Partially Relevant]royalty.monarch.kingdom[Partially Relevant][Retrieve Entity]',\n",
       "  'score': 1.0,\n",
       "  'parent': 0,\n",
       "  'topic_entity': ['m.07ssc', 'm.0hzjlmp'],\n",
       "  'context': 'base.locations.continents.countries_within[Fully Relevant]finance.currency.countries_used[Partially Relevant]royalty.monarch.kingdom[Partially Relevant]',\n",
       "  'processed_pred': '[New Retrieval]<paragraph>base.locations.continents.countries_within;finance.currency.countries_used;royalty.monarch.kingdom;base.unitednations.united_nations_body_membership.member;base.unitednations.united_nations_body.members</paragraph>base.locations.continents.countries_within[Fully Relevant]finance.currency.countries_used[Partially Relevant]royalty.monarch.kingdom[Partially Relevant]'},\n",
       " 2: {'prompt': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nwhat countries are part of the uk<|eot_id|><|start_header_id|>assistant<|end_header_id|>[New Retrieval]<paragraph>base.locations.continents.countries_within;finance.currency.countries_used;royalty.monarch.kingdom;base.unitednations.united_nations_body_membership.member;base.unitednations.united_nations_body.members</paragraph>base.locations.continents.countries_within[Fully Relevant]finance.currency.countries_used[Partially Relevant]royalty.monarch.kingdom[Partially Relevant]',\n",
       "  'pred': 'United Kingdom[Fully Relevant]Europe[Partially Relevant][Continue to Retrieve Evidence]',\n",
       "  'context': dict_keys([]),\n",
       "  'score': 1.0,\n",
       "  'parent': 1,\n",
       "  'topic_entity': ['m.07ssc', 'm.0hzjlmp'],\n",
       "  'processed_pred': 'United Kingdom[Fully Relevant]Europe[Partially Relevant]'},\n",
       " 3: {'prompt': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nwhat countries are part of the uk<|eot_id|><|start_header_id|>assistant<|end_header_id|>United Kingdom[Fully Relevant]Europe[Partially Relevant]',\n",
       "  'pred': 'Europe[Partially Relevant]Pound sterling[Unrelevant][Continue to Retrieve Evidence]',\n",
       "  'context': dict_keys([]),\n",
       "  'score': 1.8290126261508981e-125,\n",
       "  'parent': 1,\n",
       "  'topic_entity': ['m.07ssc', 'm.0hzjlmp'],\n",
       "  'processed_pred': 'Europe[Partially Relevant]Pound sterling[Unrelevant]'},\n",
       " 4: {'prompt': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nwhat countries are part of the uk<|eot_id|><|start_header_id|>assistant<|end_header_id|>Europe[Partially Relevant]Pound sterling[Unrelevant]',\n",
       "  'pred': 'Europe[Partially Relevant]Pound sterling[Unrelevant]Edward VIII of the United Kingdom[Unrelevant]Queen Victoria[Unrelevant]Edward VII[Unrelevant]Elizabeth II[Unrelevant][Continue to Retrieve Evidence]',\n",
       "  'context': dict_keys([]),\n",
       "  'score': 9.816845187195521e-142,\n",
       "  'parent': 1,\n",
       "  'topic_entity': ['m.07ssc', 'm.0hzjlmp'],\n",
       "  'processed_pred': 'Europe[Partially Relevant]Pound sterling[Unrelevant]Edward VIII of the United Kingdom[Unrelevant]Queen Victoria[Unrelevant]Edward VII[Unrelevant]Elizabeth II[Unrelevant]'},\n",
       " 5: {'prompt': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nwhat countries are part of the uk<|eot_id|><|start_header_id|>assistant<|end_header_id|>Europe[Partially Relevant]Pound sterling[Unrelevant]Edward VIII of the United Kingdom[Unrelevant]Queen Victoria[Unrelevant]Edward VII[Unrelevant]Elizabeth II[Unrelevant]',\n",
       "  'pred': 'No triplets Received[Unrelevant][Continue to Retrieve Evidence]',\n",
       "  'context': dict_keys([]),\n",
       "  'score': 1.0,\n",
       "  'parent': 1,\n",
       "  'topic_entity': ['m.07ssc', 'm.0hzjlmp'],\n",
       "  'processed_pred': 'No triplets Received[Unrelevant]'},\n",
       " 6: {'prompt': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nwhat countries are part of the uk<|eot_id|><|start_header_id|>assistant<|end_header_id|>No triplets Received[Unrelevant]',\n",
       "  'pred': 'No triplets Received[Unrelevant][Continue to Retrieve Evidence]',\n",
       "  'context': dict_keys([]),\n",
       "  'score': 1.0,\n",
       "  'parent': 1,\n",
       "  'topic_entity': ['m.07ssc', 'm.0hzjlmp'],\n",
       "  'processed_pred': 'No triplets Received[Unrelevant]'},\n",
       " 7: {'prompt': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nwhat countries are part of the uk<|eot_id|><|start_header_id|>assistant<|end_header_id|>No triplets Received[Unrelevant]',\n",
       "  'pred': 'No triplets Received[Unrelevant][Continue to Retrieve Evidence]',\n",
       "  'context': dict_keys([]),\n",
       "  'score': 1.0,\n",
       "  'parent': 1,\n",
       "  'topic_entity': ['m.07ssc', 'm.0hzjlmp'],\n",
       "  'processed_pred': 'No triplets Received[Unrelevant]'},\n",
       " 8: {'prompt': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nwhat countries are part of the uk<|eot_id|><|start_header_id|>assistant<|end_header_id|>[New Retrieval]<paragraph>base.locations.continents.countries_within;finance.currency.countries_used;royalty.monarch.kingdom;base.unitednations.united_nations_body_membership.member;base.unitednations.united_nations_body.members</paragraph>base.locations.continents.countries_within[Fully Relevant]finance.currency.countries_used[Partially Relevant]royalty.monarch.kingdom[Partially Relevant]United Kingdom[Fully Relevant]Europe[Partially Relevant]',\n",
       "  'pred': '[Continue to Retrieve Evidence]<paragraph>base.locations.continents.countries_within;finance.currency.countries_used;royalty.monarch.kingdom;base.unitednations.united_nations_body_membership.member;base.unitednations.united_nations_body.members</paragraph>base.locations.continents.countries_within[Fully Relevant]finance.currency.countries_used[Partially Relevant]royalty.monarch.kingdom[Partially Relevant][Retrieve Entity]',\n",
       "  'score': 1.0,\n",
       "  'parent': 2,\n",
       "  'topic_entity': ['m.07ssc', 'm.0hzjlmp'],\n",
       "  'context': 'base.locations.continents.countries_within[Fully Relevant]finance.currency.countries_used[Partially Relevant]royalty.monarch.kingdom[Partially Relevant]',\n",
       "  'processed_pred': '[Continue to Retrieve Evidence]<paragraph>base.locations.continents.countries_within;finance.currency.countries_used;royalty.monarch.kingdom;base.unitednations.united_nations_body_membership.member;base.unitednations.united_nations_body.members</paragraph>base.locations.continents.countries_within[Fully Relevant]finance.currency.countries_used[Partially Relevant]royalty.monarch.kingdom[Partially Relevant]'},\n",
       " 9: {'prompt': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nwhat countries are part of the uk<|eot_id|><|start_header_id|>assistant<|end_header_id|>Europe[Partially Relevant]Pound sterling[Unrelevant]Edward VIII of the United Kingdom[Unrelevant]Queen Victoria[Unrelevant]Edward VII[Unrelevant]Elizabeth II[Unrelevant]No triplets Received[Unrelevant]',\n",
       "  'pred': '[Continue to Retrieve Evidence]<paragraph>base.locations.continents.countries_within;finance.currency.countries_used;royalty.monarch.kingdom;base.unitednations.united_nations_body_membership.member;base.unitednations.united_nations_body.members</paragraph>base.locations.continents.countries_within[Fully Relevant]finance.currency.countries_used[Partially Relevant]royalty.monarch.kingdom[Partially Relevant][Retrieve Entity]',\n",
       "  'score': 1.0,\n",
       "  'parent': 5,\n",
       "  'topic_entity': ['m.07ssc', 'm.0hzjlmp'],\n",
       "  'context': 'base.locations.continents.countries_within[Fully Relevant]finance.currency.countries_used[Partially Relevant]royalty.monarch.kingdom[Partially Relevant]',\n",
       "  'processed_pred': '[Continue to Retrieve Evidence]<paragraph>base.locations.continents.countries_within;finance.currency.countries_used;royalty.monarch.kingdom;base.unitednations.united_nations_body_membership.member;base.unitednations.united_nations_body.members</paragraph>base.locations.continents.countries_within[Fully Relevant]finance.currency.countries_used[Partially Relevant]royalty.monarch.kingdom[Partially Relevant]'},\n",
       " 10: {'prompt': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nwhat countries are part of the uk<|eot_id|><|start_header_id|>assistant<|end_header_id|>[New Retrieval]<paragraph>base.locations.continents.countries_within;finance.currency.countries_used;royalty.monarch.kingdom;base.unitednations.united_nations_body_membership.member;base.unitednations.united_nations_body.members</paragraph>base.locations.continents.countries_within[Fully Relevant]finance.currency.countries_used[Partially Relevant]royalty.monarch.kingdom[Partially Relevant]United Kingdom[Fully Relevant]Europe[Partially Relevant][Continue to Retrieve Evidence]<paragraph>base.locations.continents.countries_within;finance.currency.countries_used;royalty.monarch.kingdom;base.unitednations.united_nations_body_membership.member;base.unitednations.united_nations_body.members</paragraph>base.locations.continents.countries_within[Fully Relevant]finance.currency.countries_used[Partially Relevant]royalty.monarch.kingdom[Partially Relevant]',\n",
       "  'pred': 'United Kingdom[Fully Relevant]Europe[Partially Relevant][Continue to Retrieve Evidence]',\n",
       "  'context': dict_keys([]),\n",
       "  'score': 1.0,\n",
       "  'parent': 8,\n",
       "  'topic_entity': ['m.07ssc', 'm.0hzjlmp'],\n",
       "  'processed_pred': 'United Kingdom[Fully Relevant]Europe[Partially Relevant]'},\n",
       " 11: {'prompt': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nwhat countries are part of the uk<|eot_id|><|start_header_id|>assistant<|end_header_id|>[New Retrieval]<paragraph>base.locations.continents.countries_within;finance.currency.countries_used;royalty.monarch.kingdom;base.unitednations.united_nations_body_membership.member;base.unitednations.united_nations_body.members</paragraph>base.locations.continents.countries_within[Fully Relevant]finance.currency.countries_used[Partially Relevant]royalty.monarch.kingdom[Partially Relevant]United Kingdom[Fully Relevant]Europe[Partially Relevant]United Kingdom[Fully Relevant]Europe[Partially Relevant]',\n",
       "  'pred': 'United Kingdom[Fully Relevant]Europe[Partially Relevant]Pound sterling[Unrelevant][Continue to Retrieve Evidence]',\n",
       "  'context': dict_keys([]),\n",
       "  'score': 1.0,\n",
       "  'parent': 8,\n",
       "  'topic_entity': ['m.07ssc', 'm.0hzjlmp'],\n",
       "  'processed_pred': 'United Kingdom[Fully Relevant]Europe[Partially Relevant]Pound sterling[Unrelevant]'},\n",
       " 12: {'prompt': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nwhat countries are part of the uk<|eot_id|><|start_header_id|>assistant<|end_header_id|>[New Retrieval]<paragraph>base.locations.continents.countries_within;finance.currency.countries_used;royalty.monarch.kingdom;base.unitednations.united_nations_body_membership.member;base.unitednations.united_nations_body.members</paragraph>base.locations.continents.countries_within[Fully Relevant]finance.currency.countries_used[Partially Relevant]royalty.monarch.kingdom[Partially Relevant]United Kingdom[Fully Relevant]Europe[Partially Relevant]United Kingdom[Fully Relevant]Europe[Partially Relevant]Pound sterling[Unrelevant]',\n",
       "  'pred': 'United Kingdom[Fully Relevant]Europe[Partially Relevant]Pound sterling[Unrelevant]Edward VIII of the United Kingdom[Unrelevant]Queen Victoria[Unrelevant]Edward VII[Unrelevant]Elizabeth II[Unrelevant][Continue to Retrieve Evidence]',\n",
       "  'context': dict_keys([]),\n",
       "  'score': 1.0,\n",
       "  'parent': 8,\n",
       "  'topic_entity': ['m.07ssc', 'm.0hzjlmp'],\n",
       "  'processed_pred': 'United Kingdom[Fully Relevant]Europe[Partially Relevant]Pound sterling[Unrelevant]Edward VIII of the United Kingdom[Unrelevant]Queen Victoria[Unrelevant]Edward VII[Unrelevant]Elizabeth II[Unrelevant]'},\n",
       " 13: {'prompt': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nwhat countries are part of the uk<|eot_id|><|start_header_id|>assistant<|end_header_id|>[New Retrieval]<paragraph>base.locations.continents.countries_within;finance.currency.countries_used;royalty.monarch.kingdom;base.unitednations.united_nations_body_membership.member;base.unitednations.united_nations_body.members</paragraph>base.locations.continents.countries_within[Fully Relevant]finance.currency.countries_used[Partially Relevant]royalty.monarch.kingdom[Partially Relevant]United Kingdom[Fully Relevant]Europe[Partially Relevant]United Kingdom[Fully Relevant]Europe[Partially Relevant]Pound sterling[Unrelevant]Edward VIII of the United Kingdom[Unrelevant]Queen Victoria[Unrelevant]Edward VII[Unrelevant]Elizabeth II[Unrelevant]',\n",
       "  'pred': 'base.locations.continents.countries_within[Fully Relevant]finance.currency.countries_used[Partially Relevant]royalty.monarch.kingdom[Partially Relevant][Retrieve Entity]<paragraph>(Europe, base.locations.continents.countries_within, Belgium);(Europe, base.locations.continents.countries_within, Germany);(Europe, base.locations.continents.countries_within, Slovakia);(Europe, base.locations.continents.countries_within, Republic of Kosovo);(Europe, base.locations.continents.countries_within, Iceland)</paragraph>Belgium',\n",
       "  'context': dict_keys([]),\n",
       "  'score': 1.0,\n",
       "  'parent': 8,\n",
       "  'topic_entity': ['m.07ssc', 'm.0hzjlmp'],\n",
       "  'processed_pred': 'base.locations.continents.countries_within[Fully Relevant]finance.currency.countries_used[Partially Relevant]royalty.monarch.kingdom[Partially Relevant][Retrieve Entity]<paragraph>(Europe, base.locations.continents.countries_within, Belgium);(Europe, base.locations.continents.countries_within, Germany);(Europe, base.locations.continents.countries_within, Slovakia);(Europe, base.locations.continents.countries_within, Republic of Kosovo);(Europe, base.locations.continents.countries_within, Iceland)</paragraph>Belgium'},\n",
       " 14: {'prompt': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nwhat countries are part of the uk<|eot_id|><|start_header_id|>assistant<|end_header_id|>[New Retrieval]<paragraph>base.locations.continents.countries_within;finance.currency.countries_used;royalty.monarch.kingdom;base.unitednations.united_nations_body_membership.member;base.unitednations.united_nations_body.members</paragraph>base.locations.continents.countries_within[Fully Relevant]finance.currency.countries_used[Partially Relevant]royalty.monarch.kingdom[Partially Relevant]United Kingdom[Fully Relevant]Europe[Partially Relevant]base.locations.continents.countries_within[Fully Relevant]finance.currency.countries_used[Partially Relevant]royalty.monarch.kingdom[Partially Relevant][Retrieve Entity]<paragraph>(Europe, base.locations.continents.countries_within, Belgium);(Europe, base.locations.continents.countries_within, Germany);(Europe, base.locations.continents.countries_within, Slovakia);(Europe, base.locations.continents.countries_within, Republic of Kosovo);(Europe, base.locations.continents.countries_within, Iceland)</paragraph>Belgium',\n",
       "  'pred': 'base.locations.continents.countries_within[Fully Relevant]finance.currency.countries_used[Partially Relevant]royalty.monarch.kingdom[Partially Relevant][Retrieve Entity]<paragraph>(Europe, base.locations.continents.countries_within, Belgium);(Europe, base.locations.continents.countries_within, Germany);(Europe, base.locations.continents.countries_within, Slovakia);(Europe, base.locations.continents.countries_within, Republic of Kosovo);(Europe, base.locations.continents.countries_within, Iceland)</paragraph>Belgium',\n",
       "  'context': dict_keys([]),\n",
       "  'score': 1.0,\n",
       "  'parent': 8,\n",
       "  'topic_entity': ['m.07ssc', 'm.0hzjlmp'],\n",
       "  'processed_pred': 'base.locations.continents.countries_within[Fully Relevant]finance.currency.countries_used[Partially Relevant]royalty.monarch.kingdom[Partially Relevant][Retrieve Entity]<paragraph>(Europe, base.locations.continents.countries_within, Belgium);(Europe, base.locations.continents.countries_within, Germany);(Europe, base.locations.continents.countries_within, Slovakia);(Europe, base.locations.continents.countries_within, Republic of Kosovo);(Europe, base.locations.continents.countries_within, Iceland)</paragraph>Belgium'},\n",
       " 15: {'prompt': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nwhat countries are part of the uk<|eot_id|><|start_header_id|>assistant<|end_header_id|>[New Retrieval]<paragraph>base.locations.continents.countries_within;finance.currency.countries_used;royalty.monarch.kingdom;base.unitednations.united_nations_body_membership.member;base.unitednations.united_nations_body.members</paragraph>base.locations.continents.countries_within[Fully Relevant]finance.currency.countries_used[Partially Relevant]royalty.monarch.kingdom[Partially Relevant]United Kingdom[Fully Relevant]Europe[Partially Relevant]base.locations.continents.countries_within[Fully Relevant]finance.currency.countries_used[Partially Relevant]royalty.monarch.kingdom[Partially Relevant][Retrieve Entity]<paragraph>(Europe, base.locations.continents.countries_within, Belgium);(Europe, base.locations.continents.countries_within, Germany);(Europe, base.locations.continents.countries_within, Slovakia);(Europe, base.locations.continents.countries_within, Republic of Kosovo);(Europe, base.locations.continents.countries_within, Iceland)</paragraph>Belgium',\n",
       "  'pred': 'base.locations.continents.countries_within[Fully Relevant]finance.currency.countries_used[Partially Relevant]royalty.monarch.kingdom[Partially Relevant][Retrieve Entity]<paragraph>(Europe, base.locations.continents.countries_within, Belgium);(Europe, base.locations.continents.countries_within, Germany);(Europe, base.locations.continents.countries_within, Slovakia);(Europe, base.locations.continents.countries_within, Republic of Kosovo);(Europe, base.locations.continents.countries_within, Iceland)</paragraph>Belgium',\n",
       "  'context': dict_keys([]),\n",
       "  'score': 1.0,\n",
       "  'parent': 8,\n",
       "  'topic_entity': ['m.07ssc', 'm.0hzjlmp'],\n",
       "  'processed_pred': 'base.locations.continents.countries_within[Fully Relevant]finance.currency.countries_used[Partially Relevant]royalty.monarch.kingdom[Partially Relevant][Retrieve Entity]<paragraph>(Europe, base.locations.continents.countries_within, Belgium);(Europe, base.locations.continents.countries_within, Germany);(Europe, base.locations.continents.countries_within, Slovakia);(Europe, base.locations.continents.countries_within, Republic of Kosovo);(Europe, base.locations.continents.countries_within, Iceland)</paragraph>Belgium'},\n",
       " 16: {'prompt': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nwhat countries are part of the uk<|eot_id|><|start_header_id|>assistant<|end_header_id|>Europe[Partially Relevant]Pound sterling[Unrelevant]Edward VIII of the United Kingdom[Unrelevant]Queen Victoria[Unrelevant]Edward VII[Unrelevant]Elizabeth II[Unrelevant]No triplets Received[Unrelevant][Continue to Retrieve Evidence]<paragraph>base.locations.continents.countries_within;finance.currency.countries_used;royalty.monarch.kingdom;base.unitednations.united_nations_body_membership.member;base.unitednations.united_nations_body.members</paragraph>base.locations.continents.countries_within[Fully Relevant]finance.currency.countries_used[Partially Relevant]royalty.monarch.kingdom[Partially Relevant]',\n",
       "  'pred': 'United Kingdom[Fully Relevant]Europe[Partially Relevant][Continue to Retrieve Evidence]',\n",
       "  'context': dict_keys([]),\n",
       "  'score': 1.0,\n",
       "  'parent': 9,\n",
       "  'topic_entity': ['m.07ssc', 'm.0hzjlmp'],\n",
       "  'processed_pred': 'United Kingdom[Fully Relevant]Europe[Partially Relevant]'},\n",
       " 17: {'prompt': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nwhat countries are part of the uk<|eot_id|><|start_header_id|>assistant<|end_header_id|>Europe[Partially Relevant]Pound sterling[Unrelevant]Edward VIII of the United Kingdom[Unrelevant]Queen Victoria[Unrelevant]Edward VII[Unrelevant]Elizabeth II[Unrelevant]No triplets Received[Unrelevant]United Kingdom[Fully Relevant]Europe[Partially Relevant]',\n",
       "  'pred': 'United Kingdom[Fully Relevant]Europe[Partially Relevant]Pound sterling[Unrelevant][Continue to Retrieve Evidence]',\n",
       "  'context': dict_keys([]),\n",
       "  'score': 1.0,\n",
       "  'parent': 9,\n",
       "  'topic_entity': ['m.07ssc', 'm.0hzjlmp'],\n",
       "  'processed_pred': 'United Kingdom[Fully Relevant]Europe[Partially Relevant]Pound sterling[Unrelevant]'},\n",
       " 18: {'prompt': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nwhat countries are part of the uk<|eot_id|><|start_header_id|>assistant<|end_header_id|>Europe[Partially Relevant]Pound sterling[Unrelevant]Edward VIII of the United Kingdom[Unrelevant]Queen Victoria[Unrelevant]Edward VII[Unrelevant]Elizabeth II[Unrelevant]No triplets Received[Unrelevant]United Kingdom[Fully Relevant]Europe[Partially Relevant]Pound sterling[Unrelevant]',\n",
       "  'pred': 'United Kingdom[Fully Relevant]Europe[Partially Relevant]Pound sterling[Unrelevant]Edward VIII of the United Kingdom[Unrelevant]Queen Victoria[Unrelevant]Edward VII[Unrelevant]Elizabeth II[Unrelevant][Continue to Retrieve Evidence]',\n",
       "  'context': dict_keys([]),\n",
       "  'score': 1.0,\n",
       "  'parent': 9,\n",
       "  'topic_entity': ['m.07ssc', 'm.0hzjlmp'],\n",
       "  'processed_pred': 'United Kingdom[Fully Relevant]Europe[Partially Relevant]Pound sterling[Unrelevant]Edward VIII of the United Kingdom[Unrelevant]Queen Victoria[Unrelevant]Edward VII[Unrelevant]Elizabeth II[Unrelevant]'},\n",
       " 19: {'prompt': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nwhat countries are part of the uk<|eot_id|><|start_header_id|>assistant<|end_header_id|>Europe[Partially Relevant]Pound sterling[Unrelevant]Edward VIII of the United Kingdom[Unrelevant]Queen Victoria[Unrelevant]Edward VII[Unrelevant]Elizabeth II[Unrelevant]No triplets Received[Unrelevant]United Kingdom[Fully Relevant]Europe[Partially Relevant]Pound sterling[Unrelevant]Edward VIII of the United Kingdom[Unrelevant]Queen Victoria[Unrelevant]Edward VII[Unrelevant]Elizabeth II[Unrelevant]',\n",
       "  'pred': 'No triplets Received[Unrelevant][Continue to Retrieve Evidence]',\n",
       "  'context': dict_keys([]),\n",
       "  'score': 5.368602725924285e-17,\n",
       "  'parent': 9,\n",
       "  'topic_entity': ['m.07ssc', 'm.0hzjlmp'],\n",
       "  'processed_pred': 'No triplets Received[Unrelevant]'},\n",
       " 20: {'prompt': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nwhat countries are part of the uk<|eot_id|><|start_header_id|>assistant<|end_header_id|>Europe[Partially Relevant]Pound sterling[Unrelevant]Edward VIII of the United Kingdom[Unrelevant]Queen Victoria[Unrelevant]Edward VII[Unrelevant]Elizabeth II[Unrelevant]No triplets Received[Unrelevant]No triplets Received[Unrelevant]',\n",
       "  'pred': 'No triplets Received[Unrelevant][Continue to Retrieve Evidence]',\n",
       "  'context': dict_keys([]),\n",
       "  'score': 1.0,\n",
       "  'parent': 9,\n",
       "  'topic_entity': ['m.07ssc', 'm.0hzjlmp'],\n",
       "  'processed_pred': 'No triplets Received[Unrelevant]'},\n",
       " 21: {'prompt': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nwhat countries are part of the uk<|eot_id|><|start_header_id|>assistant<|end_header_id|>Europe[Partially Relevant]Pound sterling[Unrelevant]Edward VIII of the United Kingdom[Unrelevant]Queen Victoria[Unrelevant]Edward VII[Unrelevant]Elizabeth II[Unrelevant]No triplets Received[Unrelevant]No triplets Received[Unrelevant]',\n",
       "  'pred': 'No triplets Received[Unrelevant][Continue to Retrieve Evidence]',\n",
       "  'context': dict_keys([]),\n",
       "  'score': 1.0,\n",
       "  'parent': 9,\n",
       "  'topic_entity': ['m.07ssc', 'm.0hzjlmp'],\n",
       "  'processed_pred': 'No triplets Received[Unrelevant]'}}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_eval_metric(best_pred, preds, answers):\n",
    "    correct, total = 0.0, 0.0\n",
    "    for entity in preds:\n",
    "        if entity in answers:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "    if len(answers) == 0:\n",
    "        if total == 0:\n",
    "            return 1.0, 1.0, 1.0, 1.0 # precision, recall, f1, hits\n",
    "        else:\n",
    "            return 0.0, 1.0, 0.0, 1.0 # precision, recall, f1, hits\n",
    "    else:\n",
    "        hits = float(best_pred in answers)\n",
    "        if total == 0:\n",
    "            return 1.0, 0.0, 0.0, hits # precision, recall, f1, hits\n",
    "        else:\n",
    "            precision, recall = correct / total, correct / len(answers)\n",
    "            f1 = 2.0 / (1.0 / precision + 1.0 / recall) if precision != 0 and recall != 0 else 0.0\n",
    "            return precision, recall, f1, hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 1\n",
    "answers = [get_label(ans) for ans in test_data[index]['answer']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['United States Representative',\n",
       " 'Governor of Tennessee',\n",
       " 'Speaker of the United States House of Representatives']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = ['Governor of Tennessee']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_eval_metric(preds, answers):\n",
    "    correct, total = 0.0, 0.0\n",
    "    for entity in preds:\n",
    "        if entity in answers:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "    # if len(answers) == 0:\n",
    "    #     if total == 0:\n",
    "    #         return 1.0, 1.0, 1.0, 1.0 # precision, recall, f1, hits\n",
    "    #     else:\n",
    "    #         return 0.0, 1.0, 0.0, 1.0 # precision, recall, f1, hits\n",
    "    # else:\n",
    "    if total != 0:\n",
    "        hits = 1\n",
    "    else: \n",
    "        hits = 0\n",
    "    if total == 0:\n",
    "        return 1.0, 0.0, 0.0, hits # precision, recall, f1, hits\n",
    "    else:\n",
    "        precision, recall = correct / total, correct / len(answers)\n",
    "        f1 = 2.0 / (1.0 / precision + 1.0 / recall) if precision != 0 and recall != 0 else 0.0\n",
    "        return precision, recall, f1, hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 0.3333333333333333, 0.5, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cal_eval_metric(preds, answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'FlagEmbedding'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mFlagEmbedding\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BGEM3FlagModel\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m BGEM3FlagModel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBAAI/bge-m3\u001b[39m\u001b[38;5;124m'\u001b[39m,  use_fp16\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;66;03m# Setting use_fp16 to True speeds up computation with a slight performance degradation\u001b[39;00m\n\u001b[1;32m      5\u001b[0m sentences_1 \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is BGE M3?\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDefination of BM25\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'FlagEmbedding'"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "self-rag",
   "language": "python",
   "name": "self-rag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
