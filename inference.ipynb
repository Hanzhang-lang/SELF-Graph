{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"2b219db0d2984f9dae28b651ab8ab3d9\"\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://smsh.openai.azure.com/\"\n",
    "os.environ[\"AZURE_OPENAI_API_VERSION\"] = \"2024-03-01-preview\"\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",\n",
    ")\n",
    "\n",
    "from langchain.storage import LocalFileStore, RedisStore\n",
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "store = RedisStore(redis_url=\"redis://localhost:6379\")\n",
    "cached_embedder = CacheBackedEmbeddings.from_bytes_store(\n",
    "embeddings, store, namespace=\"openai\"\n",
    ")\n",
    "row_string = []\n",
    "with open('./data/clean_relations', 'r') as f:\n",
    "    data = [line.strip() for line in f]\n",
    "db = FAISS.from_texts(data, cached_embedder)\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create with GPT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-09 08:25:14 multiproc_worker_utils.py:133] Terminating local vLLM worker processes\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "import os\n",
    "\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"2b219db0d2984f9dae28b651ab8ab3d9\"\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://smsh.openai.azure.com/\"\n",
    "os.environ[\"AZURE_OPENAI_API_VERSION\"] = \"2024-02-01\"\n",
    "os.environ[\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"] = \"gpt-35-turbo\"\n",
    "\n",
    "model = AzureChatOpenAI(\n",
    "    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    "    azure_deployment=os.environ[\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"],\n",
    "    temperature=1,\n",
    "    n = 3,\n",
    "    max_retries=5, request_timeout=600\n",
    ")\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_701401/3707533165.py:28: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  llm_chain = LLMChain(llm=model, prompt=few_shot_intepretable_prompt, verbose=True)\n"
     ]
    }
   ],
   "source": [
    "# The name of Justin Bieber's brother is Jaxon Bieber. This is based on the reasoning path that connects Justin Bieber to Jaxon Bieber through the relationship of sibling. The other paths that connect Justin Bieber to Jazmyn Bieber or back to Justin Bieber himself are incorrect in this context.\n",
    "few_shot_intepretable_prompt = FewShotPromptTemplate(\n",
    "        examples=[{\n",
    "            \"query\": \"Who was the prime minister of Japan in 2011, that served in the New Party Sakigake?\",\n",
    "            \"evidence\": \"Relations retrieved: language.human_language.countries_spoken_in\\n Entities retrieved: Japan\",\n",
    "            \"preceding_sentences\": \"\",\n",
    "            \"output\": \"Naoto Kan\",\n",
    "            \"rating\": \"[Continue to Retrieve Evidence]\"\n",
    "        }],\n",
    "        example_prompt=PromptTemplate.from_template(\"\"\"Query: {query}\n",
    "Preceding sentences: {preceding_sentences}\n",
    "Evidence: {evidence}\n",
    "Output: {target_output}\n",
    "Rating: {rating}\"\"\"),\n",
    "        prefix=\n",
    "        \"You will be provided with a query, evidence, output sentence, and preceding sentences (optional). Your task is to determine whether the information in the output sentence can be fully verified by the evidence or if it requires further external verification. There are three cases:\\n\" \n",
    "        \"- If the output sentence can be verified solely with the evidence and the preceding sentences, then respond with [No Retrieval]. \\n\"\n",
    "        \"- If additional information about the tail entity in evidence is needed to verify the output sentence, respond with [Continue to Retrieve Evidence] \\n\"\n",
    "        \"- If more information unrelated to the evidence is needed, e.g. totally new relationship or new entity, reponse with [New Retrieval].\\n\\n\",\n",
    "        suffix=\n",
    "        \"Query: {query}\\n\"\n",
    "        \"Preceding sentences: {preceding_sentences}\\n\"\n",
    "        \"Evidence: {evidence}\\n\"\n",
    "        \"Output: {target_output}\",\n",
    "        input_variables=[\"query\", \"evidence\", \"preceding_sentences\", \"topic\"],\n",
    ")\n",
    "\n",
    "llm_chain = LLMChain(llm=model, prompt=few_shot_intepretable_prompt, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relationship relevance\n",
    "graph_intepretable =  \"\"\"You will receive a query, evidence and optional preceding sentences containing history information. The evidence contains graph relationships or entities may be useful to answer the query. Your task is to filters out valid information from the evidence to answer the given query, evaluate your output and provide explanations on your result.\n",
    "\n",
    "###\n",
    "Query: Name the president of the country whose main spoken language was Brahui in 1980?\n",
    "Topic Entity: Brahui Language\n",
    "Evidence: language.human_language.main_country; language.human_language.language_family; language.human_language.iso_639_3_code; base.rosetta.languoid.parent; language.human_language.writing_system; base.rosetta.languoid.languoid_class; language.human_language.countries_spoken_in; kg.object_profile.prominent_type; base.rosetta.languoid.document; base.ontologies.ontology_instance.equivalent_instances; base.rosetta.languoid.local_name; language.human_language.region\n",
    "Preceding sentences: \n",
    "Output: \n",
    "1. {{language.human_language.main_country (Score: Fully relavant))}}: This relation is highly relevant as it directly relates to the country whose president is being asked for, and the main country where Brahui language is spoken in 1980.\n",
    "2. {{language.human_language.countries_spoken_in (Score: Fully relavant)}}: This relation is also relevant as it provides information on the countries where Brahui language is spoken, which could help narrow down the search for the president.\n",
    "3. {{base.rosetta.languoid.parent (Score: Partially relevant)}}: This relation is less relevant but still provides some context on the language family to which Brahui belongs, which could be useful in understanding the linguistic and cultural background of the country in question.\n",
    "\n",
    "###\n",
    "Query: {query}\n",
    "Topic Entity: {topic}\n",
    "Evidence: {evidence}\n",
    "Preceding sentences: {preceding_sentences}\n",
    "Output: \n",
    "\"\"\"\n",
    "# The name of Justin Bieber's brother is Jaxon Bieber. This is based on the reasoning path that connects Justin Bieber to Jaxon Bieber through the relationship of sibling. The other paths that connect Justin Bieber to Jazmyn Bieber or back to Justin Bieber himself are incorrect in this context.\n",
    "few_shot_intepretable_prompt = FewShotPromptTemplate(\n",
    "        examples=[{\n",
    "            \"query\": \"Name the president of the country whose main spoken language was Brahui in 1980?\",\n",
    "            \"topic\": \"Brahui Language\",\n",
    "            \"evidence\": \"language.human_language.main_country; language.human_language.language_family; language.human_language.iso_639_3_code; base.rosetta.languoid.parent; language.human_language.writing_system; base.rosetta.languoid.languoid_class; language.human_language.countries_spoken_in; kg.object_profile.prominent_type; base.rosetta.languoid.document; base.ontologies.ontology_instance.equivalent_instances; base.rosetta.languoid.local_name; language.human_language.region\",\n",
    "            \"preceding_sentences\": \"\",\n",
    "            \"output\": \"\"\"1. {{language.human_language.main_country (Score: [Fully Relavant])}}: This relation is highly relevant as it directly relates to the country whose president is being asked for, and the main country where Brahui language is spoken in 1980.\n",
    "2. {{language.human_language.countries_spoken_in (Score: [Fully Relavant])}}: This relation is also relevant as it provides information on the countries where Brahui language is spoken, which could help narrow down the search for the president.\n",
    "3. {{base.rosetta.languoid.parent (Score: [Partially Relevant])}}: This relation is less relevant but still provides some context on the language family to which Brahui belongs, which could be useful in understanding the linguistic and cultural background of the country in question.\"\"\"\n",
    "        }],\n",
    "        example_prompt=PromptTemplate.from_template(\"\"\"###\n",
    "Query: {query}\n",
    "Topic Entity: {topic}\n",
    "Evidence: {evidence}\n",
    "Preceding sentences: {preceding_sentences}\n",
    "Output: {output}\n",
    "\"\"\"),\n",
    "        prefix=\n",
    "        \"\"\"You will receive a query, topic entity, evidence and optional preceding sentences containing history information. The evidence contains graph relationships or entities may be useful to answer the query. Your task is to filters out 3 valid information from the evidence that contribute to answering the query and provide a relevance score for each output, output your explanations for the score.\n",
    "The score of relevance range from [Fully Relavant], [Partially Relevant] to [Unrelevant].\"\"\",\n",
    "        suffix=\n",
    "        \"\"\"###\n",
    "Query: {query}\n",
    "Topic Entity: {topic}\n",
    "Evidence: {evidence}\n",
    "Preceding sentences: {preceding_sentences}\n",
    "Output: \"\"\",\n",
    "        input_variables=[\"query\", \"evidence\", \"preceding_sentences\", \"topic\"],\n",
    ")\n",
    "graph_intepretable_prompt = PromptTemplate(input_variables=[\"query\", \"evidence\", \"preceding_sentences\", \"topic\"], template=\n",
    "graph_intepretable)\n",
    "llm_chain = LLMChain(llm=model, prompt=few_shot_intepretable_prompt, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "few_shot_intepretable_prompt = FewShotPromptTemplate(\n",
    "        examples=[{\n",
    "            \"query\": \"what is the name of justin bieber brother?\",\n",
    "            \"output\": \"[Retreive New Relationship]<paragraph>people.sibling_relationship.sibling;fictional_universe.fictional_character.siblings;fictional_universe.sibling_relationship_of_fictional_characters.siblings;people.person.sibling_s;people.family.members;people.person.parents</paragraph>Retrieved relationship: people.person.parents[Fully Relevant][Retrieve Entity]<paragraph>(Justin Bieber, people.person.parents, Pattie Mallette);(Justin Bieber, people.person.parents, Jeremy Bieber);(Justin Bieber, people.person.parents, Jeremy Bieber)</paragraph>Retrieved triplet: (Justin Bieber, people.person.parents, Jeremy Bieber)[Fully Relevant][Continue to Retrieve Evidence]<paragraph>people.sibling_relationship.sibling;people.person.sibling_s;people.person.parents;fictional_universe.fictional_character.siblings;people.family.members;people.person.children</paragraph>Retrieved relationship: people.person.children[Fully Relevant][Retrieve Entity]<paragraph>(Jeremy Bieber, people.person.children, Jazmyn Bieber);(Jeremy Bieber, people.person.children, Justin Bieber);(Jeremy Bieber, people.person.children, Jaxon Bieber);(Jeremy Bieber, people.person.children, Jaxon Bieber)</paragraph>Retrieved triplet: (Jeremy Bieber, people.person.children, Jaxon Bieber)[Fully Relevant][No Retrieval] Answer: Jaxon Bieber\",\n",
    "            \"explanation\": \"The output provides the name of justin bieber's brother. This is based on the reasoning path that connects Justin Bieber to Jaxon Bieber through the relationship of sibling. The other paths that connect Justin Bieber to Jazmyn Bieber or back to Justin Bieber himself are incorrect in this context.\",\n",
    "            \"rating\": \"[Confidence:5]\"\n",
    "        }],\n",
    "        example_prompt=PromptTemplate.from_template(\"\"\"\n",
    "Query: {query}\\n\n",
    "Output: {output}\n",
    "Explanation: {explanation}\n",
    "Rating: {rating}\n",
    "\"\"\"),\n",
    "        prefix=\"\"\"Given a query and an output, rate whether the response and the thought process appears to be a helpful and informative answer to the query, from 1 (lowest) - 5 (highest). We call this confidence score.\n",
    "[Confidence:5]: The response provides a complete and correct reasoning chain to the query, and the final answer is complete and logically correct.\n",
    "[Confidence:4]: The response mostly fulfills the need in the query and provides correct answers, while there can be some minor improvements such as shorter reasoning chain or less repetition.\n",
    "[Confidence:3]: The response is acceptable, but the answers may be not complete or needs minor improvement.\n",
    "[Confidence:2]: The reasoning process still addresses the main request, but the answers are not correct or not relevant to the query.\n",
    "[Confidence:1]: The reasoning is barely irrelevant or does not give an answer in the end.\"\"\",\n",
    "        suffix=\n",
    "        \"\"\"\n",
    "Query: {query}\\n\n",
    "Output: {output}\\n\"\"\",\n",
    "        input_variables=[\"query\", \"output\"],\n",
    ")\n",
    "# confidence_prompt = PromptTemplate(input_variables=[\"query\", \"output\"], template=\n",
    "# graph_intepretable)\n",
    "llm_chain = LLMChain(llm=model, prompt=few_shot_intepretable_prompt, verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run_long_form answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/disk1/chatgpt/miniconda3/envs/self-rag/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 12-11 10:43:23 cuda.py:22] You are using a deprecated `pynvml` package. Please install `nvidia-ml-py` instead, and make sure to uninstall `pynvml`. When both of them are installed, `pynvml` will take precedence and cause errors. See https://pypi.org/project/pynvml for more information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-11 10:43:24,689\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-11 10:43:28 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='/media/disk2/llama_factory/generation_1209_no_mask', speculative_config=None, tokenizer='/media/disk2/llama_factory/generation_1209_no_mask', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/media/disk2/llama_factory/generation_1209_no_mask, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "INFO 12-11 10:43:29 model_runner.py:1056] Starting to load model /media/disk2/llama_factory/generation_1209_no_mask...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:00,  6.56it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:00<00:01,  1.99it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  1.54it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.39it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.56it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-11 10:43:32 model_runner.py:1067] Loading model weights took 14.9605 GB\n",
      "INFO 12-11 10:43:33 gpu_executor.py:122] # GPU blocks: 9656, # CPU blocks: 2048\n",
      "INFO 12-11 10:43:33 gpu_executor.py:126] Maximum concurrency for 8192 tokens per request: 18.86x\n",
      "INFO 12-11 10:43:35 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 12-11 10:43:35 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 12-11 10:43:46 model_runner.py:1523] Graph capturing finished in 12 secs.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "# model = LLM(model='/media/disk2/llama_factory/generation_1124_special', trust_remote_code=True, tensor_parallel_size=4)\n",
    "model = LLM(model='/media/disk2/llama_factory/generation_1209_no_mask', trust_remote_code=True, tensor_parallel_size=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.51s/it, est. speed input: 11.30 toks/s, output: 66.47 toks/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletionOutput(index=0, text='business[Partially Relevant]organization.organization_founder.organizations_founded;organization.organization.founders;organization.organization_membership.organization;organization.leadership.organization;business.sponsorship.sponsored_by</paragraph>organization.organization_membership.organization[Fully Relevant]organization.organization_founder.organizations_founded[Partially Relevant]organization.leadership.organization[Partially Relevant][Retrieve Entity]<paragraph>(Google, organization.organization.founders, Sergey Brin);(Google, organization.organization.founders, Larry Page)</paragraph>Sergey Brin[Partially Relevant]Larry Page[Partially Relevant][Continue to Retrieve Evidence]<paragraph>people.person.profession;', token_ids=(27243, 128262, 24844, 70324, 22200, 261, 51272, 8200, 766, 13382, 26, 24844, 70324, 840, 801, 388, 26, 24844, 70324, 85735, 70324, 26, 24844, 31602, 6527, 2200, 70324, 26, 27243, 516, 31341, 5383, 516, 35841, 3795, 128264, 24844, 70324, 85735, 70324, 128261, 24844, 70324, 22200, 261, 51272, 8200, 766, 13382, 128262, 24844, 31602, 6527, 2200, 70324, 128262, 128259, 128263, 6838, 2738, 11, 7471, 70324, 840, 801, 388, 11, 74529, 3320, 258, 1237, 7, 14783, 11, 7471, 70324, 840, 801, 388, 11, 30390, 5874, 8, 128264, 50, 10286, 88, 3320, 258, 128262, 89595, 5874, 128262, 128258, 128263, 16455, 31970, 93605, 1362, 26), cumulative_logprob=-0.6931509971545893, logprobs=[{27243: Logprob(logprob=0.0, rank=1, decoded_token='business'), 128259: Logprob(logprob=-43.707275390625, rank=2, decoded_token='[Retrieve Entity]'), 2588: Logprob(logprob=-156.0975341796875, rank=3, decoded_token='location'), 11789: Logprob(logprob=-174.8292236328125, rank=4, decoded_token='language'), 128256: Logprob(logprob=-212.2926025390625, rank=5, decoded_token='[No Retrieval]')}, {128262: Logprob(logprob=0.0, rank=1, decoded_token='[Partially Relevant]'), 128264: Logprob(logprob=-93.658447265625, rank=2, decoded_token='</paragraph>'), 128260: Logprob(logprob=-106.1463623046875, rank=3, decoded_token='[Unrelevant]'), 26: Logprob(logprob=-124.8780517578125, rank=4, decoded_token=';'), 288: Logprob(logprob=-199.8048095703125, rank=5, decoded_token='es')}, {24844: Logprob(logprob=0.0, rank=1, decoded_token='organization'), 20510: Logprob(logprob=-249.756103515625, rank=2, decoded_token='law'), 2588: Logprob(logprob=-256.0, rank=3, decoded_token='location'), 128259: Logprob(logprob=-280.9755859375, rank=4, decoded_token='[Retrieve Entity]'), 86843: Logprob(logprob=-312.195068359375, rank=5, decoded_token='internet')}, {70324: Logprob(logprob=0.0, rank=1, decoded_token='.organization'), 128262: Logprob(logprob=-87.4146728515625, rank=2, decoded_token='[Partially Relevant]'), 31602: Logprob(logprob=-156.09765625, rank=3, decoded_token='.le'), 128260: Logprob(logprob=-206.048828125, rank=4, decoded_token='[Unrelevant]'), 128261: Logprob(logprob=-243.51220703125, rank=5, decoded_token='[Fully Relevant]')}, {22200: Logprob(logprob=0.0, rank=1, decoded_token='_found'), 840: Logprob(logprob=-237.268310546875, rank=2, decoded_token='.f'), 49065: Logprob(logprob=-299.707275390625, rank=3, decoded_token='_spin'), 85735: Logprob(logprob=-324.682861328125, rank=4, decoded_token='_membership'), 31602: Logprob(logprob=-362.1463623046875, rank=5, decoded_token='.le')}, {261: Logprob(logprob=0.0, rank=1, decoded_token='er'), 388: Logprob(logprob=-1286.243896484375, rank=2, decoded_token='ers'), 287: Logprob(logprob=-1411.1219482421875, rank=3, decoded_token='ing'), 1697: Logprob(logprob=-1723.3170166015625, rank=4, decoded_token='ational'), 811: Logprob(logprob=-1810.731689453125, rank=5, decoded_token='ations')}, {51272: Logprob(logprob=0.0, rank=1, decoded_token='.organ'), 13: Logprob(logprob=-836.6829833984375, rank=2, decoded_token='.'), 70324: Logprob(logprob=-955.317138671875, rank=3, decoded_token='.organization'), 840: Logprob(logprob=-1030.243896484375, rank=4, decoded_token='.f'), 2726: Logprob(logprob=-1048.9757080078125, rank=5, decoded_token='.org')}, {8200: Logprob(logprob=0.0, rank=1, decoded_token='izations'), 56276: Logprob(logprob=-1298.7318115234375, rank=2, decoded_token='isations'), 12509: Logprob(logprob=-1348.6829833984375, rank=3, decoded_token='izers'), 3213: Logprob(logprob=-1386.146484375, rank=4, decoded_token='izer'), 77491: Logprob(logprob=-1398.63427734375, rank=5, decoded_token='izational')}, {766: Logprob(logprob=0.0, rank=1, decoded_token='_f'), 840: Logprob(logprob=-1111.41455078125, rank=2, decoded_token='.f'), 78811: Logprob(logprob=-1136.39013671875, rank=3, decoded_token=' Founded'), 1426: Logprob(logprob=-1186.3414306640625, rank=4, decoded_token='_F'), 8074: Logprob(logprob=-1236.2926025390625, rank=5, decoded_token='_form')}, {13382: Logprob(logprob=0.0, rank=1, decoded_token='ounded'), 13900: Logprob(logprob=-1348.682861328125, rank=2, decoded_token='ounding'), 3171: Logprob(logprob=-1635.90234375, rank=3, decoded_token='ounds'), 37153: Logprob(logprob=-1704.5853271484375, rank=4, decoded_token='unded'), 801: Logprob(logprob=-1717.0731201171875, rank=5, decoded_token='ound')}, {26: Logprob(logprob=0.0, rank=1, decoded_token=';'), 82960: Logprob(logprob=-649.3658447265625, rank=2, decoded_token=';base'), 128264: Logprob(logprob=-699.3170166015625, rank=3, decoded_token='</paragraph>'), 40514: Logprob(logprob=-936.5853271484375, rank=4, decoded_token=';s'), 128262: Logprob(logprob=-1061.46337890625, rank=5, decoded_token='[Partially Relevant]')}, {24844: Logprob(logprob=0.0, rank=1, decoded_token='organization'), 27243: Logprob(logprob=-399.609619140625, rank=2, decoded_token='business'), 20510: Logprob(logprob=-536.9755859375, rank=3, decoded_token='law'), 86843: Logprob(logprob=-736.7803955078125, rank=4, decoded_token='internet'), 2239: Logprob(logprob=-774.2437744140625, rank=5, decoded_token='book')}, {70324: Logprob(logprob=0.0, rank=1, decoded_token='.organization'), 31602: Logprob(logprob=-624.390380859375, rank=2, decoded_token='.le'), 24844: Logprob(logprob=-1136.3902587890625, rank=3, decoded_token='organization'), 51272: Logprob(logprob=-1173.853759765625, rank=4, decoded_token='.organ'), 83452: Logprob(logprob=-1242.53662109375, rank=5, decoded_token='_organization')}, {840: Logprob(logprob=0.0, rank=1, decoded_token='.f'), 85735: Logprob(logprob=-449.5609130859375, rank=2, decoded_token='_membership'), 10108: Logprob(logprob=-524.48779296875, rank=3, decoded_token='.date'), 49065: Logprob(logprob=-574.43896484375, rank=4, decoded_token='_spin'), 22200: Logprob(logprob=-768.0, rank=5, decoded_token='_found')}, {801: Logprob(logprob=0.0, rank=1, decoded_token='ound'), 13900: Logprob(logprob=-1386.146240234375, rank=2, decoded_token='ounding'), 13382: Logprob(logprob=-1436.0975341796875, rank=3, decoded_token='ounded'), 13891: Logprob(logprob=-1560.9755859375, rank=4, decoded_token='OUND'), 21846: Logprob(logprob=-1748.2926025390625, rank=5, decoded_token='actors')}, {388: Logprob(logprob=0.0, rank=1, decoded_token='ers'), 4419: Logprob(logprob=-1498.5364990234375, rank=2, decoded_token='ERS'), 56090: Logprob(logprob=-1760.7803955078125, rank=3, decoded_token='ersh'), 5544: Logprob(logprob=-1854.43896484375, rank=4, decoded_token='rs'), 4257: Logprob(logprob=-1916.8780517578125, rank=5, decoded_token='_date')}, {26: Logprob(logprob=0.0, rank=1, decoded_token=';'), 82960: Logprob(logprob=-611.90234375, rank=2, decoded_token=';base'), 40514: Logprob(logprob=-974.0487060546875, rank=3, decoded_token=';s'), 128264: Logprob(logprob=-1261.2681884765625, rank=4, decoded_token='</paragraph>'), 79732: Logprob(logprob=-1329.951171875, rank=5, decoded_token=';c')}, {24844: Logprob(logprob=0.0, rank=1, decoded_token='organization'), 27243: Logprob(logprob=-337.1708984375, rank=2, decoded_token='business'), 20510: Logprob(logprob=-349.65869140625, rank=3, decoded_token='law'), 86843: Logprob(logprob=-749.2684326171875, rank=4, decoded_token='internet'), 37838: Logprob(logprob=-774.2440185546875, rank=5, decoded_token='education')}, {70324: Logprob(logprob=0.0, rank=1, decoded_token='.organization'), 31602: Logprob(logprob=-449.56103515625, rank=2, decoded_token='.le'), 24844: Logprob(logprob=-1136.3902587890625, rank=3, decoded_token='organization'), 83452: Logprob(logprob=-1223.8048095703125, rank=4, decoded_token='_organization'), 50983: Logprob(logprob=-1323.707275390625, rank=5, decoded_token='.non')}, {85735: Logprob(logprob=0.0, rank=1, decoded_token='_membership'), 10108: Logprob(logprob=-174.8291015625, rank=2, decoded_token='.date'), 49065: Logprob(logprob=-299.7071533203125, rank=3, decoded_token='_spin'), 25463: Logprob(logprob=-412.097412109375, rank=4, decoded_token='.place'), 26270: Logprob(logprob=-474.5364990234375, rank=5, decoded_token='.board')}, {70324: Logprob(logprob=0.0, rank=1, decoded_token='.organization'), 51272: Logprob(logprob=-1061.4635009765625, rank=2, decoded_token='.organ'), 27097: Logprob(logprob=-1086.4390869140625, rank=3, decoded_token='.role'), 24844: Logprob(logprob=-1111.414794921875, rank=4, decoded_token='organization'), 45939: Logprob(logprob=-1130.146484375, rank=5, decoded_token='.members')}, {26: Logprob(logprob=0.0, rank=1, decoded_token=';'), 82960: Logprob(logprob=-574.43896484375, rank=2, decoded_token=';base'), 40514: Logprob(logprob=-699.317138671875, rank=3, decoded_token=';s'), 128264: Logprob(logprob=-1173.8536376953125, rank=4, decoded_token='</paragraph>'), 56033: Logprob(logprob=-1236.2926025390625, rank=5, decoded_token=';b')}, {24844: Logprob(logprob=0.0, rank=1, decoded_token='organization'), 27243: Logprob(logprob=-199.8046875, rank=2, decoded_token='business'), 20510: Logprob(logprob=-586.9267578125, rank=3, decoded_token='law'), 86843: Logprob(logprob=-649.36572265625, rank=4, decoded_token='internet'), 37838: Logprob(logprob=-811.7071533203125, rank=5, decoded_token='education')}, {31602: Logprob(logprob=0.0, rank=1, decoded_token='.le'), 70324: Logprob(logprob=-62.43896484375, rank=2, decoded_token='.organization'), 24844: Logprob(logprob=-855.41455078125, rank=3, decoded_token='organization'), 12013: Logprob(logprob=-899.121826171875, rank=4, decoded_token='_le'), 51272: Logprob(logprob=-1017.7559814453125, rank=5, decoded_token='.organ')}, {6527: Logprob(logprob=0.0, rank=1, decoded_token='aders'), 1013: Logprob(logprob=-1748.292724609375, rank=2, decoded_token='ader'), 13572: Logprob(logprob=-2659.902587890625, rank=3, decoded_token='iders'), 10798: Logprob(logprob=-2709.853759765625, rank=4, decoded_token='ADER'), 2277: Logprob(logprob=-2747.3173828125, rank=5, decoded_token='ading')}, {2200: Logprob(logprob=0.0, rank=1, decoded_token='hip'), 69154: Logprob(logprob=-1523.51220703125, rank=2, decoded_token='HIP'), 6151: Logprob(logprob=-1698.341552734375, rank=3, decoded_token='hi'), 5383: Logprob(logprob=-1898.146484375, rank=4, decoded_token='ship'), 34322: Logprob(logprob=-2129.1708984375, rank=5, decoded_token='hips')}, {70324: Logprob(logprob=0.0, rank=1, decoded_token='.organization'), 24844: Logprob(logprob=-1098.9267578125, rank=2, decoded_token='organization'), 83452: Logprob(logprob=-1111.41455078125, rank=3, decoded_token='_organization'), 2726: Logprob(logprob=-1123.90234375, rank=4, decoded_token='.org'), 7471: Logprob(logprob=-1248.7803955078125, rank=5, decoded_token=' organization')}, {26: Logprob(logprob=0.0, rank=1, decoded_token=';'), 82960: Logprob(logprob=-399.60986328125, rank=2, decoded_token=';base'), 40514: Logprob(logprob=-599.41455078125, rank=3, decoded_token=';s'), 128264: Logprob(logprob=-1061.46337890625, rank=4, decoded_token='</paragraph>'), 56033: Logprob(logprob=-1073.951171875, rank=5, decoded_token=';b')}, {27243: Logprob(logprob=0.0, rank=1, decoded_token='business'), 24844: Logprob(logprob=-112.390380859375, rank=2, decoded_token='organization'), 86843: Logprob(logprob=-661.853759765625, rank=3, decoded_token='internet'), 20510: Logprob(logprob=-699.317138671875, rank=4, decoded_token='law'), 37838: Logprob(logprob=-774.2440185546875, rank=5, decoded_token='education')}, {516: Logprob(logprob=0.0, rank=1, decoded_token='.s'), 28020: Logprob(logprob=-212.2926025390625, rank=2, decoded_token='.company'), 83118: Logprob(logprob=-249.7559814453125, rank=3, decoded_token='.shopping'), 26270: Logprob(logprob=-387.121826171875, rank=4, decoded_token='.board'), 13: Logprob(logprob=-387.121826171875, rank=5, decoded_token='.')}, {31341: Logprob(logprob=0.0, rank=1, decoded_token='ponsor'), 35841: Logprob(logprob=-536.9755859375, rank=2, decoded_token='ponsored'), 2805: Logprob(logprob=-924.097412109375, rank=3, decoded_token='pons'), 51976: Logprob(logprob=-1011.5120849609375, rank=4, decoded_token='ponsors'), 621: Logprob(logprob=-1148.8779296875, rank=5, decoded_token='pon')}, {5383: Logprob(logprob=0.0, rank=1, decoded_token='ship'), 18143: Logprob(logprob=-1198.829345703125, rank=2, decoded_token='ships'), 2200: Logprob(logprob=-1311.2196044921875, rank=3, decoded_token='hip'), 71320: Logprob(logprob=-1461.0732421875, rank=4, decoded_token='SHIP'), 70131: Logprob(logprob=-1673.365966796875, rank=5, decoded_token='_relationship')}, {516: Logprob(logprob=0.0, rank=1, decoded_token='.s'), 11448: Logprob(logprob=-1092.6829833984375, rank=2, decoded_token='.rec'), 6064: Logprob(logprob=-1092.6829833984375, rank=3, decoded_token='.aw'), 646: Logprob(logprob=-1130.1463623046875, rank=4, decoded_token='_s'), 70324: Logprob(logprob=-1136.3902587890625, rank=5, decoded_token='.organization')}, {35841: Logprob(logprob=0.0, rank=1, decoded_token='ponsored'), 31341: Logprob(logprob=-1148.878173828125, rank=2, decoded_token='ponsor'), 74847: Logprob(logprob=-1248.780517578125, rank=3, decoded_token=' Sponsored'), 30638: Logprob(logprob=-1311.219482421875, rank=4, decoded_token=' sponsored'), 1042: Logprob(logprob=-1585.9512939453125, rank=5, decoded_token='ponse')}, {3795: Logprob(logprob=0.0, rank=1, decoded_token='_by'), 14656: Logprob(logprob=-1286.243896484375, rank=2, decoded_token='-by'), 1729: Logprob(logprob=-1286.243896484375, rank=3, decoded_token='by'), 7225: Logprob(logprob=-1311.219482421875, rank=4, decoded_token='_rec'), 1383: Logprob(logprob=-1461.0731201171875, rank=5, decoded_token='By')}, {128264: Logprob(logprob=0.0, rank=1, decoded_token='</paragraph>'), 26: Logprob(logprob=-399.609619140625, rank=2, decoded_token=';'), 83452: Logprob(logprob=-1036.4876708984375, rank=3, decoded_token='_organization'), 70324: Logprob(logprob=-1036.4876708984375, rank=4, decoded_token='.organization'), 128262: Logprob(logprob=-1161.36572265625, rank=5, decoded_token='[Partially Relevant]')}, {24844: Logprob(logprob=0.0, rank=1, decoded_token='organization'), 27243: Logprob(logprob=-299.707275390625, rank=2, decoded_token='business'), 128259: Logprob(logprob=-611.90234375, rank=3, decoded_token='[Retrieve Entity]'), 2588: Logprob(logprob=-936.5853271484375, rank=4, decoded_token='location'), 14783: Logprob(logprob=-992.7803955078125, rank=5, decoded_token='Google')}, {70324: Logprob(logprob=0.0, rank=1, decoded_token='.organization'), 31602: Logprob(logprob=-249.756103515625, rank=2, decoded_token='.le'), 24844: Logprob(logprob=-899.1219482421875, rank=3, decoded_token='organization'), 51272: Logprob(logprob=-911.60986328125, rank=4, decoded_token='.organ'), 83452: Logprob(logprob=-967.804931640625, rank=5, decoded_token='_organization')}, {85735: Logprob(logprob=-3.814689989667386e-06, rank=1, decoded_token='_membership'), 22200: Logprob(logprob=-12.487796783447266, rank=2, decoded_token='_found'), 45939: Logprob(logprob=-337.170654296875, rank=3, decoded_token='.members'), 840: Logprob(logprob=-349.658447265625, rank=4, decoded_token='.f'), 31602: Logprob(logprob=-374.634033203125, rank=5, decoded_token='.le')}, {70324: Logprob(logprob=0.0, rank=1, decoded_token='.organization'), 7471: Logprob(logprob=-1186.341552734375, rank=2, decoded_token=' organization'), 24844: Logprob(logprob=-1223.804931640625, rank=3, decoded_token='organization'), 83452: Logprob(logprob=-1261.2684326171875, rank=4, decoded_token='_organization'), 51272: Logprob(logprob=-1398.63427734375, rank=5, decoded_token='.organ')}, {128261: Logprob(logprob=0.0, rank=1, decoded_token='[Fully Relevant]'), 128262: Logprob(logprob=-49.951171875, rank=2, decoded_token='[Partially Relevant]'), 128260: Logprob(logprob=-624.39013671875, rank=3, decoded_token='[Unrelevant]'), 58: Logprob(logprob=-1186.3414306640625, rank=4, decoded_token='['), 11: Logprob(logprob=-1311.219482421875, rank=5, decoded_token=',')}, {24844: Logprob(logprob=0.0, rank=1, decoded_token='organization'), 27243: Logprob(logprob=-262.243896484375, rank=2, decoded_token='business'), 8629: Logprob(logprob=-1017.756103515625, rank=3, decoded_token='organ'), 128259: Logprob(logprob=-1036.48779296875, rank=4, decoded_token='[Retrieve Entity]'), 1813: Logprob(logprob=-1042.731689453125, rank=5, decoded_token='org')}, {70324: Logprob(logprob=0.0, rank=1, decoded_token='.organization'), 31602: Logprob(logprob=-62.43896484375, rank=2, decoded_token='.le'), 24844: Logprob(logprob=-799.2196044921875, rank=3, decoded_token='organization'), 51272: Logprob(logprob=-811.7073974609375, rank=4, decoded_token='.organ'), 7471: Logprob(logprob=-955.317138671875, rank=5, decoded_token=' organization')}, {22200: Logprob(logprob=0.0, rank=1, decoded_token='_found'), 31602: Logprob(logprob=-262.243896484375, rank=2, decoded_token='.le'), 840: Logprob(logprob=-387.1220703125, rank=3, decoded_token='.f'), 12013: Logprob(logprob=-636.8780517578125, rank=4, decoded_token='_le'), 766: Logprob(logprob=-905.365966796875, rank=5, decoded_token='_f')}, {261: Logprob(logprob=0.0, rank=1, decoded_token='er'), 388: Logprob(logprob=-1511.0244140625, rank=2, decoded_token='ers'), 38149: Logprob(logprob=-1560.9755859375, rank=3, decoded_token='rer'), 81: Logprob(logprob=-1798.243896484375, rank=4, decoded_token='r'), 643: Logprob(logprob=-1810.731689453125, rank=5, decoded_token='ER')}, {51272: Logprob(logprob=0.0, rank=1, decoded_token='.organ'), 70324: Logprob(logprob=-1486.0487060546875, rank=2, decoded_token='.organization'), 8609: Logprob(logprob=-1654.634033203125, rank=3, decoded_token='rgan'), 8629: Logprob(logprob=-1673.36572265625, rank=4, decoded_token='organ'), 2726: Logprob(logprob=-1767.0242919921875, rank=5, decoded_token='.org')}, {8200: Logprob(logprob=0.0, rank=1, decoded_token='izations'), 56276: Logprob(logprob=-1698.3414306640625, rank=2, decoded_token='isations'), 13978: Logprob(logprob=-1723.3170166015625, rank=3, decoded_token='isms'), 70353: Logprob(logprob=-1873.170654296875, rank=4, decoded_token='organizations'), 811: Logprob(logprob=-1935.6097412109375, rank=5, decoded_token='ations')}, {766: Logprob(logprob=0.0, rank=1, decoded_token='_f'), 840: Logprob(logprob=-1411.1220703125, rank=2, decoded_token='.f'), 22200: Logprob(logprob=-1486.048828125, rank=3, decoded_token='_found'), 1426: Logprob(logprob=-1704.58544921875, rank=4, decoded_token='_F'), 2269: Logprob(logprob=-1748.292724609375, rank=5, decoded_token='-f')}, {13382: Logprob(logprob=0.0, rank=1, decoded_token='ounded'), 13900: Logprob(logprob=-1710.8292236328125, rank=2, decoded_token='ounding'), 37153: Logprob(logprob=-1804.48779296875, rank=3, decoded_token='unded'), 4159: Logprob(logprob=-1848.195068359375, rank=4, decoded_token='oundation'), 3023: Logprob(logprob=-1860.682861328125, rank=5, decoded_token='oud')}, {128262: Logprob(logprob=0.0, rank=1, decoded_token='[Partially Relevant]'), 128261: Logprob(logprob=-586.9268798828125, rank=2, decoded_token='[Fully Relevant]'), 128260: Logprob(logprob=-899.1220703125, rank=3, decoded_token='[Unrelevant]'), 58: Logprob(logprob=-1329.9512939453125, rank=4, decoded_token='['), 11: Logprob(logprob=-1529.7562255859375, rank=5, decoded_token=',')}, {24844: Logprob(logprob=-0.6931471824645996, rank=2, decoded_token='organization'), 27243: Logprob(logprob=-0.6931471824645996, rank=1, decoded_token='business'), 79486: Logprob(logprob=-731.2297973632812, rank=3, decoded_token='leaders'), 128259: Logprob(logprob=-743.7175903320312, rank=4, decoded_token='[Retrieve Entity]'), 2588: Logprob(logprob=-812.4004516601562, rank=5, decoded_token='location')}, {31602: Logprob(logprob=0.0, rank=1, decoded_token='.le'), 70324: Logprob(logprob=-936.58544921875, rank=2, decoded_token='.organization'), 12013: Logprob(logprob=-1186.341552734375, rank=3, decoded_token='_le'), 51272: Logprob(logprob=-1486.048828125, rank=4, decoded_token='.organ'), 75924: Logprob(logprob=-1498.53662109375, rank=5, decoded_token='(le')}, {6527: Logprob(logprob=0.0, rank=1, decoded_token='aders'), 1013: Logprob(logprob=-1910.6341552734375, rank=2, decoded_token='ader'), 7819: Logprob(logprob=-1985.56103515625, rank=3, decoded_token='ads'), 13572: Logprob(logprob=-2060.48779296875, rank=4, decoded_token='iders'), 300: Logprob(logprob=-2229.0732421875, rank=5, decoded_token='as')}, {2200: Logprob(logprob=0.0, rank=1, decoded_token='hip'), 5383: Logprob(logprob=-1610.9267578125, rank=2, decoded_token='ship'), 69154: Logprob(logprob=-1698.34130859375, rank=3, decoded_token='HIP'), 6151: Logprob(logprob=-1998.0487060546875, rank=4, decoded_token='hi'), 34322: Logprob(logprob=-2060.48779296875, rank=5, decoded_token='hips')}, {70324: Logprob(logprob=0.0, rank=1, decoded_token='.organization'), 83452: Logprob(logprob=-1161.36572265625, rank=2, decoded_token='_organization'), 24844: Logprob(logprob=-1223.8048095703125, rank=3, decoded_token='organization'), 7471: Logprob(logprob=-1223.8048095703125, rank=4, decoded_token=' organization'), 51272: Logprob(logprob=-1411.121826171875, rank=5, decoded_token='.organ')}, {128262: Logprob(logprob=0.0, rank=1, decoded_token='[Partially Relevant]'), 128260: Logprob(logprob=-262.243896484375, rank=2, decoded_token='[Unrelevant]'), 128261: Logprob(logprob=-936.5853271484375, rank=3, decoded_token='[Fully Relevant]'), 58: Logprob(logprob=-1123.90234375, rank=4, decoded_token='['), 128259: Logprob(logprob=-1473.5609130859375, rank=5, decoded_token='[Retrieve Entity]')}, {128259: Logprob(logprob=0.0, rank=1, decoded_token='[Retrieve Entity]'), 27243: Logprob(logprob=-1386.1463623046875, rank=2, decoded_token='business'), 76: Logprob(logprob=-1773.268310546875, rank=3, decoded_token='m'), 3231: Logprob(logprob=-1835.7073974609375, rank=4, decoded_token='base'), 24844: Logprob(logprob=-1841.9512939453125, rank=5, decoded_token='organization')}, {128263: Logprob(logprob=0.0, rank=1, decoded_token='<paragraph>'), 10789: Logprob(logprob=-1829.46337890625, rank=2, decoded_token='king'), 287: Logprob(logprob=-1929.3658447265625, rank=3, decoded_token='ing'), 128256: Logprob(logprob=-1973.0732421875, rank=4, decoded_token='[No Retrieval]'), 953: Logprob(logprob=-2029.268310546875, rank=5, decoded_token='ix')}, {6838: Logprob(logprob=0.0, rank=1, decoded_token='(G'), 5063: Logprob(logprob=-1248.7803955078125, rank=2, decoded_token='(L'), 7: Logprob(logprob=-1273.756103515625, rank=3, decoded_token='('), 14783: Logprob(logprob=-1348.682861328125, rank=4, decoded_token='Google'), 91316: Logprob(logprob=-1361.170654296875, rank=5, decoded_token=' GOOGLE')}, {2738: Logprob(logprob=0.0, rank=1, decoded_token='oogle'), 46: Logprob(logprob=-1036.48779296875, rank=2, decoded_token='O'), 103062: Logprob(logprob=-1461.0731201171875, rank=3, decoded_token='oog'), 78: Logprob(logprob=-1473.5609130859375, rank=4, decoded_token='o'), 1786: Logprob(logprob=-1623.41455078125, rank=5, decoded_token='ool')}, {11: Logprob(logprob=0.0, rank=1, decoded_token=','), 4800: Logprob(logprob=-649.365966796875, rank=2, decoded_token=' Now'), 4953: Logprob(logprob=-686.829345703125, rank=3, decoded_token=' Inc'), 15620: Logprob(logprob=-699.317138671875, rank=4, decoded_token=' LLC'), 34120: Logprob(logprob=-849.1707763671875, rank=5, decoded_token=' Apps')}, {7471: Logprob(logprob=0.0, rank=1, decoded_token=' organization'), 4953: Logprob(logprob=-949.0732421875, rank=2, decoded_token=' Inc'), 70324: Logprob(logprob=-986.53662109375, rank=3, decoded_token='.organization'), 24844: Logprob(logprob=-999.0244140625, rank=4, decoded_token='organization'), 2626: Logprob(logprob=-1086.4390869140625, rank=5, decoded_token=' business')}, {70324: Logprob(logprob=0.0, rank=1, decoded_token='.organization'), 7471: Logprob(logprob=-1286.243896484375, rank=2, decoded_token=' organization'), 24844: Logprob(logprob=-1298.731689453125, rank=3, decoded_token='organization'), 83452: Logprob(logprob=-1323.7073974609375, rank=4, decoded_token='_organization'), 31602: Logprob(logprob=-1461.0732421875, rank=5, decoded_token='.le')}, {840: Logprob(logprob=0.0, rank=1, decoded_token='.f'), 22200: Logprob(logprob=-599.41455078125, rank=2, decoded_token='_found'), 26270: Logprob(logprob=-1111.4146728515625, rank=3, decoded_token='.board'), 85735: Logprob(logprob=-1311.219482421875, rank=4, decoded_token='_membership'), 766: Logprob(logprob=-1329.951171875, rank=5, decoded_token='_f')}, {801: Logprob(logprob=0.0, rank=1, decoded_token='ound'), 13900: Logprob(logprob=-1336.195068359375, rank=2, decoded_token='ounding'), 1263: Logprob(logprob=-1473.5609130859375, rank=3, decoded_token='und'), 3171: Logprob(logprob=-1573.46337890625, rank=4, decoded_token='ounds'), 13382: Logprob(logprob=-1598.43896484375, rank=5, decoded_token='ounded')}, {388: Logprob(logprob=0.0, rank=1, decoded_token='ers'), 5544: Logprob(logprob=-2048.0, rank=2, decoded_token='rs'), 4419: Logprob(logprob=-2060.48779296875, rank=3, decoded_token='ERS'), 1105: Logprob(logprob=-2085.46337890625, rank=4, decoded_token='ors'), 56090: Logprob(logprob=-2422.63427734375, rank=5, decoded_token='ersh')}, {11: Logprob(logprob=0.0, rank=1, decoded_token=','), 8: Logprob(logprob=-1866.9267578125, rank=2, decoded_token=')'), 1174: Logprob(logprob=-1873.170654296875, rank=3, decoded_token=' ,'), 345: Logprob(logprob=-1885.658447265625, rank=4, decoded_token=',\\n'), 13: Logprob(logprob=-1885.658447265625, rank=5, decoded_token='.')}, {74529: Logprob(logprob=0.0, rank=1, decoded_token=' Sergey'), 30390: Logprob(logprob=-499.5123291015625, rank=2, decoded_token=' Larry'), 85098: Logprob(logprob=-880.390380859375, rank=3, decoded_token=' Sergei'), 5874: Logprob(logprob=-992.7806396484375, rank=4, decoded_token=' Page'), 5195: Logprob(logprob=-999.0245361328125, rank=5, decoded_token=' Google')}, {3320: Logprob(logprob=0.0, rank=1, decoded_token=' Br'), 5874: Logprob(logprob=-824.195068359375, rank=2, decoded_token=' Page'), 30444: Logprob(logprob=-980.292724609375, rank=3, decoded_token=' Bin'), 74529: Logprob(logprob=-999.0244140625, rank=4, decoded_token=' Sergey'), 6971: Logprob(logprob=-1186.3414306640625, rank=5, decoded_token='Br')}, {258: Logprob(logprob=0.0, rank=1, decoded_token='in'), 19479: Logprob(logprob=-1311.219482421875, rank=2, decoded_token='ин'), 1354: Logprob(logprob=-1354.9267578125, rank=3, decoded_token='ins'), 6258: Logprob(logprob=-1417.36572265625, rank=4, decoded_token='inn'), 3817: Logprob(logprob=-1523.5120849609375, rank=5, decoded_token='lin')}, {1237: Logprob(logprob=0.0, rank=1, decoded_token=');'), 8: Logprob(logprob=-1173.8536376953125, rank=2, decoded_token=')'), 1680: Logprob(logprob=-1473.56103515625, rank=3, decoded_token='):'), 323: Logprob(logprob=-1523.51220703125, rank=4, decoded_token=' and'), 4772: Logprob(logprob=-1536.0, rank=5, decoded_token=\"');\")}, {7: Logprob(logprob=0.0, rank=1, decoded_token='('), 14783: Logprob(logprob=-1998.0487060546875, rank=2, decoded_token='Google'), 320: Logprob(logprob=-2185.36572265625, rank=3, decoded_token=' ('), 1209: Logprob(logprob=-2241.56103515625, rank=4, decoded_token='(('), 6838: Logprob(logprob=-2254.048828125, rank=5, decoded_token='(G')}, {14783: Logprob(logprob=0.0, rank=1, decoded_token='Google'), 17943: Logprob(logprob=-1610.9267578125, rank=2, decoded_token='google'), 48255: Logprob(logprob=-1798.243896484375, rank=3, decoded_token='_google'), 5195: Logprob(logprob=-1835.707275390625, rank=4, decoded_token=' Google'), 63745: Logprob(logprob=-1860.682861328125, rank=5, decoded_token='-google')}, {11: Logprob(logprob=0.0, rank=1, decoded_token=','), 34120: Logprob(logprob=-1211.3170166015625, rank=2, decoded_token=' Apps'), 28508: Logprob(logprob=-1373.658447265625, rank=3, decoded_token=' Maps'), 9289: Logprob(logprob=-1473.5609130859375, rank=4, decoded_token='plex'), 7694: Logprob(logprob=-1486.0487060546875, rank=5, decoded_token=' Search')}, {7471: Logprob(logprob=0.0, rank=1, decoded_token=' organization'), 24844: Logprob(logprob=-1198.829345703125, rank=2, decoded_token='organization'), 70324: Logprob(logprob=-1223.804931640625, rank=3, decoded_token='.organization'), 83452: Logprob(logprob=-1635.9024658203125, rank=4, decoded_token='_organization'), 22139: Logprob(logprob=-1648.3902587890625, rank=5, decoded_token=' organisation')}, {70324: Logprob(logprob=0.0, rank=1, decoded_token='.organization'), 7471: Logprob(logprob=-1223.804931640625, rank=2, decoded_token=' organization'), 83452: Logprob(logprob=-1236.292724609375, rank=3, decoded_token='_organization'), 24844: Logprob(logprob=-1248.780517578125, rank=4, decoded_token='organization'), 2726: Logprob(logprob=-1585.9512939453125, rank=5, decoded_token='.org')}, {840: Logprob(logprob=0.0, rank=1, decoded_token='.f'), 22200: Logprob(logprob=-1785.756103515625, rank=2, decoded_token='_found'), 48727: Logprob(logprob=-1798.243896484375, rank=3, decoded_token=' founders'), 968: Logprob(logprob=-1954.341552734375, rank=4, decoded_token='(f'), 17514: Logprob(logprob=-1966.829345703125, rank=5, decoded_token=',f')}, {801: Logprob(logprob=0.0, rank=1, decoded_token='ound'), 1263: Logprob(logprob=-1061.46337890625, rank=2, decoded_token='und'), 3171: Logprob(logprob=-1386.146240234375, rank=3, decoded_token='ounds'), 13382: Logprob(logprob=-1398.6341552734375, rank=4, decoded_token='ounded'), 3023: Logprob(logprob=-1398.6341552734375, rank=5, decoded_token='oud')}, {388: Logprob(logprob=0.0, rank=1, decoded_token='ers'), 1105: Logprob(logprob=-1723.3173828125, rank=2, decoded_token='ors'), 4419: Logprob(logprob=-1898.146484375, rank=3, decoded_token='ERS'), 5079: Logprob(logprob=-1985.561279296875, rank=4, decoded_token='ners'), 261: Logprob(logprob=-1985.561279296875, rank=5, decoded_token='er')}, {11: Logprob(logprob=0.0, rank=1, decoded_token=','), 13: Logprob(logprob=-1798.2440185546875, rank=2, decoded_token='.'), 2637: Logprob(logprob=-1979.317138671875, rank=3, decoded_token='.,'), 31214: Logprob(logprob=-1998.048828125, rank=4, decoded_token=',L'), 1236: Logprob(logprob=-2129.1708984375, rank=5, decoded_token='.L')}, {30390: Logprob(logprob=0.0, rank=1, decoded_token=' Larry'), 28574: Logprob(logprob=-1023.9998779296875, rank=2, decoded_token=' Lawrence'), 89595: Logprob(logprob=-1173.8536376953125, rank=3, decoded_token='Larry'), 5874: Logprob(logprob=-1198.8292236328125, rank=4, decoded_token=' Page'), 29808: Logprob(logprob=-1273.7559814453125, rank=5, decoded_token=' Jerry')}, {5874: Logprob(logprob=0.0, rank=1, decoded_token=' Page'), 2732: Logprob(logprob=-1398.6341552734375, rank=2, decoded_token='Page'), 87511: Logprob(logprob=-1411.1219482421875, rank=3, decoded_token=' Paige'), 2199: Logprob(logprob=-1473.56103515625, rank=4, decoded_token=' page'), 52640: Logprob(logprob=-1560.9757080078125, rank=5, decoded_token='_Page')}, {8: Logprob(logprob=0.0, rank=1, decoded_token=')'), 340: Logprob(logprob=-1298.7318115234375, rank=2, decoded_token=')\\n'), 1237: Logprob(logprob=-1398.63427734375, rank=3, decoded_token=');'), 60: Logprob(logprob=-1604.6829833984375, rank=4, decoded_token=']'), 595: Logprob(logprob=-1610.9268798828125, rank=5, decoded_token='))')}, {128264: Logprob(logprob=0.0, rank=1, decoded_token='</paragraph>'), 128262: Logprob(logprob=-1898.146484375, rank=2, decoded_token='[Partially Relevant]'), 650: Logprob(logprob=-1923.1220703125, rank=3, decoded_token=' V'), 128258: Logprob(logprob=-1935.60986328125, rank=4, decoded_token='[Continue to Retrieve Evidence]'), 128260: Logprob(logprob=-1991.804931640625, rank=5, decoded_token='[Unrelevant]')}, {50: Logprob(logprob=0.0, rank=1, decoded_token='S'), 14783: Logprob(logprob=-699.317138671875, rank=2, decoded_token='Google'), 74529: Logprob(logprob=-1024.0001220703125, rank=3, decoded_token=' Sergey'), 32845: Logprob(logprob=-1173.853759765625, rank=4, decoded_token='Ser'), 89595: Logprob(logprob=-1223.804931640625, rank=5, decoded_token='Larry')}, {10286: Logprob(logprob=0.0, rank=1, decoded_token='erge'), 2431: Logprob(logprob=-899.121826171875, rank=2, decoded_token='erg'), 1216: Logprob(logprob=-1111.41455078125, rank=3, decoded_token='ey'), 43043: Logprob(logprob=-1273.756103515625, rank=4, decoded_token='ergy'), 74529: Logprob(logprob=-1361.170654296875, rank=5, decoded_token=' Sergey')}, {88: Logprob(logprob=0.0, rank=1, decoded_token='y'), 72: Logprob(logprob=-1585.9510498046875, rank=2, decoded_token='i'), 73: Logprob(logprob=-1629.658447265625, rank=3, decoded_token='j'), 1065: Logprob(logprob=-1717.072998046875, rank=4, decoded_token='ys'), 85407: Logprob(logprob=-1791.9998779296875, rank=5, decoded_token='yb')}, {3320: Logprob(logprob=0.0, rank=1, decoded_token=' Br'), 6971: Logprob(logprob=-1386.146240234375, rank=2, decoded_token='Br'), 30444: Logprob(logprob=-1461.0731201171875, rank=3, decoded_token=' Bin'), 426: Logprob(logprob=-1554.731689453125, rank=4, decoded_token=' B'), 1437: Logprob(logprob=-1560.9755859375, rank=5, decoded_token=' br')}, {258: Logprob(logprob=0.0, rank=1, decoded_token='in'), 41622: Logprob(logprob=-1685.8536376953125, rank=2, decoded_token='inz'), 318: Logprob(logprob=-1685.8536376953125, rank=3, decoded_token='im'), 321: Logprob(logprob=-1760.780517578125, rank=4, decoded_token='il'), 485: Logprob(logprob=-1798.243896484375, rank=5, decoded_token='ind')}, {128262: Logprob(logprob=0.0, rank=1, decoded_token='[Partially Relevant]'), 128260: Logprob(logprob=-24.9755859375, rank=2, decoded_token='[Unrelevant]'), 58: Logprob(logprob=-174.829345703125, rank=3, decoded_token='['), 128261: Logprob(logprob=-874.1463623046875, rank=4, decoded_token='[Fully Relevant]'), 323: Logprob(logprob=-917.853759765625, rank=5, decoded_token=' and')}, {89595: Logprob(logprob=0.0, rank=1, decoded_token='Larry'), 30390: Logprob(logprob=-936.58544921875, rank=2, decoded_token=' Larry'), 39166: Logprob(logprob=-1436.09765625, rank=3, decoded_token='Law'), 128258: Logprob(logprob=-1685.8536376953125, rank=4, decoded_token='[Continue to Retrieve Evidence]'), 43: Logprob(logprob=-1773.268310546875, rank=5, decoded_token='L')}, {5874: Logprob(logprob=0.0, rank=1, decoded_token=' Page'), 2199: Logprob(logprob=-1436.097412109375, rank=2, decoded_token=' page'), 2732: Logprob(logprob=-1648.39013671875, rank=3, decoded_token='Page'), 43455: Logprob(logprob=-1735.8048095703125, rank=4, decoded_token=' Pag'), 66539: Logprob(logprob=-1810.7315673828125, rank=5, decoded_token='\\tPage')}, {128262: Logprob(logprob=0.0, rank=1, decoded_token='[Partially Relevant]'), 58: Logprob(logprob=-936.58544921875, rank=2, decoded_token='['), 128260: Logprob(logprob=-974.048828125, rank=3, decoded_token='[Unrelevant]'), 128261: Logprob(logprob=-1236.292724609375, rank=4, decoded_token='[Fully Relevant]'), 128264: Logprob(logprob=-1873.1707763671875, rank=5, decoded_token='</paragraph>')}, {128258: Logprob(logprob=0.0, rank=1, decoded_token='[Continue to Retrieve Evidence]'), 128256: Logprob(logprob=-412.09765625, rank=2, decoded_token='[No Retrieval]'), 14783: Logprob(logprob=-911.6097412109375, rank=3, decoded_token='Google'), 128259: Logprob(logprob=-1117.6585693359375, rank=4, decoded_token='[Retrieve Entity]'), 24844: Logprob(logprob=-1130.1463623046875, rank=5, decoded_token='organization')}, {128263: Logprob(logprob=0.0, rank=1, decoded_token='<paragraph>'), 95805: Logprob(logprob=-2054.243896484375, rank=2, decoded_token=' Cyril'), 128258: Logprob(logprob=-2135.41455078125, rank=3, decoded_token='[Continue to Retrieve Evidence]'), 99632: Logprob(logprob=-2172.8779296875, rank=4, decoded_token=' Ting'), 49988: Logprob(logprob=-2172.8779296875, rank=5, decoded_token='任')}, {16455: Logprob(logprob=0.0, rank=1, decoded_token='people'), 27243: Logprob(logprob=-462.0487060546875, rank=2, decoded_token='business'), 20510: Logprob(logprob=-462.0487060546875, rank=3, decoded_token='law'), 258: Logprob(logprob=-561.951171875, rank=4, decoded_token='in'), 32261: Logprob(logprob=-624.39013671875, rank=5, decoded_token='music')}, {31970: Logprob(logprob=0.0, rank=1, decoded_token='.person'), 49190: Logprob(logprob=-1261.268310546875, rank=2, decoded_token='.eth'), 2337: Logprob(logprob=-1261.268310546875, rank=3, decoded_token='.de'), 88230: Logprob(logprob=-1286.243896484375, rank=4, decoded_token='.performance'), 24309: Logprob(logprob=-1348.682861328125, rank=5, decoded_token='_person')}, {93605: Logprob(logprob=0.0, rank=1, decoded_token='.prof'), 1276: Logprob(logprob=-474.53662109375, rank=2, decoded_token='.n'), 13: Logprob(logprob=-674.3414306640625, rank=3, decoded_token='.'), 10265: Logprob(logprob=-736.780517578125, rank=4, decoded_token='.ed'), 73861: Logprob(logprob=-911.6097412109375, rank=5, decoded_token='.parents')}, {1362: Logprob(logprob=0.0, rank=1, decoded_token='ession'), 8719: Logprob(logprob=-1298.731689453125, rank=2, decoded_token='essions'), 16151: Logprob(logprob=-1523.5120849609375, rank=3, decoded_token='essional'), 57081: Logprob(logprob=-1623.41455078125, rank=4, decoded_token='esion'), 434: Logprob(logprob=-1760.7803955078125, rank=5, decoded_token='ess')}, {26: Logprob(logprob=0.0, rank=1, decoded_token=';'), 82960: Logprob(logprob=-237.26806640625, rank=2, decoded_token=';base'), 40514: Logprob(logprob=-611.90234375, rank=3, decoded_token=';s'), 68336: Logprob(logprob=-1061.4632568359375, rank=4, decoded_token=';m'), 56033: Logprob(logprob=-1092.682861328125, rank=5, decoded_token=';b')}], finish_reason=length, stop_reason=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#是否带special token 分开传入，用于检索和生成\n",
    "sampling_params = SamplingParams(\n",
    "            temperature=0.01, top_p=1.0,max_tokens=100, logprobs=5, skip_special_tokens=False, include_stop_str_in_output=True)\n",
    "PROMPT_DICT = {\"llama3\": '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n{input}<|eot_id|><|start_header_id|>assistant<|end_header_id|>'}\n",
    "model.generate([PROMPT_DICT[\"llama3\"].format(input=\"what all does google now do?\")], sampling_params)[0].outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('/media/disk2/llama_factory/generation_1124_special')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_special_tokens(tokenizer, use_grounding=False, use_utility=False):\n",
    "    rel_tokens = {}\n",
    "    for token in ['[Unrelevant]','[Partially Relevant]','[Fully Relevant]']:\n",
    "        rel_tokens[token] = tokenizer.convert_tokens_to_ids(token)\n",
    "\n",
    "    # ut_tokens = None\n",
    "    # if use_utility is True:\n",
    "    #     ut_tokens = {}\n",
    "    #     for token in utility_tokens_names:\n",
    "    #         ut_tokens[token] = tokenizer.convert_tokens_to_ids(token)\n",
    "\n",
    "    return rel_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_tokens = load_special_tokens(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from src.sparql_utils import *\n",
    "def run_step_generation_batch(model, prompt, topic_entity,new_retrieval, beam_width=3):\n",
    "    pattern = r'(.*?)\\[(.*?)\\]'\n",
    "    rel_score_dict = {}\n",
    "    return_entities = []\n",
    "    final_preds = []\n",
    "    overall_scores = {}\n",
    "    paragraph = ';'.join([page.page_content.strip() for page in retriever.invoke(prompt.split('\\n\\n')[1].split('<|eot_id|>')[0])])\n",
    "    print(paragraph)\n",
    "    if new_retrieval:\n",
    "        retrieval_token = \"[New Retrieval]\"\n",
    "        aug_prompts =  [\"<paragraph>{}</paragraph>\".format(paragraph)]\n",
    "    else:\n",
    "        retrieval_token = \"[Continue to Retrieve Evidence]\"\n",
    "        aug_prompts =  [\"<paragraph>{}</paragraph>\".format(paragraph)]\n",
    "    \n",
    "    pred = model.generate(prompt + retrieval_token + aug_prompts[0], sampling_params)[0]\n",
    "    pred_token_ids = pred.outputs[0].token_ids\n",
    "    pred_text_1 = pred.outputs[0].text\n",
    "    pred_log_probs = pred.outputs[0].logprobs\n",
    "    seq_score = pred.outputs[0].cumulative_logprob / \\\n",
    "        max(len(pred.outputs[0].token_ids), 1)\n",
    "    relevance_indices = []\n",
    "    for tok_idx, tok in enumerate(pred_token_ids):\n",
    "        if tok in rel_tokens.values():\n",
    "            relevance_indices.append(tok_idx)\n",
    "    if len(relevance_indices) > 0:\n",
    "        for idx in relevance_indices:\n",
    "            for token, token_id in rel_tokens.items():\n",
    "                prob = pred_log_probs[idx][token_id].logprob if token_id in pred_log_probs[idx] else -100\n",
    "                rel_score_dict[token] = np.exp(prob)\n",
    "    relevance_score = rel_score_dict['[Fully Relevant]']+ rel_score_dict['[Partially Relevant]'] / np.sum(list(rel_score_dict.values()))\n",
    "    processed_pred = pred_text_1.split('[Retrieve Entity]')[0]\n",
    "    matches =  dict(re.findall(pattern,processed_pred))\n",
    "    \n",
    "    name2id = dict()\n",
    "    entity_prompts = []\n",
    "    for _, entity in enumerate(topic_entity):\n",
    "        entities = []\n",
    "        for k, v in matches.items():\n",
    "            if v in ['Fully Relevant', 'Partially Relevant']:\n",
    "                another_entities = get_another_entity(entity, k, return_label=True)\n",
    "                # print(another_entities)\n",
    "                name2id.update(another_entities)\n",
    "                entities.extend([f'({get_label(entity)}, {k}, {e})' for e in another_entities.values()])\n",
    "        entity_prompts.append(aug_prompts[0] + processed_pred +  '[Retrieve Entity]' + \"<paragraph>{}</paragraph>\".format(';'.join(entities[:10])))\n",
    "    # print(aug_prompts)\n",
    "    preds = model.generate([prompt + retrieval_token+ entity_prompts[i] for i in range(len(entity_prompts))], sampling_params)\n",
    "    \n",
    "    for p_idx, pred in enumerate(preds):\n",
    "        pred_token_ids = pred.outputs[0].token_ids\n",
    "        pred_text_2 = pred.outputs[0].text\n",
    "        pred_log_probs = pred.outputs[0].logprobs\n",
    "        rel_score_dict = {}\n",
    "        relevance_indices = []\n",
    "        for tok_idx, tok in enumerate(pred_token_ids):\n",
    "            if tok in rel_tokens.values():\n",
    "                relevance_indices.append(tok_idx)\n",
    "        if len(relevance_indices) > 0:\n",
    "            # print(relevance_indices)\n",
    "            for idx in relevance_indices:\n",
    "                for token, token_id in rel_tokens.items():\n",
    "                    prob = pred_log_probs[idx][token_id].logprob if token_id in pred_log_probs[idx] else -100\n",
    "                    rel_score_dict[token] = np.exp(prob)\n",
    "        overall_scores[p_idx] = relevance_score + rel_score_dict['[Fully Relevant]'] + rel_score_dict['[Partially Relevant]']/ np.sum(list(rel_score_dict.values()))\n",
    "        if '[Continue to Retrieve Evidence]' in pred_text_2:\n",
    "            processed_pred = pred_text_2.split('[Continue to Retrieve Evidence]')[0]\n",
    "            matches =  dict(re.findall(pattern, processed_pred))\n",
    "            for k, v in matches.items():\n",
    "                if v in ['Fully Relevant', 'Partially Relevant']:\n",
    "                    if k in name2id:\n",
    "                        return_entities.append(name2id[k])\n",
    "            processed_pred += '[Continue to Retrieve Evidence]'\n",
    "        else:\n",
    "            processed_pred = pred_text_2\n",
    "        final_preds.append(aug_prompts[0] + processed_pred)\n",
    "    return final_preds, [overall_scores[p_idx] for p_idx in overall_scores], return_entities\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1639\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('./data/merged/WebQSP_test.json', 'r',encoding='utf-8') as f:\n",
    "    test_data = json.load(f)\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 4\n",
      "location.us_state.capital;people.family.country;people.ethnicity.geographic_distribution;fictional_universe.fictional_character.place_of_birth;people.marriage.from\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 55.59 toks/s, output: 63.68 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  2.54it/s, est. speed input: 193.99 toks/s, output: 63.81 toks/s]\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "# for i in range(len(test_data)):\n",
    "i=6\n",
    "print(f'Process {i}')\n",
    "data_input = test_data[i]['question']\n",
    "prompt = PROMPT_DICT['llama3'].format(input= data_input)\n",
    "max_depth = 3\n",
    "topic_entity = list(test_data[i]['gold_entity_map'].keys())\n",
    "# pred = model.generate([prompt], sampling_params)[0]\n",
    "# pred_text = pred.outputs[0].text\n",
    "# if '[New Retrieval]' in pred_text:\n",
    "curr_depth = 1\n",
    "terminated = False\n",
    "node_id = 0\n",
    "prediction_tree = {}\n",
    "levels = {}\n",
    "prediction_tree[node_id] = {\"prompt\": prompt, \"pred\": \"[New Retrieval]\",\n",
    "                            \"processed_pred\": \"\", \"score\": None, \"topic_entity\": topic_entity, \"parent\": None}\n",
    "levels[0] = [0]\n",
    "while curr_depth < max_depth:\n",
    "    levels[curr_depth] = []\n",
    "    if curr_depth-1 in levels and terminated is False:\n",
    "        for node in levels[curr_depth-1]:\n",
    "            pred = prediction_tree[node][\"pred\"]\n",
    "            if \"<|eot_id|>\" in pred:\n",
    "                terminated = True\n",
    "                continue\n",
    "            prompt = prediction_tree[node][\"prompt\"]\n",
    "            prev_generation = prediction_tree[node][\"processed_pred\"]\n",
    "            score = prediction_tree[node][\"score\"]\n",
    "            topic_entity = prediction_tree[node][\"topic_entity\"]\n",
    "            if \"[New Retrieval]\" in pred or \"[Continue to Retrieve Evidence]\" in pred:\n",
    "                retrieval_results = {}\n",
    "                preds, scores, next_entities = run_step_generation_batch(\n",
    "                    model, prompt + prev_generation, topic_entity, new_retrieval=True if (\"[New Retrieval]\" in pred) else False)\n",
    "                for i, (pred, p_score) in enumerate(zip(preds, scores)):\n",
    "                    retrieval_results[i] = {\n",
    "                        \"pred\": pred, \"score\": p_score}\n",
    "\n",
    "                for i, result in retrieval_results.items():\n",
    "                    node_id += 1\n",
    "                    node_score = result[\"score\"] * \\\n",
    "                        score if score is not None else result[\"score\"]\n",
    "                    pred = result[\"pred\"]\n",
    "                    if len(next_entities) == 0:\n",
    "                        next_entities = [topic_entity]\n",
    "                    prediction_tree[node_id] = {\"prompt\": prompt + prev_generation, \"pred\": pred,\n",
    "                                                \"score\": node_score, \"parent\": node,\n",
    "                                                \"topic_entity\": next_entities[0]}\n",
    "                    if \"[Continue to Retrieve Evidence]\" in pred:\n",
    "                        gen_result_index = pred.index(\"[Continue to Retrieve Evidence]\")\n",
    "                        prev_generation = pred[:gen_result_index]\n",
    "                    else:\n",
    "                        prev_generation = pred\n",
    "                    prediction_tree[node_id][\"processed_pred\"] = prev_generation\n",
    "                    levels[curr_depth].append(node_id)\n",
    "            current_rank = levels[curr_depth]\n",
    "            node2score = {\n",
    "                node_id: prediction_tree[node_id][\"score\"] for node_id in current_rank}\n",
    "            top_nodes = sorted(node2score.items(), key=lambda x: x[1], reverse=True)[\n",
    "                :2]\n",
    "            levels[curr_depth] = [node[0] for node in top_nodes]\n",
    "            curr_depth += 1\n",
    "    else:\n",
    "        break\n",
    "labels = [get_label(ans) for ans in test_data[i]['answer']]\n",
    "for label in labels:\n",
    "    if label in prediction_tree[len(prediction_tree)-1]['processed_pred']:\n",
    "        print(f'correct {i}')\n",
    "        count += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['m.03_r3']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(test_data[i]['gold_entity_map'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'prompt': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nwhere was george washington carver from<|eot_id|><|start_header_id|>assistant<|end_header_id|>',\n",
       "  'pred': '[New Retrieval]',\n",
       "  'processed_pred': '',\n",
       "  'score': None,\n",
       "  'topic_entity': ['m.03djm'],\n",
       "  'parent': None},\n",
       " 1: {'prompt': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nwhere was george washington carver from<|eot_id|><|start_header_id|>assistant<|end_header_id|>',\n",
       "  'pred': '<paragraph>location.us_state.capital;people.family.country;people.ethnicity.geographic_distribution;fictional_universe.fictional_character.place_of_birth;people.marriage.from</paragraph>people.family.country[Fully Relevant]people.ethnicity.geographic_distribution[Partially Relevant]location.us_state.capital[Unrelevant][Retrieve Entity]<paragraph>[No Retrieval]Answer: Minneapolis<|eot_id|>',\n",
       "  'score': 2.578040019318527e-07,\n",
       "  'parent': 0,\n",
       "  'topic_entity': ['m.03djm'],\n",
       "  'processed_pred': '<paragraph>location.us_state.capital;people.family.country;people.ethnicity.geographic_distribution;fictional_universe.fictional_character.place_of_birth;people.marriage.from</paragraph>people.family.country[Fully Relevant]people.ethnicity.geographic_distribution[Partially Relevant]location.us_state.capital[Unrelevant][Retrieve Entity]<paragraph>[No Retrieval]Answer: Minneapolis<|eot_id|>'}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtracking_prediction_tree(levels: dict[int,list[int]], curr_depth: int, prediction_tree: dict[int, dict]) -> dict[int,list[int]]:\n",
    "    '''\n",
    "    get best tracking from prediction_tree base on levels\n",
    "    '''\n",
    "    parent = 0 \n",
    "    best_selections = {}\n",
    "    # Traverse from the bottom \n",
    "    levels = {k: v for k, v in levels.items() if len(v) > 0 and k != 0} # remove empty list in levels\n",
    "    for path_i, node in enumerate(levels[len(levels)]): # beam search \n",
    "        if node == 0:\n",
    "            break\n",
    "        best_selections[path_i] = [node] \n",
    "        current_node = node \n",
    "        current_level = curr_depth \n",
    "        if current_node is None:\n",
    "            continue\n",
    "        while current_level > 0 and current_node is not None:\n",
    "            parent = prediction_tree[current_node][\"parent\"]\n",
    "            best_selections[path_i] = [parent] + best_selections[path_i] \n",
    "            current_node = parent \n",
    "            current_level -= 1\n",
    "    return best_selections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_selections = backtracking_prediction_tree(levels, curr_depth, prediction_tree)\n",
    "final_prediction = {}\n",
    "splitted_sentences = {}\n",
    "original_splitted_sentences = {}\n",
    "ctxs = {}\n",
    "for path_i, nodes in best_selections.items():\n",
    "    final_prediction[path_i] = \" \".join([prediction_tree[node][\"processed_pred\"] for node in nodes if node is not None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: ' <paragraph>government.us_vice_president.to_president;government.us_president.vice_president;government.politician.government_positions_held;government.political_appointer.appointees;base.obamabase.cabinet_member.cabinet_position</paragraph>None[Fully Relevant]George M. Dallas[Unrelevant][No Retrieval]Answer: Speaker of the House of Representatives;President of the House of Representatives<|eot_id|>'}\n"
     ]
    }
   ],
   "source": [
    "print(final_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from src.sparql_utils import *\n",
    "import random\n",
    "def run_relation_generation_batch(model, prompt, new_retrieval, context, hypo=True):\n",
    "    rel_score_dict = {}\n",
    "    final_preds = []\n",
    "    overall_scores = []\n",
    "    final_contexts = []\n",
    "    if new_retrieval:\n",
    "        retrieval_token = \"[New Retrieval]\"\n",
    "    else:\n",
    "        retrieval_token = \"[Continue to Retrieve Evidence]\"\n",
    "    paragraph = [page.page_content.strip() for page in retriever.invoke(prompt.split('\\n\\n')[1].split('<|eot_id|>')[0] + ' '+ context)]\n",
    "    if hypo:\n",
    "        hypo_rel = model.generate(prompt + retrieval_token, sampling_params)[0].outputs[0].text\n",
    "        pattern = r'(\\w+\\.\\w+\\.\\w+)\\[(.*?)\\]'\n",
    "        if '[Retrieve Entity]' in hypo_rel:\n",
    "            matches =  dict(re.findall(pattern,hypo_rel.split('[Retrieve Entity]')[0]))\n",
    "            string = ''\n",
    "            for k,v in matches.items():\n",
    "                if v in ['Fully Relevant', 'Partially Relevant']:\n",
    "                    string += k + ' '\n",
    "            for extra_rel in retriever.invoke(string):\n",
    "                if extra_rel.page_content.strip() not in paragraph:\n",
    "                    paragraph.append(extra_rel.page_content.strip())\n",
    "    \n",
    "    aug_prompts =  [\"<paragraph>{}</paragraph>\".format(';'.join(p))  for p in [paragraph[i: i+5] for i in range(0, len(paragraph), 5)]]\n",
    "    \n",
    "    preds = model.generate([prompt + retrieval_token + aug for aug in aug_prompts], sampling_params)\n",
    "    for p_id, pred in enumerate(preds):\n",
    "        pred_token_ids = pred.outputs[0].token_ids\n",
    "        pred_text_1 = pred.outputs[0].text\n",
    "        pred_log_probs = pred.outputs[0].logprobs\n",
    "        seq_score = pred.outputs[0].cumulative_logprob / \\\n",
    "            max(len(pred.outputs[0].token_ids), 1)\n",
    "        relevance_indices = []\n",
    "        for tok_idx, tok in enumerate(pred_token_ids):\n",
    "            if tok in rel_tokens.values():\n",
    "                relevance_indices.append(tok_idx)\n",
    "        if len(relevance_indices) > 0:\n",
    "            for idx in relevance_indices:\n",
    "                for token, token_id in rel_tokens.items():\n",
    "                    prob = pred_log_probs[idx][token_id].logprob if token_id in pred_log_probs[idx] else -100\n",
    "                    rel_score_dict[token] = np.exp(prob)\n",
    "        relevance_score = rel_score_dict['[Fully Relevant]']+ rel_score_dict['[Partially Relevant]'] / np.sum(list(rel_score_dict.values()))\n",
    "        assert '[Retrieve Entity]' in pred_text_1\n",
    "        processed_pred = pred_text_1.split('[Retrieve Entity]')[0] + '[Retrieve Entity]'\n",
    "        final_preds.append(retrieval_token + aug_prompts[p_id] + processed_pred)\n",
    "        overall_scores.append(relevance_score)\n",
    "        final_contexts.append(pred_text_1.split('[Retrieve Entity]')[0])\n",
    "    return final_preds, overall_scores, final_contexts\n",
    "\n",
    "def run_entity_generation_batch(model, prompt, topic_entity, context):\n",
    "    final_preds = []\n",
    "    overall_scores = {}\n",
    "    final_entities = []\n",
    "    final_context = []\n",
    "    pattern = r'(.*?)\\[(.*?)\\]'\n",
    "    matches =  dict(re.findall(pattern,context))\n",
    "    name2id = dict()\n",
    "    entity_prompts = []\n",
    "    for _, entity in enumerate(topic_entity):\n",
    "        entities = []\n",
    "        for k, v in matches.items():\n",
    "            if v in ['Fully Relevant', 'Partially Relevant']:\n",
    "                another_entities = get_another_entity(entity, k, return_label=True)\n",
    "                # handle Unkown mid entities\n",
    "                if len(another_entities):\n",
    "                    name2id.update(another_entities)\n",
    "                    entities.extend([f'({get_label(entity)}, {k}, {e})' for e in another_entities.keys()])\n",
    "                    entity_prompts.append(prompt+  '[Retrieve Entity]' + \"<paragraph>{}</paragraph>\".format(';'.join(entities[:5])))\n",
    "    # print(aug_prompts)\n",
    "    preds = model.generate([prompt + entity_prompts[i] for i in range(len(entity_prompts))], sampling_params)\n",
    "    \n",
    "    for p_idx, pred in enumerate(preds):\n",
    "        return_entities = dict()\n",
    "        pred_token_ids = pred.outputs[0].token_ids\n",
    "        pred_text_2 = pred.outputs[0].text\n",
    "        pred_log_probs = pred.outputs[0].logprobs\n",
    "        rel_score_dict = {}\n",
    "        relevance_indices = []\n",
    "        for tok_idx, tok in enumerate(pred_token_ids):\n",
    "            if tok in rel_tokens.values():\n",
    "                relevance_indices.append(tok_idx)\n",
    "        if len(relevance_indices) > 0:\n",
    "            # print(relevance_indices)\n",
    "            for idx in relevance_indices:\n",
    "                for token, token_id in rel_tokens.items():\n",
    "                    prob = pred_log_probs[idx][token_id].logprob if token_id in pred_log_probs[idx] else -100\n",
    "                    rel_score_dict[token] = np.exp(prob)\n",
    "        overall_scores[p_idx] = rel_score_dict['[Fully Relevant]'] + rel_score_dict['[Partially Relevant]']/ np.sum(list(rel_score_dict.values()))\n",
    "        if '[Continue to Retrieve Evidence]' in pred_text_2:\n",
    "            processed_pred = pred_text_2.split('[Continue to Retrieve Evidence]')[0]\n",
    "            matches =  dict(re.findall(pattern, processed_pred))\n",
    "            for k, v in matches.items():\n",
    "                if v in ['Fully Relevant', 'Partially Relevant']:\n",
    "                    if k in name2id:\n",
    "                        return_entities[k] = name2id[k]\n",
    "                    elif k == 'Unknown Entity':\n",
    "                        random_key = random.choice(list(name2id.keys()))\n",
    "                        return_entities[random_key] = name2id[random_key]\n",
    "            processed_pred += '[Continue to Retrieve Evidence]'\n",
    "        elif '[No Retrieval]' in pred_text_2:\n",
    "            processed_pred = pred_text_2\n",
    "        else:\n",
    "            processed_pred = '[No Retrieval]'\n",
    "        final_preds.append(processed_pred)\n",
    "        final_entities.append(return_entities.values())\n",
    "        final_context.append(' '.join(return_entities.keys()))\n",
    "    return final_preds, [overall_scores[p_idx] for p_idx in overall_scores], final_entities, final_context\n",
    " \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.59s/it, est. speed input: 11.99 toks/s, output: 63.10 toks/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:01<00:00,  1.30it/s, est. speed input: 73.42 toks/s, output: 98.11 toks/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:01<00:00,  1.32it/s, est. speed input: 312.41 toks/s, output: 131.54 toks/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00,  8.02it/s, est. speed input: 1303.00 toks/s, output: 84.71 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.48s/it, est. speed input: 60.83 toks/s, output: 67.59 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.47s/it, est. speed input: 90.53 toks/s, output: 68.06 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "correct_ids = []\n",
    "for index in range(0, 200):\n",
    "# index = 3\n",
    "    hit = 0\n",
    "    print(f'Process {index}')\n",
    "    data_input = test_data[index]['question']\n",
    "    prompt = PROMPT_DICT['llama3'].format(input= data_input)\n",
    "    max_depth = 5\n",
    "    topic_entity = list(test_data[index]['gold_entity_map'].keys())\n",
    "    # pred = model.generate([prompt], sampling_params)[0]\n",
    "    # pred_text = pred.outputs[0].text\n",
    "    # if '[New Retrieval]' in pred_text:\n",
    "    curr_depth = 1\n",
    "    terminated = False\n",
    "    node_id = 0\n",
    "    prediction_tree = {}\n",
    "    levels = {}\n",
    "    prediction_tree[node_id] = {\"prompt\": prompt, \"pred\": \"[New Retrieval]\",\n",
    "                                \"processed_pred\": \"\", \"score\": None, \"topic_entity\": topic_entity, \"parent\": None, \"context\": ''}\n",
    "    levels[0] = [0]\n",
    "    while curr_depth < max_depth:\n",
    "        levels[curr_depth] = []\n",
    "        if curr_depth-1 in levels and terminated is False:\n",
    "            for node in levels[curr_depth-1]:\n",
    "                curr_pred = prediction_tree[node][\"pred\"]\n",
    "                if \"<|eot_id|>\" in curr_pred:\n",
    "                    terminated = True\n",
    "                    continue\n",
    "                prompt = prediction_tree[node][\"prompt\"]\n",
    "                prev_generation = prediction_tree[node][\"processed_pred\"]\n",
    "                score = prediction_tree[node][\"score\"]\n",
    "                topic_entity = prediction_tree[node][\"topic_entity\"]\n",
    "                context = prediction_tree[node]['context']\n",
    "                cur_prompt = prompt + prev_generation\n",
    "                if \"Retrieve Entity\" in curr_pred.split('[')[-1]:\n",
    "                    retrieval_results = {}\n",
    "                    preds, scores, next_entities, contexts = run_entity_generation_batch(\n",
    "                        model, cur_prompt, topic_entity, context)\n",
    "                    for i, (pred, p_score,next_topic, context) in enumerate(zip(preds, scores, next_entities, contexts)):\n",
    "                        retrieval_results[i] = {\n",
    "                            \"pred\": pred, \"score\": p_score, \"next_topic\": next_topic, \"context\": context}\n",
    "\n",
    "                    for i, result in retrieval_results.items():\n",
    "                        node_id += 1\n",
    "                        node_score = result[\"score\"] * \\\n",
    "                            score if score is not None else result[\"score\"]\n",
    "                        pred = result[\"pred\"]\n",
    "                        next_entity = result['next_topic']\n",
    "                        if len(next_entity) == 0:\n",
    "                            next_entity = topic_entity\n",
    "                        prediction_tree[node_id] = {\"prompt\": cur_prompt, \"pred\": pred, \"context\": result['context'],\n",
    "                                                    \"score\": node_score, \"parent\": node,\n",
    "                                                    \"topic_entity\": next_entity}\n",
    "                        if \"[Continue to Retrieve Evidence]\" in pred:\n",
    "                            gen_result_index = pred.index(\"[Continue to Retrieve Evidence]\")\n",
    "                            prev_generation = pred[:gen_result_index]\n",
    "                        else:\n",
    "                            prev_generation = pred\n",
    "                        prediction_tree[node_id][\"processed_pred\"] = prev_generation\n",
    "                        levels[curr_depth].append(node_id)\n",
    "                #存在前后逻辑粘连   \n",
    "                if \"New Retrieval\" in curr_pred.split('[')[-1] or \"Continue to Retrieve Evidence\" in curr_pred.split('[')[-1]:\n",
    "                    retrieval_results = {}\n",
    "                    preds, scores, contexts = run_relation_generation_batch(\n",
    "                        model, cur_prompt, new_retrieval=True if (\"[New Retrieval]\" in curr_pred) else False, context=context, hypo=True)\n",
    "                    for i, (pred, p_score, context) in enumerate(zip(preds, scores, contexts)):\n",
    "                        retrieval_results[i] = {\n",
    "                            \"pred\": pred, \"score\": p_score, \"context\": context}\n",
    "\n",
    "                    for i, result in retrieval_results.items():\n",
    "                        node_id += 1\n",
    "                        node_score = result[\"score\"] * \\\n",
    "                            score if score is not None else result[\"score\"]\n",
    "                        pred = result[\"pred\"]\n",
    "                        context = result[\"context\"]\n",
    "                        prediction_tree[node_id] = {\"prompt\": cur_prompt, \"pred\": pred,\n",
    "                                                    \"score\": node_score, \"parent\": node,\n",
    "                                                    \"topic_entity\": topic_entity, \"context\": context}\n",
    "                        if \"[Retrieve Entity]\" in pred:\n",
    "                            gen_result_index = pred.index(\"[Retrieve Entity]\")\n",
    "                            prev_generation = pred[:gen_result_index]\n",
    "                        else:\n",
    "                            prev_generation = pred\n",
    "                        prediction_tree[node_id][\"processed_pred\"] = prev_generation\n",
    "                        levels[curr_depth].append(node_id)\n",
    "            current_rank = levels[curr_depth]\n",
    "            node2score = {\n",
    "                node_id: prediction_tree[node_id][\"score\"] for node_id in current_rank}\n",
    "            top_nodes = sorted(node2score.items(), key=lambda x: x[1], reverse=True)[\n",
    "                :3]\n",
    "            levels[curr_depth] = [node[0] for node in top_nodes]\n",
    "            curr_depth += 1\n",
    "        else:\n",
    "            break\n",
    "    labels = [get_label(ans) for ans in test_data[index]['answer']]\n",
    "    for tree_node in prediction_tree.values():\n",
    "        if 'Answer' in tree_node['processed_pred']:\n",
    "            answer = tree_node['processed_pred'].split('Answer:')[-1]\n",
    "            for label in labels:\n",
    "                if label and label in answer:\n",
    "                    hit = 1\n",
    "    if hit == 1:\n",
    "        print('Correct')\n",
    "        count += 1\n",
    "        correct_ids.append(index)\n",
    "\n",
    "    #注意有value标签, e.g. WebQTest-31\n",
    "    # for label in labels:\n",
    "    #     if label and label in prediction_tree[len(prediction_tree)-1]['processed_pred']:\n",
    "    #         count += 1\n",
    "    #         break\n",
    "    # except:\n",
    "    #     print(f'{index} Error')\n",
    "    #     continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_another_entity('m.045c7b', 'location.location.contains')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'prompt': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nwho plays ken barlow in coronation street<|eot_id|><|start_header_id|>assistant<|end_header_id|>',\n",
       "  'pred': '[New Retrieval]',\n",
       "  'processed_pred': '',\n",
       "  'score': None,\n",
       "  'topic_entity': ['m.015lwh', 'm.01_2n'],\n",
       "  'parent': None,\n",
       "  'context': ''},\n",
       " 1: {'prompt': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nwho plays ken barlow in coronation street<|eot_id|><|start_header_id|>assistant<|end_header_id|>',\n",
       "  'pred': '[New Retrieval]<paragraph>tv.tv_actor.starring_roles;tv.regular_tv_appearance.actor;tv.tv_guest_role.actor;base.gossipgirl.character.played_by;tv.tv_actor.guest_roles</paragraph>tv.tv_actor.starring_roles[Fully Relevant]tv.regular_tv_appearance.actor[Fully Relevant]tv.tv_guest_role.actor[Partially Relevant][Retrieve Entity]',\n",
       "  'score': 1.0,\n",
       "  'parent': 0,\n",
       "  'topic_entity': ['m.015lwh', 'm.01_2n'],\n",
       "  'context': 'tv.tv_actor.starring_roles[Fully Relevant]tv.regular_tv_appearance.actor[Fully Relevant]tv.tv_guest_role.actor[Partially Relevant]',\n",
       "  'processed_pred': '[New Retrieval]<paragraph>tv.tv_actor.starring_roles;tv.regular_tv_appearance.actor;tv.tv_guest_role.actor;base.gossipgirl.character.played_by;tv.tv_actor.guest_roles</paragraph>tv.tv_actor.starring_roles[Fully Relevant]tv.regular_tv_appearance.actor[Fully Relevant]tv.tv_guest_role.actor[Partially Relevant]'},\n",
       " 2: {'prompt': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nwho plays ken barlow in coronation street<|eot_id|><|start_header_id|>assistant<|end_header_id|>',\n",
       "  'pred': '[New Retrieval]<paragraph>tv.tv_series_season.regular_cast;tv.tv_program.regular_cast;tv.regular_tv_appearance.series;tv.regular_tv_appearance.character</paragraph>tv.tv_series_season.regular_cast[Fully Relevant]tv.tv_program.regular_cast[Fully Relevant]tv.regular_tv_appearance.character[Partially Relevant][Retrieve Entity]',\n",
       "  'score': 1.0,\n",
       "  'parent': 0,\n",
       "  'topic_entity': ['m.015lwh', 'm.01_2n'],\n",
       "  'context': 'tv.tv_series_season.regular_cast[Fully Relevant]tv.tv_program.regular_cast[Fully Relevant]tv.regular_tv_appearance.character[Partially Relevant]',\n",
       "  'processed_pred': '[New Retrieval]<paragraph>tv.tv_series_season.regular_cast;tv.tv_program.regular_cast;tv.regular_tv_appearance.series;tv.regular_tv_appearance.character</paragraph>tv.tv_series_season.regular_cast[Fully Relevant]tv.tv_program.regular_cast[Fully Relevant]tv.regular_tv_appearance.character[Partially Relevant]'},\n",
       " 3: {'prompt': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nwho plays ken barlow in coronation street<|eot_id|><|start_header_id|>assistant<|end_header_id|>[New Retrieval]<paragraph>tv.tv_series_season.regular_cast;tv.tv_program.regular_cast;tv.regular_tv_appearance.series;tv.regular_tv_appearance.character</paragraph>tv.tv_series_season.regular_cast[Fully Relevant]tv.tv_program.regular_cast[Fully Relevant]tv.regular_tv_appearance.character[Partially Relevant]',\n",
       "  'pred': 'Unknown Entity[Partially Relevant][Continue to Retrieve Evidence]',\n",
       "  'context': 'm.0x139sw',\n",
       "  'score': 1.0,\n",
       "  'parent': 2,\n",
       "  'topic_entity': dict_values(['m.0x139sw']),\n",
       "  'processed_pred': 'Unknown Entity[Partially Relevant]'},\n",
       " 4: {'prompt': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nwho plays ken barlow in coronation street<|eot_id|><|start_header_id|>assistant<|end_header_id|>[New Retrieval]<paragraph>tv.tv_series_season.regular_cast;tv.tv_program.regular_cast;tv.regular_tv_appearance.series;tv.regular_tv_appearance.character</paragraph>tv.tv_series_season.regular_cast[Fully Relevant]tv.tv_program.regular_cast[Fully Relevant]tv.regular_tv_appearance.character[Partially Relevant]',\n",
       "  'pred': 'Unknown Entity[Partially Relevant][Continue to Retrieve Evidence]',\n",
       "  'context': 'm.0x0l8r_',\n",
       "  'score': 1.0,\n",
       "  'parent': 2,\n",
       "  'topic_entity': dict_values(['m.0x0l8r_']),\n",
       "  'processed_pred': 'Unknown Entity[Partially Relevant]'},\n",
       " 5: {'prompt': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nwho plays ken barlow in coronation street<|eot_id|><|start_header_id|>assistant<|end_header_id|>[New Retrieval]<paragraph>tv.tv_series_season.regular_cast;tv.tv_program.regular_cast;tv.regular_tv_appearance.series;tv.regular_tv_appearance.character</paragraph>tv.tv_series_season.regular_cast[Fully Relevant]tv.tv_program.regular_cast[Fully Relevant]tv.regular_tv_appearance.character[Partially Relevant]Unknown Entity[Partially Relevant]',\n",
       "  'pred': '[Continue to Retrieve Evidence]<paragraph>tv.tv_actor.starring_roles;tv.regular_tv_appearance.actor;tv.tv_guest_role.actor;tv.tv_actor.guest_roles;base.gossipgirl.character.played_by</paragraph>tv.tv_actor.starring_roles[Fully Relevant]tv.regular_tv_appearance.actor[Fully Relevant]tv.tv_guest_role.actor[Partially Relevant][Retrieve Entity]',\n",
       "  'score': 1.0,\n",
       "  'parent': 3,\n",
       "  'topic_entity': dict_values(['m.0x139sw']),\n",
       "  'context': 'tv.tv_actor.starring_roles[Fully Relevant]tv.regular_tv_appearance.actor[Fully Relevant]tv.tv_guest_role.actor[Partially Relevant]',\n",
       "  'processed_pred': '[Continue to Retrieve Evidence]<paragraph>tv.tv_actor.starring_roles;tv.regular_tv_appearance.actor;tv.tv_guest_role.actor;tv.tv_actor.guest_roles;base.gossipgirl.character.played_by</paragraph>tv.tv_actor.starring_roles[Fully Relevant]tv.regular_tv_appearance.actor[Fully Relevant]tv.tv_guest_role.actor[Partially Relevant]'},\n",
       " 6: {'prompt': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nwho plays ken barlow in coronation street<|eot_id|><|start_header_id|>assistant<|end_header_id|>[New Retrieval]<paragraph>tv.tv_series_season.regular_cast;tv.tv_program.regular_cast;tv.regular_tv_appearance.series;tv.regular_tv_appearance.character</paragraph>tv.tv_series_season.regular_cast[Fully Relevant]tv.tv_program.regular_cast[Fully Relevant]tv.regular_tv_appearance.character[Partially Relevant]Unknown Entity[Partially Relevant]',\n",
       "  'pred': '[Continue to Retrieve Evidence]<paragraph>tv.tv_actor.starring_roles;tv.regular_tv_appearance.actor;tv.tv_guest_role.actor;base.gossipgirl.character.played_by;tv.tv_actor.guest_roles</paragraph>tv.tv_actor.starring_roles[Fully Relevant]tv.regular_tv_appearance.actor[Fully Relevant]tv.tv_guest_role.actor[Partially Relevant][Retrieve Entity]',\n",
       "  'score': 1.0,\n",
       "  'parent': 4,\n",
       "  'topic_entity': dict_values(['m.0x0l8r_']),\n",
       "  'context': 'tv.tv_actor.starring_roles[Fully Relevant]tv.regular_tv_appearance.actor[Fully Relevant]tv.tv_guest_role.actor[Partially Relevant]',\n",
       "  'processed_pred': '[Continue to Retrieve Evidence]<paragraph>tv.tv_actor.starring_roles;tv.regular_tv_appearance.actor;tv.tv_guest_role.actor;base.gossipgirl.character.played_by;tv.tv_actor.guest_roles</paragraph>tv.tv_actor.starring_roles[Fully Relevant]tv.regular_tv_appearance.actor[Fully Relevant]tv.tv_guest_role.actor[Partially Relevant]'},\n",
       " 7: {'prompt': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nwho plays ken barlow in coronation street<|eot_id|><|start_header_id|>assistant<|end_header_id|>[New Retrieval]<paragraph>tv.tv_series_season.regular_cast;tv.tv_program.regular_cast;tv.regular_tv_appearance.series;tv.regular_tv_appearance.character</paragraph>tv.tv_series_season.regular_cast[Fully Relevant]tv.tv_program.regular_cast[Fully Relevant]tv.regular_tv_appearance.character[Partially Relevant]Unknown Entity[Partially Relevant][Continue to Retrieve Evidence]<paragraph>tv.tv_actor.starring_roles;tv.regular_tv_appearance.actor;tv.tv_guest_role.actor;tv.tv_actor.guest_roles;base.gossipgirl.character.played_by</paragraph>tv.tv_actor.starring_roles[Fully Relevant]tv.regular_tv_appearance.actor[Fully Relevant]tv.tv_guest_role.actor[Partially Relevant]',\n",
       "  'pred': 'Naomi Radcliffe[Partially Relevant][No Retrieval]Answer: Naomi Radcliffe<|eot_id|>',\n",
       "  'context': '',\n",
       "  'score': 1.0,\n",
       "  'parent': 5,\n",
       "  'topic_entity': dict_values(['m.0x139sw']),\n",
       "  'processed_pred': 'Naomi Radcliffe[Partially Relevant][No Retrieval]Answer: Naomi Radcliffe<|eot_id|>'},\n",
       " 8: {'prompt': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nwho plays ken barlow in coronation street<|eot_id|><|start_header_id|>assistant<|end_header_id|>[New Retrieval]<paragraph>tv.tv_series_season.regular_cast;tv.tv_program.regular_cast;tv.regular_tv_appearance.series;tv.regular_tv_appearance.character</paragraph>tv.tv_series_season.regular_cast[Fully Relevant]tv.tv_program.regular_cast[Fully Relevant]tv.regular_tv_appearance.character[Partially Relevant]Unknown Entity[Partially Relevant][Continue to Retrieve Evidence]<paragraph>tv.tv_actor.starring_roles;tv.regular_tv_appearance.actor;tv.tv_guest_role.actor;tv.tv_actor.guest_roles;base.gossipgirl.character.played_by</paragraph>tv.tv_actor.starring_roles[Fully Relevant]tv.regular_tv_appearance.actor[Fully Relevant]tv.tv_guest_role.actor[Partially Relevant]',\n",
       "  'pred': 'Naomi Radcliffe[Fully Relevant][No Retrieval]Answer: Naomi Radcliffe<|eot_id|>',\n",
       "  'context': '',\n",
       "  'score': 1.0,\n",
       "  'parent': 5,\n",
       "  'topic_entity': dict_values(['m.0x139sw']),\n",
       "  'processed_pred': 'Naomi Radcliffe[Fully Relevant][No Retrieval]Answer: Naomi Radcliffe<|eot_id|>'},\n",
       " 9: {'prompt': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nwho plays ken barlow in coronation street<|eot_id|><|start_header_id|>assistant<|end_header_id|>[New Retrieval]<paragraph>tv.tv_series_season.regular_cast;tv.tv_program.regular_cast;tv.regular_tv_appearance.series;tv.regular_tv_appearance.character</paragraph>tv.tv_series_season.regular_cast[Fully Relevant]tv.tv_program.regular_cast[Fully Relevant]tv.regular_tv_appearance.character[Partially Relevant]Unknown Entity[Partially Relevant][Continue to Retrieve Evidence]<paragraph>tv.tv_actor.starring_roles;tv.regular_tv_appearance.actor;tv.tv_guest_role.actor;base.gossipgirl.character.played_by;tv.tv_actor.guest_roles</paragraph>tv.tv_actor.starring_roles[Fully Relevant]tv.regular_tv_appearance.actor[Fully Relevant]tv.tv_guest_role.actor[Partially Relevant]',\n",
       "  'pred': 'Bill Ward[Fully Relevant][No Retrieval]Answer: Bill Ward<|eot_id|>',\n",
       "  'context': '',\n",
       "  'score': 1.0,\n",
       "  'parent': 6,\n",
       "  'topic_entity': dict_values(['m.0x0l8r_']),\n",
       "  'processed_pred': 'Bill Ward[Fully Relevant][No Retrieval]Answer: Bill Ward<|eot_id|>'},\n",
       " 10: {'prompt': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nwho plays ken barlow in coronation street<|eot_id|><|start_header_id|>assistant<|end_header_id|>[New Retrieval]<paragraph>tv.tv_series_season.regular_cast;tv.tv_program.regular_cast;tv.regular_tv_appearance.series;tv.regular_tv_appearance.character</paragraph>tv.tv_series_season.regular_cast[Fully Relevant]tv.tv_program.regular_cast[Fully Relevant]tv.regular_tv_appearance.character[Partially Relevant]Unknown Entity[Partially Relevant][Continue to Retrieve Evidence]<paragraph>tv.tv_actor.starring_roles;tv.regular_tv_appearance.actor;tv.tv_guest_role.actor;base.gossipgirl.character.played_by;tv.tv_actor.guest_roles</paragraph>tv.tv_actor.starring_roles[Fully Relevant]tv.regular_tv_appearance.actor[Fully Relevant]tv.tv_guest_role.actor[Partially Relevant]',\n",
       "  'pred': 'Bill Ward[Fully Relevant][No Retrieval]Answer: Bill Ward<|eot_id|>',\n",
       "  'context': '',\n",
       "  'score': 1.0,\n",
       "  'parent': 6,\n",
       "  'topic_entity': dict_values(['m.0x0l8r_']),\n",
       "  'processed_pred': 'Bill Ward[Fully Relevant][No Retrieval]Answer: Bill Ward<|eot_id|>'}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'m.0wk9sqk': 'm.0wk9sqk',\n",
       " 'm.0x12w15': 'm.0x12w15',\n",
       " 'm.0bnjt4f': 'm.0bnjt4f',\n",
       " 'm.0bvz0wn': 'm.0bvz0wn',\n",
       " 'm.0mtj7y1': 'm.0mtj7y1',\n",
       " 'm.0wkph1b': 'm.0wkph1b',\n",
       " 'm.0z3vtml': 'm.0z3vtml',\n",
       " 'm.0q54wmv': 'm.0q54wmv',\n",
       " 'm.0x00v8j': 'm.0x00v8j',\n",
       " 'm.0bmmtrm': 'm.0bmmtrm',\n",
       " 'm.0r5_vr1': 'm.0r5_vr1',\n",
       " 'm.0nh9b29': 'm.0nh9b29',\n",
       " 'm.0tm618z': 'm.0tm618z',\n",
       " 'm.0tm61pg': 'm.0tm61pg',\n",
       " 'm.0bvyb5k': 'm.0bvyb5k',\n",
       " 'm.0x03fqc': 'm.0x03fqc',\n",
       " 'm.0x01l7p': 'm.0x01l7p',\n",
       " 'm.0w_xg_v': 'm.0w_xg_v',\n",
       " 'm.0khhm3s': 'm.0khhm3s',\n",
       " 'm.0nh9b1y': 'm.0nh9b1y',\n",
       " 'm.0x0g7b2': 'm.0x0g7b2',\n",
       " 'm.0krnwbm': 'm.0krnwbm',\n",
       " 'm.0x18h4t': 'm.0x18h4t',\n",
       " 'm.0wk6_z0': 'm.0wk6_z0',\n",
       " 'm.0bw1lcr': 'm.0bw1lcr',\n",
       " 'm.0x0s1qc': 'm.0x0s1qc',\n",
       " 'm.0wkd82f': 'm.0wkd82f',\n",
       " 'm.0bvxqfy': 'm.0bvxqfy',\n",
       " 'm.0x0x0zk': 'm.0x0x0zk',\n",
       " 'm.010vy8pb': 'm.010vy8pb',\n",
       " 'm.0w_hmd9': 'm.0w_hmd9',\n",
       " 'm.0w2tqh3': 'm.0w2tqh3',\n",
       " 'm.0dk_sc3': 'm.0dk_sc3',\n",
       " 'm.0dk_w2b': 'm.0dk_w2b',\n",
       " 'm.0wlpq02': 'm.0wlpq02',\n",
       " 'm.0mtjdyy': 'm.0mtjdyy',\n",
       " 'm.0w_hlz4': 'm.0w_hlz4',\n",
       " 'm.0wl1lm0': 'm.0wl1lm0',\n",
       " 'm.0wlvlyc': 'm.0wlvlyc',\n",
       " 'm.010dqk3_': 'm.010dqk3_',\n",
       " 'm.0q54wn8': 'm.0q54wn8',\n",
       " 'm.0wlt9sz': 'm.0wlt9sz',\n",
       " 'm.0bngkjc': 'm.0bngkjc',\n",
       " 'm.0nh99s3': 'm.0nh99s3',\n",
       " 'm.011201wm': 'm.011201wm',\n",
       " 'm.0whxg6v': 'm.0whxg6v',\n",
       " 'm.0z4425r': 'm.0z4425r',\n",
       " 'm.0wysr6t': 'm.0wysr6t',\n",
       " 'm.0wj60mn': 'm.0wj60mn',\n",
       " 'm.0z43zrc': 'm.0z43zrc',\n",
       " 'm.0bmnfyw': 'm.0bmnfyw',\n",
       " 'm.0bngy_q': 'm.0bngy_q',\n",
       " 'm.0dl1d63': 'm.0dl1d63',\n",
       " 'm.0y_v4cv': 'm.0y_v4cv',\n",
       " 'm.0wkdl6m': 'm.0wkdl6m',\n",
       " 'm.0_hq072': 'm.0_hq072',\n",
       " 'm.0w_l_ym': 'm.0w_l_ym',\n",
       " 'm.0wlt3vh': 'm.0wlt3vh',\n",
       " 'm.0bnj82f': 'm.0bnj82f',\n",
       " 'm.0x0m4k4': 'm.0x0m4k4',\n",
       " 'm.05lgy3b': 'm.05lgy3b',\n",
       " 'm.0wk93zz': 'm.0wk93zz',\n",
       " 'm.0wlxzxz': 'm.0wlxzxz',\n",
       " 'm.0khhm4t': 'm.0khhm4t',\n",
       " 'm.0whz2c8': 'm.0whz2c8',\n",
       " 'm.0bvy1td': 'm.0bvy1td',\n",
       " 'm.0bvv6xf': 'm.0bvv6xf',\n",
       " 'm.0bvv2dt': 'm.0bvv2dt',\n",
       " 'm.0w_gn9p': 'm.0w_gn9p',\n",
       " 'm.0bvvdsv': 'm.0bvvdsv',\n",
       " 'm.0q54wmd': 'm.0q54wmd',\n",
       " 'm.010f1wbt': 'm.010f1wbt',\n",
       " 'm.05v289v': 'm.05v289v',\n",
       " 'm.0bmnft1': 'm.0bmnft1',\n",
       " 'm.0bvv19r': 'm.0bvv19r',\n",
       " 'm.0bvzbhx': 'm.0bvzbhx',\n",
       " 'm.0khhm3m': 'm.0khhm3m',\n",
       " 'm.0khhm3z': 'm.0khhm3z',\n",
       " 'm.0khhm43': 'm.0khhm43',\n",
       " 'm.0khhm48': 'm.0khhm48',\n",
       " 'm.0khhm4g': 'm.0khhm4g',\n",
       " 'm.0khhm4z': 'm.0khhm4z',\n",
       " 'm.0khhm53': 'm.0khhm53',\n",
       " 'm.0khhm58': 'm.0khhm58',\n",
       " 'm.0n54w3y': 'm.0n54w3y',\n",
       " 'm.0qzj336': 'm.0qzj336',\n",
       " 'm.0w_hmgq': 'm.0w_hmgq',\n",
       " 'm.0w_hnpn': 'm.0w_hnpn',\n",
       " 'm.0w_m39y': 'm.0w_m39y',\n",
       " 'm.0w_sns3': 'm.0w_sns3',\n",
       " 'm.0wg6jd_': 'm.0wg6jd_',\n",
       " 'm.0wg6jrs': 'm.0wg6jrs',\n",
       " 'm.0wh0wvl': 'm.0wh0wvl',\n",
       " 'm.0wkw5jr': 'm.0wkw5jr',\n",
       " 'm.0wlc9cr': 'm.0wlc9cr',\n",
       " 'm.0wlmm02': 'm.0wlmm02',\n",
       " 'm.0x012h5': 'm.0x012h5',\n",
       " 'm.0x0188s': 'm.0x0188s',\n",
       " 'm.0x0f6yw': 'm.0x0f6yw',\n",
       " 'm.0x0m220': 'm.0x0m220',\n",
       " 'm.0x139sw': 'm.0x139sw',\n",
       " 'm.0x14syw': 'm.0x14syw',\n",
       " 'm.0x18dwj': 'm.0x18dwj',\n",
       " 'm.0x18fhd': 'm.0x18fhd',\n",
       " 'm.0y64m9f': 'm.0y64m9f',\n",
       " 'm.0z4j1x8': 'm.0z4j1x8',\n",
       " 'm.0x0l8r_': 'm.0x0l8r_',\n",
       " 'm.0wkkgmt': 'm.0wkkgmt'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_another_entity('m.01_2n', 'tv.tv_program.regular_cast')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Governor of Tennessee': 'm.04x_n9q'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_another_entity('m.04j5sk8', 'government.government_position_held.office_position_or_title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_eval_metric(best_pred, preds, answers):\n",
    "    correct, total = 0.0, 0.0\n",
    "    for entity in preds:\n",
    "        if entity in answers:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "    if len(answers) == 0:\n",
    "        if total == 0:\n",
    "            return 1.0, 1.0, 1.0, 1.0 # precision, recall, f1, hits\n",
    "        else:\n",
    "            return 0.0, 1.0, 0.0, 1.0 # precision, recall, f1, hits\n",
    "    else:\n",
    "        hits = float(best_pred in answers)\n",
    "        if total == 0:\n",
    "            return 1.0, 0.0, 0.0, hits # precision, recall, f1, hits\n",
    "        else:\n",
    "            precision, recall = correct / total, correct / len(answers)\n",
    "            f1 = 2.0 / (1.0 / precision + 1.0 / recall) if precision != 0 and recall != 0 else 0.0\n",
    "            return precision, recall, f1, hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 1\n",
    "answers = [get_label(ans) for ans in test_data[index]['answer']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['United States Representative',\n",
       " 'Governor of Tennessee',\n",
       " 'Speaker of the United States House of Representatives']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = ['Governor of Tennessee']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_eval_metric(preds, answers):\n",
    "    correct, total = 0.0, 0.0\n",
    "    for entity in preds:\n",
    "        if entity in answers:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "    # if len(answers) == 0:\n",
    "    #     if total == 0:\n",
    "    #         return 1.0, 1.0, 1.0, 1.0 # precision, recall, f1, hits\n",
    "    #     else:\n",
    "    #         return 0.0, 1.0, 0.0, 1.0 # precision, recall, f1, hits\n",
    "    # else:\n",
    "    if total != 0:\n",
    "        hits = 1\n",
    "    else: \n",
    "        hits = 0\n",
    "    if total == 0:\n",
    "        return 1.0, 0.0, 0.0, hits # precision, recall, f1, hits\n",
    "    else:\n",
    "        precision, recall = correct / total, correct / len(answers)\n",
    "        f1 = 2.0 / (1.0 / precision + 1.0 / recall) if precision != 0 and recall != 0 else 0.0\n",
    "        return precision, recall, f1, hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 0.3333333333333333, 0.5, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cal_eval_metric(preds, answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'FlagEmbedding'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mFlagEmbedding\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BGEM3FlagModel\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m BGEM3FlagModel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBAAI/bge-m3\u001b[39m\u001b[38;5;124m'\u001b[39m,  use_fp16\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;66;03m# Setting use_fp16 to True speeds up computation with a slight performance degradation\u001b[39;00m\n\u001b[1;32m      5\u001b[0m sentences_1 \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is BGE M3?\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDefination of BM25\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'FlagEmbedding'"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "self-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
