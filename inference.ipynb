{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_972831/2109291646.py:13: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n",
      "/media/disk1/chatgpt/miniconda3/envs/self-rag/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain.embeddings import (\n",
    "    HuggingFaceEmbeddings,\n",
    ")\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"2b219db0d2984f9dae28b651ab8ab3d9\"\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://smsh.openai.azure.com/\"\n",
    "os.environ[\"AZURE_OPENAI_API_VERSION\"] = \"2024-03-01-preview\"\n",
    "# embeddings = AzureOpenAIEmbeddings(\n",
    "#     model=\"text-embedding-3-small\",\n",
    "# )\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "                model_name=\"BAAI/bge-large-en-v1.5\",\n",
    "                # model_kwargs={'device': 'cpu'},\n",
    "                encode_kwargs={'normalize_embeddings': False},\n",
    "            )\n",
    "from langchain.storage import LocalFileStore, RedisStore\n",
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "store = RedisStore(redis_url=\"redis://localhost:6379\")\n",
    "cached_embedder = CacheBackedEmbeddings.from_bytes_store(\n",
    "embeddings, store, namespace=\"bge-large\"\n",
    ")\n",
    "row_string = []\n",
    "with open('./data/clean_relations', 'r') as f:\n",
    "    r_data = [line.strip() for line in f]\n",
    "all_db = FAISS.from_texts(r_data, cached_embedder)\n",
    "# retriever = db.as_retriever(search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run_long_form answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 01-27 02:05:05 cuda.py:22] You are using a deprecated `pynvml` package. Please install `nvidia-ml-py` instead, and make sure to uninstall `pynvml`. When both of them are installed, `pynvml` will take precedence and cause errors. See https://pypi.org/project/pynvml for more information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-27 02:05:05,677\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-27 02:05:09 config.py:905] Defaulting to use mp for distributed inference\n",
      "INFO 01-27 02:05:09 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='/media/disk2/llama_factory/generation_0122_no_mask', speculative_config=None, tokenizer='/media/disk2/llama_factory/generation_0122_no_mask', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/media/disk2/llama_factory/generation_0122_no_mask, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "WARNING 01-27 02:05:10 multiproc_gpu_executor.py:127] CUDA was previously initialized. We must use the `spawn` multiprocessing start method. Setting VLLM_WORKER_MULTIPROC_METHOD to 'spawn'.\n",
      "WARNING 01-27 02:05:10 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 01-27 02:05:10 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "WARNING 01-27 02:05:11 cuda.py:22] You are using a deprecated `pynvml` package. Please install `nvidia-ml-py` instead, and make sure to uninstall `pynvml`. When both of them are installed, `pynvml` will take precedence and cause errors. See https://pypi.org/project/pynvml for more information.\n",
      "WARNING 01-27 02:05:11 cuda.py:22] You are using a deprecated `pynvml` package. Please install `nvidia-ml-py` instead, and make sure to uninstall `pynvml`. When both of them are installed, `pynvml` will take precedence and cause errors. See https://pypi.org/project/pynvml for more information.\n",
      "WARNING 01-27 02:05:11 cuda.py:22] You are using a deprecated `pynvml` package. Please install `nvidia-ml-py` instead, and make sure to uninstall `pynvml`. When both of them are installed, `pynvml` will take precedence and cause errors. See https://pypi.org/project/pynvml for more information.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=980329)\u001b[0;0m INFO 01-27 02:05:13 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=980330)\u001b[0;0m INFO 01-27 02:05:13 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=980331)\u001b[0;0m INFO 01-27 02:05:13 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=980331)\u001b[0;0m INFO 01-27 02:05:14 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "INFO 01-27 02:05:14 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=980331)\u001b[0;0m INFO 01-27 02:05:14 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=980330)\u001b[0;0m INFO 01-27 02:05:14 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 01-27 02:05:14 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=980329)\u001b[0;0m INFO 01-27 02:05:14 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=980329)\u001b[0;0m INFO 01-27 02:05:14 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=980330)\u001b[0;0m INFO 01-27 02:05:14 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=980331)\u001b[0;0m WARNING 01-27 02:05:15 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=980330)\u001b[0;0m WARNING 01-27 02:05:15 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 01-27 02:05:15 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=980329)\u001b[0;0m WARNING 01-27 02:05:15 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "INFO 01-27 02:05:15 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7f580c666530>, local_subscribe_port=58077, remote_subscribe_port=None)\n",
      "INFO 01-27 02:05:15 model_runner.py:1056] Starting to load model /media/disk2/llama_factory/generation_0122_no_mask...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=980329)\u001b[0;0m INFO 01-27 02:05:15 model_runner.py:1056] Starting to load model /media/disk2/llama_factory/generation_0122_no_mask...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=980331)\u001b[0;0m INFO 01-27 02:05:15 model_runner.py:1056] Starting to load model /media/disk2/llama_factory/generation_0122_no_mask...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=980330)\u001b[0;0m INFO 01-27 02:05:15 model_runner.py:1056] Starting to load model /media/disk2/llama_factory/generation_0122_no_mask...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:00<00:00,  4.91it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:00<00:00,  3.34it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:01<00:00,  2.78it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:01<00:00,  3.07it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=980330)\u001b[0;0m INFO 01-27 02:05:16 model_runner.py:1067] Loading model weights took 3.7420 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=980331)\u001b[0;0m INFO 01-27 02:05:16 model_runner.py:1067] Loading model weights took 3.7420 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=980329)\u001b[0;0m INFO 01-27 02:05:16 model_runner.py:1067] Loading model weights took 3.7420 GB\n",
      "INFO 01-27 02:05:16 model_runner.py:1067] Loading model weights took 3.7420 GB\n",
      "INFO 01-27 02:05:18 distributed_gpu_executor.py:57] # GPU blocks: 61347, # CPU blocks: 8192\n",
      "INFO 01-27 02:05:18 distributed_gpu_executor.py:61] Maximum concurrency for 8192 tokens per request: 119.82x\n",
      "\u001b[1;36m(VllmWorkerProcess pid=980331)\u001b[0;0m INFO 01-27 02:05:20 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=980331)\u001b[0;0m INFO 01-27 02:05:20 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=980330)\u001b[0;0m INFO 01-27 02:05:20 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=980330)\u001b[0;0m INFO 01-27 02:05:20 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=980329)\u001b[0;0m INFO 01-27 02:05:20 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=980329)\u001b[0;0m INFO 01-27 02:05:20 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 01-27 02:05:20 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 01-27 02:05:20 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=980329)\u001b[0;0m INFO 01-27 02:05:36 model_runner.py:1523] Graph capturing finished in 16 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=980330)\u001b[0;0m INFO 01-27 02:05:36 model_runner.py:1523] Graph capturing finished in 16 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=980331)\u001b[0;0m INFO 01-27 02:05:36 model_runner.py:1523] Graph capturing finished in 16 secs.\n",
      "INFO 01-27 02:05:36 model_runner.py:1523] Graph capturing finished in 16 secs.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "# model = LLM(model='/media/disk2/llama_factory/generation_1124_special', trust_remote_code=True, tensor_parallel_size=4)\n",
    "# model = LLM(model='/media/disk2/llama_factory/generation_1209_no_mask', trust_remote_code=True, tensor_parallel_size=1)\n",
    "# model = LLM(model='/media/disk1/chatgpt/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B/snapshots/8cde5ca8380496c9a6cc7ef3a8b46a0372a1d920', trust_remote_code=True, tensor_parallel_size=1)\n",
    "\n",
    "model = LLM(model='/media/disk2/llama_factory/generation_0122_no_mask', trust_remote_code=True, tensor_parallel_size=1)\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('/media/disk2/llama_factory/generation_0122_no_mask')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.05s/it, est. speed input: 16.22 toks/s, output: 95.44 toks/s]\n"
     ]
    }
   ],
   "source": [
    "#是否带special token 分开传入，用于检索和生成\n",
    "sampling_params = SamplingParams(\n",
    "            temperature=0.0, top_p=1.0,max_tokens=100, logprobs=5, skip_special_tokens=False, include_stop_str_in_output=True)\n",
    "PROMPT_DICT = {\"llama3\": '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n{input}<|eot_id|><|start_header_id|>assistant<|end_header_id|>'}\n",
    "test = model.generate([PROMPT_DICT[\"llama3\"].format(input=\"what all does google now do?\")], sampling_params)[0].outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_special_tokens(tokenizer, use_grounding=False, use_utility=False):\n",
    "    rel_tokens = {}\n",
    "    for token in ['[Unrelevant]','[Partially Relevant]','[Fully Relevant]']:\n",
    "        rel_tokens[token] = tokenizer.convert_tokens_to_ids(token)\n",
    "    reason_tokens = {}\n",
    "    for token in ['[Fully Reasonable]', '[Partially Reasonable]', '[Unreasonable]']:\n",
    "        reason_tokens[token] = tokenizer.convert_tokens_to_ids(token)\n",
    "    # ut_tokens = None\n",
    "    # if use_utility is True:\n",
    "    #     ut_tokens = {}\n",
    "    #     for token in utility_tokens_names:\n",
    "    #         ut_tokens[token] = tokenizer.convert_tokens_to_ids(token)\n",
    "\n",
    "    return rel_tokens, reason_tokens\n",
    "rel_tokens, reason_tokens = load_special_tokens(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "with open('/media/disk1/chatgpt/zh/ToG/data/graliqa.json', 'r',encoding='utf-8') as f:\n",
    "    test_data = json.load(f)\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1639\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('./data/merged/WebQSP_test.json', 'r',encoding='utf-8') as f:\n",
    "    test_data = json.load(f)\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3531\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('./data/merged/CWQ_test.json', 'r', encoding='utf-8') as f:\n",
    "    test_data = json.load(f)\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import random\n",
    "def save_to_json(data: List, data_path=''):\n",
    "    if not os.path.isfile(data_path):\n",
    "        # 文件不存在，创建新列表并写入文件\n",
    "        with open(data_path, 'w', encoding='utf-8') as file:\n",
    "            json.dump(data, file, ensure_ascii=False, indent=4)\n",
    "        return\n",
    "    try:\n",
    "        # 尝试读取现有文件\n",
    "        with open(data_path, 'r', encoding='utf-8') as file:\n",
    "            # 加载现有的JSON数据\n",
    "            existing_data = json.load(file)\n",
    "            existing_data.extend(data)\n",
    "    except json.JSONDecodeError:\n",
    "        # 文件不是有效的JSON，打印错误信息并退出\n",
    "        print(f\"文件 {data_path} 不是有效的JSON格式。\")\n",
    "        return\n",
    "    except ValueError as e:\n",
    "        # 打印错误信息并退出\n",
    "        print(e)\n",
    "        return\n",
    "    # 将更新后的数据写回文件\n",
    "    with open(data_path, 'w', encoding='utf-8') as file:\n",
    "        json.dump(existing_data, file, ensure_ascii=False, indent=4)\n",
    "def random_sample(lst, k=3):\n",
    "    try:\n",
    "        # 尝试从列表中随机抽取k个不重复的元素\n",
    "        return random.sample(lst, k)\n",
    "    except ValueError:\n",
    "        # 如果列表长度小于k，返回整个列表\n",
    "        return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from src.sparql_utils import *\n",
    "import random\n",
    "#维护一个relation 池\n",
    "def run_relation_generation_batch(model, prompt, new_retrieval, context, topic_entity, hypo=True, use_1hop=True, income=None):\n",
    "    # 初始化关系得分字典、最终预测列表、总体得分列表、最终上下文列表、段落集合\n",
    "    rel_score_dict = {}\n",
    "    final_preds = []\n",
    "    overall_scores = []\n",
    "    final_contexts = []\n",
    "    paragraph = set()\n",
    "    # 如果是新检索，则设置检索标记为\"[Retrieve Relation]\"，否则设置为\"[Retrieve Relation]\"\n",
    "    if new_retrieval:\n",
    "        retrieval_token = \"[Retrieve Relation]\"\n",
    "    else:\n",
    "        retrieval_token = \"[Retrieve Relation]\"\n",
    "    # 如果使用1跳关系，则获取1跳关系，否则使用所有关系\n",
    "    if use_1hop:\n",
    "        candidate_relations = []\n",
    "        for entity in topic_entity:\n",
    "            try:\n",
    "                candidate_relations.extend(get_1hop_relations_with_odbc(entity))\n",
    "            except:\n",
    "                continue\n",
    "        if len(candidate_relations):\n",
    "            vec_db = FAISS.from_texts(candidate_relations, cached_embedder)\n",
    "        else:\n",
    "            vec_db = all_db\n",
    "        retriever = vec_db.as_retriever(search_kwargs={\"k\": 5})\n",
    "    else:\n",
    "        retriever = all_db.as_retriever(search_kwargs={\"k\": 5})\n",
    "    # 获取段落内容\n",
    "    paragraph.update([page.page_content.strip() for page in retriever.invoke(prompt.split('\\n\\n')[1].split('<|eot_id|>')[0] + ' '+ context)])\n",
    "    # 如果使用假设，则生成假设关系\n",
    "    if hypo:\n",
    "        hypo_rel = model.generate(prompt + retrieval_token, sampling_params)[0].outputs[0].text\n",
    "        pattern = r'(\\w+\\.\\w+\\.\\w+)\\[(.*?)\\]'\n",
    "        if '[Retrieve Entity]' in hypo_rel:\n",
    "            hypo_rel = hypo_rel.split('[Retrieve Entity]')[0]\n",
    "        matches =  dict(re.findall(pattern, hypo_rel))\n",
    "        string = ''\n",
    "        for k,v in matches.items():\n",
    "            if v in ['Fully Relevant', 'Partially Relevant']:\n",
    "                string += k + ' '\n",
    "        paragraph.update([page.page_content.strip() for page in retriever.invoke(string)])\n",
    "    paragraph.discard(income)\n",
    "    aug_prompts =  [\"<paragraph>{}</paragraph>\".format(';'.join(p))  for p in [list(paragraph)[i: i+5] for i in range(0, len(paragraph), 5)]]\n",
    "    \n",
    "    preds = model.generate([prompt + retrieval_token + aug for aug in aug_prompts], sampling_params)\n",
    "    for p_id, pred in enumerate(preds):\n",
    "        pred_token_ids = pred.outputs[0].token_ids\n",
    "        pred_text_1 = pred.outputs[0].text\n",
    "        pred_log_probs = pred.outputs[0].logprobs\n",
    "        seq_score = pred.outputs[0].cumulative_logprob / \\\n",
    "            max(len(pred.outputs[0].token_ids), 1)\n",
    "        relevance_indices = []\n",
    "        for tok_idx, tok in enumerate(pred_token_ids):\n",
    "            if tok in rel_tokens.values():\n",
    "                relevance_indices.append(tok_idx)\n",
    "        if len(relevance_indices) > 0:\n",
    "            for idx in relevance_indices:\n",
    "                for token, token_id in rel_tokens.items():\n",
    "                    prob = pred_log_probs[idx][token_id].logprob if token_id in pred_log_probs[idx] else -100\n",
    "                    rel_score_dict[token] = np.exp(prob)\n",
    "        relevance_score = rel_score_dict['[Fully Relevant]']+ rel_score_dict['[Partially Relevant]'] / np.sum(list(rel_score_dict.values()))\n",
    "        if '[Retrieve Entity]' in pred_text_1:\n",
    "            processed_pred = pred_text_1.split('[Retrieve Entity]')[0] + '[Retrieve Entity]'\n",
    "        else:\n",
    "            processed_pred = pred_text_1\n",
    "        final_preds.append(retrieval_token + aug_prompts[p_id] + processed_pred)\n",
    "        overall_scores.append(0)\n",
    "        final_contexts.append(pred_text_1.split('[Retrieve Entity]')[0])\n",
    "\n",
    "    return final_preds, overall_scores, final_contexts\n",
    "\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def run_entity_generation_batch(model, prompt, topic_entities, context):\n",
    "    # 初始化变量\n",
    "    final_predictions, final_entities, final_contexts = [], [], []\n",
    "    entity_prompts, income_rel = [], []\n",
    "    overall_scores, name_to_id = {}, {}\n",
    "    effective_count = 0\n",
    "\n",
    "    # 提取上下文中的实体和相关性匹配\n",
    "    context_pattern = r'(.+?)\\[(.*?)\\]'\n",
    "    context_matches = dict(re.findall(context_pattern, context))\n",
    "\n",
    "    # 构建实体提示\n",
    "    for entity in topic_entities:\n",
    "        for key, relevance in context_matches.items():\n",
    "            if relevance in ['Fully Relevant', 'Partially Relevant']:\n",
    "                try:\n",
    "                    related_entities = get_another_entity(entity, key, return_label=True)\n",
    "                except Exception:\n",
    "                    related_entities = {}\n",
    "\n",
    "                if related_entities:\n",
    "                    # 初始化分数和实体映射\n",
    "                    income_rel.append(key)\n",
    "                    name_to_id[effective_count] = related_entities\n",
    "                    overall_scores[effective_count] = {\n",
    "                        'r_relevance': 1.0 if relevance == 'Fully Relevant' else 0.5\n",
    "                    }\n",
    "                    # 构造实体提示\n",
    "                    entities = [\n",
    "                        f'({get_label(entity)}, {key}, {rel_entity})'\n",
    "                        for rel_entity in related_entities.keys()\n",
    "                    ]\n",
    "                    overall_scores[effective_count]['path'] = entities[:5]\n",
    "                    entity_prompts.append(\n",
    "                        f\"<paragraph>{';'.join(entities[:5])}</paragraph>\"\n",
    "                    )\n",
    "                    effective_count += 1\n",
    "\n",
    "    # 生成模型预测\n",
    "    augmented_prompts = [\n",
    "        f\"{prompt}[Retrieve Entity]{entity_prompt}\"\n",
    "        for entity_prompt in entity_prompts\n",
    "    ]\n",
    "    predictions = model.generate(augmented_prompts, sampling_params)\n",
    "\n",
    "    # 解析预测结果\n",
    "    for idx, prediction in enumerate(predictions):\n",
    "        pred_output = prediction.outputs[0]\n",
    "        pred_text = pred_output.text\n",
    "        pred_log_probs = pred_output.logprobs\n",
    "        pred_token_ids = pred_output.token_ids\n",
    "        seq_score = pred_output.cumulative_logprob / max(len(pred_token_ids), 1)\n",
    "\n",
    "        processed_pred, rel_score, reason_score, return_entities, matches = process_prediction(\n",
    "            idx, pred_text, context_pattern, name_to_id, overall_scores\n",
    "        )\n",
    "\n",
    "        # 更新分数和最终结果\n",
    "        overall_scores[idx].update({\n",
    "            \"r_match\": {income_rel[idx]: context_matches[income_rel[idx]]},\n",
    "            \"e_match\": matches,\n",
    "            \"reason_score\": reason_score,\n",
    "            \"final_score\": np.exp(rel_score) * np.exp(reason_score) * np.exp(overall_scores[idx]['r_relevance'])\n",
    "        })\n",
    "\n",
    "        final_predictions.append(f\"[Retrieve Entity]{entity_prompts[idx]}{processed_pred}\")\n",
    "        final_entities.append(list(return_entities.values()))\n",
    "        final_contexts.append(' '.join(return_entities.keys()))\n",
    "\n",
    "    # 返回最终结果\n",
    "    return final_predictions, [\n",
    "        overall_scores[idx]['final_score'] for idx in overall_scores\n",
    "    ], final_entities, final_contexts, overall_scores, income_rel\n",
    "\n",
    "\n",
    "def process_prediction(idx, pred_text, pattern, name_to_id, overall_scores):\n",
    "    \"\"\"处理单条预测结果，计算相关性分数和返回实体。\"\"\"\n",
    "    rel_score = 0\n",
    "    reason_score = 0\n",
    "    return_entities = {}\n",
    "    count = 0\n",
    "\n",
    "    if '[Retrieve Relation]' in pred_text:\n",
    "        processed_pred = pred_text.split('[Retrieve Relation]')[0]\n",
    "        matches = dict(re.findall(pattern, processed_pred))\n",
    "        for key, relevance in matches.items():\n",
    "            if relevance in ['Fully Relevant', 'Partially Relevant']:\n",
    "                if key in name_to_id[idx]:\n",
    "                    return_entities[key] = name_to_id[idx][key]\n",
    "                elif key == 'Unknown Entity':\n",
    "                    random_key = random.choice(list(name_to_id[idx].keys()))\n",
    "                    return_entities[random_key] = name_to_id[idx][random_key]\n",
    "                # 更新相关性分数\n",
    "                rel_score = ((1 if relevance == 'Fully Relevant' else 0.5) +\n",
    "                             count * rel_score) / (count + 1)\n",
    "                count += 1\n",
    "        processed_pred += '[Retrieve Relation]'\n",
    "\n",
    "    elif '[No Retrieval]' in pred_text:\n",
    "        processed_pred = pred_text\n",
    "        matches = dict(re.findall(pattern, pred_text.split('[No Retrieval]')[0]))\n",
    "        for key, relevance in matches.items():\n",
    "            if relevance in ['Fully Relevant', 'Partially Relevant']:\n",
    "                rel_score = ((1 if relevance == 'Fully Relevant' else 0.5) +\n",
    "                             count * rel_score) / (count + 1)\n",
    "                count += 1\n",
    "    else:\n",
    "        matches = dict()\n",
    "        processed_pred = '[No Retrieval]Answer: None'\n",
    "        rel_score = 0\n",
    "\n",
    "    # 处理理由分数\n",
    "    if '[Fully Reasonable]' in processed_pred:\n",
    "        reason_score += 1\n",
    "    elif '[Partially Reasonable]' in processed_pred:\n",
    "        reason_score += 0.5\n",
    "\n",
    "    return processed_pred, rel_score, reason_score, return_entities, matches\n",
    " \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.10s/it, est. speed input: 20.08 toks/s, output: 91.29 toks/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00,  2.01it/s, est. speed input: 88.56 toks/s, output: 201.26 toks/s]\n",
      "Processed prompts: 100%|██████████| 5/5 [00:01<00:00,  4.33it/s, est. speed input: 623.54 toks/s, output: 168.24 toks/s]\n"
     ]
    }
   ],
   "source": [
    "# add reasonable score\n",
    "\n",
    "count = 0\n",
    "correct_ids = []\n",
    "logging_res = []\n",
    "for index in range(200, len(test_data)):\n",
    "    hit = 0\n",
    "    print(f'Process {index}')\n",
    "    data_input = test_data[index]['question']\n",
    "    prompt = PROMPT_DICT['llama3'].format(input= data_input)\n",
    "    max_depth = 5\n",
    "    # topic_entity = list(test_data[index]['topic_entity'].keys())\n",
    "    topic_entity = list(test_data[index]['gold_entity_map'].keys())\n",
    "    curr_depth = 1\n",
    "    node_id = 0\n",
    "    prediction_tree = {\n",
    "        node_id: {\n",
    "            \"prompt\": prompt,\n",
    "            \"pred\": \"[Retrieve Relation]\",\n",
    "            \"processed_pred\": \"\",\n",
    "            \"score\": None,\n",
    "            \"topic_entity\": topic_entity,\n",
    "            \"parent\": None,\n",
    "            \"context\": \"\",\n",
    "            \"income\": \"\"\n",
    "        }\n",
    "    }\n",
    "    levels = {0: [node_id]}\n",
    "    while curr_depth < max_depth:\n",
    "        levels[curr_depth] = []\n",
    "        if curr_depth-1 in levels:\n",
    "            for node in levels[curr_depth-1]:\n",
    "                curr_pred = prediction_tree[node][\"pred\"]\n",
    "                if \"<|eot_id|>\" in curr_pred or 'No Retrieval' in curr_pred:\n",
    "                    continue\n",
    "                cur_prompt = prediction_tree[node][\"prompt\"] + prediction_tree[node][\"processed_pred\"]\n",
    "                score = prediction_tree[node][\"score\"] or 0\n",
    "                topic_entity = prediction_tree[node][\"topic_entity\"]\n",
    "                context = prediction_tree[node][\"context\"]\n",
    "                income_rel = prediction_tree[node][\"income\"]\n",
    "                if \"Retrieve Entity\" in curr_pred.split('[')[-1]:\n",
    "                    retrieval_results = {}\n",
    "                    preds, scores, next_entities, contexts, overall_scores, income_rel = run_entity_generation_batch(\n",
    "                        model, cur_prompt, topic_entity, context)\n",
    "                    for i, (pred, p_score,next_topic, context, rel) in enumerate(zip(preds, scores, next_entities, contexts,  income_rel)):\n",
    "                        retrieval_results[i] = {\n",
    "                            \"pred\": pred, \"score\": p_score, \"next_topic\": next_topic, \"context\": context, \"income\": rel, \"verbose\": {\"path\": overall_scores[i]['path'], \"e_match\": overall_scores[i]['e_match'], \"r_match\": overall_scores[i]['r_match'], 'reason_score': overall_scores[i]['reason_score']}}\n",
    "                    for i, result in retrieval_results.items():\n",
    "                        node_id += 1\n",
    "                        node_score = (result[\"score\"] + score) / (curr_depth // 2)\n",
    "                        pred = result[\"pred\"]\n",
    "                        next_entity = result['next_topic']\n",
    "                        if len(next_entity) == 0:\n",
    "                            next_entity = topic_entity\n",
    "                        prediction_tree[node_id] = {\"prompt\": cur_prompt, \"pred\": pred, \"context\": result['context'],\n",
    "                                                    \"score\": node_score, \"parent\": node,\n",
    "                                                    \"topic_entity\": next_entity, \"income\": result['income'], \"verbose\": result['verbose']}\n",
    "                        if \"[Continue to Retrieve Evidence]\" in pred:\n",
    "                            gen_result_index = pred.index(\"[Continue to Retrieve Evidence]\")\n",
    "                            prev_generation = pred[:gen_result_index]\n",
    "                        else:\n",
    "                            prev_generation = pred\n",
    "                        prediction_tree[node_id][\"processed_pred\"] = prev_generation\n",
    "                        levels[curr_depth].append(node_id)\n",
    "                #存在前后逻辑粘连   \n",
    "                if \"Retrieve Relation\" in curr_pred.split('[')[-1]:\n",
    "                    retrieval_results = {}\n",
    "                    preds, scores, contexts = run_relation_generation_batch(\n",
    "                        model, cur_prompt, new_retrieval=True if (\"[New Retrieval]\" in curr_pred) else False, context=context, topic_entity=topic_entity, hypo=True, income=income_rel)\n",
    "                    for i, (pred, p_score, context) in enumerate(zip(preds, scores, contexts)):\n",
    "                        retrieval_results[i] = {\n",
    "                            \"pred\": pred, \"score\": p_score, \"context\": context}\n",
    "\n",
    "                    for i, result in retrieval_results.items():\n",
    "                        node_id += 1\n",
    "                        #计算score\n",
    "                        node_score = result[\"score\"] + score if score is not None else result[\"score\"]\n",
    "                        # node_score = result['score']\n",
    "                        pred = result[\"pred\"]\n",
    "                        context = result[\"context\"]\n",
    "                        prediction_tree[node_id] = {\"prompt\": cur_prompt, \"pred\": pred,\n",
    "                                                    \"score\": node_score, \"parent\": node,\n",
    "                                                \"topic_entity\": topic_entity, \"context\": context, 'income': income_rel}\n",
    "                        if \"[Retrieve Entity]\" in pred:\n",
    "                            gen_result_index = pred.index(\"[Retrieve Entity]\")\n",
    "                            prev_generation = pred[:gen_result_index]\n",
    "                        else:\n",
    "                            prev_generation = pred\n",
    "                        prediction_tree[node_id][\"processed_pred\"] = prev_generation\n",
    "                        levels[curr_depth].append(node_id)\n",
    "            current_rank = levels[curr_depth]\n",
    "            node2score = {\n",
    "                node_id: prediction_tree[node_id][\"score\"] for node_id in current_rank}\n",
    "            top_nodes = sorted(node2score.items(), key=lambda x: x[1], reverse=True)[\n",
    "                :3]\n",
    "            levels[curr_depth] = [node[0] for node in top_nodes]\n",
    "            curr_depth += 1\n",
    "        else:\n",
    "            break\n",
    "    labels = [get_label(ans) if ans.startswith('m.') else ans for ans in test_data[index]['answer']]\n",
    "    # labels = [ans['entity_name'] for ans in test_data[index]['answer']]\n",
    "    # print(labels)\n",
    "    max_score = 0\n",
    "    for ind, tree_node in enumerate(prediction_tree.values()):\n",
    "        if 'Answer' in tree_node['processed_pred']:\n",
    "            if tree_node['score'] > max_score:\n",
    "                max_score = tree_node['score']\n",
    "                answer = tree_node['processed_pred'].split('Answer:')[-1]\n",
    "    # for tree_node in prediction_tree.values():\n",
    "    #     if 'Answer' in tree_node['processed_pred']:\n",
    "    #         answer = tree_node['processed_pred'].split('Answer:')[-1]\n",
    "    for label in labels:\n",
    "        if label and label in answer:\n",
    "            hit = 1\n",
    "    if hit == 1:\n",
    "        print('Correct')\n",
    "        count += 1\n",
    "        correct_ids.append(index)\n",
    "    break\n",
    "    # else:\n",
    "        # break\n",
    "\n",
    "        # logging_res.append({\"index\": index, \"tree\": prediction_tree})\n",
    "    # if len(logging_res) == 20:\n",
    "        # save_to_json(logging_res, './output/inference/cwq_test_res_1217.json')\n",
    "    #     logging_res = []\n",
    "#1217修改了relation-1hop,修改了score\n",
    "# save_to_json(logging_res, './output/inference/cwq_test_res_1217.json')\n",
    "    #注意有value标签, e.g. WebQTest-31\n",
    "    # for label in labels:\n",
    "    #     if label and label in prediction_tree[len(prediction_tree)-1]['processed_pred']:\n",
    "    #         count += 1\n",
    "    #         break\n",
    "    # except:\n",
    "    #     print(f'{index} Error')\n",
    "    #     continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7182818284590455  Brussels<|eot_id|>\n",
      "5.935372584634358  Brussels<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "for tree_node in prediction_tree.values():\n",
    "    if 'Answer' in tree_node['processed_pred']:\n",
    "        answer = tree_node['processed_pred'].split('Answer:')[-1]\n",
    "        print(tree_node['score'], answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_score = 0\n",
    "for ind, tree_node in enumerate(prediction_tree.values()):\n",
    "    if 'Answer' in tree_node['processed_pred']:\n",
    "        if tree_node['score'] > max_score:\n",
    "            max_score = tree_node['score']\n",
    "            answer = tree_node['processed_pred'].split('Answer:')[-1]\n",
    "        # print(tree_node['score'])\n",
    "        # print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Germanic peoples<|eot_id|>'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Belgium',\n",
       " 'Brazil',\n",
       " 'United States of America',\n",
       " 'Australia',\n",
       " 'Canada',\n",
       " 'France',\n",
       " 'South Africa']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'prompt': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nwhere did flemish people come from<|eot_id|><|start_header_id|>assistant<|end_header_id|>',\n",
       "  'pred': '[Retrieve Relation]',\n",
       "  'processed_pred': '',\n",
       "  'score': None,\n",
       "  'topic_entity': ['m.018hlv'],\n",
       "  'parent': None,\n",
       "  'context': '',\n",
       "  'income': ''},\n",
       " 1: {'prompt': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nwhere did flemish people come from<|eot_id|><|start_header_id|>assistant<|end_header_id|>',\n",
       "  'pred': '[Retrieve Relation]<paragraph>people.ethnicity.people;people.ethnicity.population;people.ethnicity.included_in_group;people.ethnicity.geographic_distribution;people.ethnicity.includes_groups</paragraph>people.ethnicity.people[Partially Relevant]people.ethnicity.population[Partially Relevant]people.ethnicity.included_in_group[Partially Relevant]people.ethnicity.geographic_distribution[Fully Relevant]people.ethnicity.includes_groups[Partially Relevant][Retrieve Entity]',\n",
       "  'score': 0,\n",
       "  'parent': 0,\n",
       "  'topic_entity': ['m.018hlv'],\n",
       "  'context': 'people.ethnicity.people[Partially Relevant]people.ethnicity.population[Partially Relevant]people.ethnicity.included_in_group[Partially Relevant]people.ethnicity.geographic_distribution[Fully Relevant]people.ethnicity.includes_groups[Partially Relevant]',\n",
       "  'income': '',\n",
       "  'processed_pred': '[Retrieve Relation]<paragraph>people.ethnicity.people;people.ethnicity.population;people.ethnicity.included_in_group;people.ethnicity.geographic_distribution;people.ethnicity.includes_groups</paragraph>people.ethnicity.people[Partially Relevant]people.ethnicity.population[Partially Relevant]people.ethnicity.included_in_group[Partially Relevant]people.ethnicity.geographic_distribution[Fully Relevant]people.ethnicity.includes_groups[Partially Relevant]'},\n",
       " 2: {'prompt': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nwhere did flemish people come from<|eot_id|><|start_header_id|>assistant<|end_header_id|>',\n",
       "  'pred': '[Retrieve Relation]<paragraph>people.person.ethnicity</paragraph>people.person.ethnicity[Partially Relevant]people.ethnicity.includes_groups[Partially Relevant]people.ethnicity.included_in_group[Partially Relevant]people.ethnicity.languages_spoken[Partially Relevant]people.ethnicity.geographic_distribution</paragraph>people.person.ethnicity[Partially Relevant]people.ethnicity.includes_groups[Partially Relevant]people.ethnicity.included_in_group[Partially Relevant]people.ethnicity.languages_spoken[Partially Relevant]people.ethnicity.geographic_distribution[Fully Relevant][Retrieve Entity]',\n",
       "  'score': 0,\n",
       "  'parent': 0,\n",
       "  'topic_entity': ['m.018hlv'],\n",
       "  'context': 'people.person.ethnicity[Partially Relevant]people.ethnicity.includes_groups[Partially Relevant]people.ethnicity.included_in_group[Partially Relevant]people.ethnicity.languages_spoken[Partially Relevant]people.ethnicity.geographic_distribution</paragraph>people.person.ethnicity[Partially Relevant]people.ethnicity.includes_groups[Partially Relevant]people.ethnicity.included_in_group[Partially Relevant]people.ethnicity.languages_spoken[Partially Relevant]people.ethnicity.geographic_distribution[Fully Relevant]',\n",
       "  'income': '',\n",
       "  'processed_pred': '[Retrieve Relation]<paragraph>people.person.ethnicity</paragraph>people.person.ethnicity[Partially Relevant]people.ethnicity.includes_groups[Partially Relevant]people.ethnicity.included_in_group[Partially Relevant]people.ethnicity.languages_spoken[Partially Relevant]people.ethnicity.geographic_distribution</paragraph>people.person.ethnicity[Partially Relevant]people.ethnicity.includes_groups[Partially Relevant]people.ethnicity.included_in_group[Partially Relevant]people.ethnicity.languages_spoken[Partially Relevant]people.ethnicity.geographic_distribution[Fully Relevant]'},\n",
       " 3: {'prompt': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nwhere did flemish people come from<|eot_id|><|start_header_id|>assistant<|end_header_id|>[Retrieve Relation]<paragraph>people.ethnicity.people;people.ethnicity.population;people.ethnicity.included_in_group;people.ethnicity.geographic_distribution;people.ethnicity.includes_groups</paragraph>people.ethnicity.people[Partially Relevant]people.ethnicity.population[Partially Relevant]people.ethnicity.included_in_group[Partially Relevant]people.ethnicity.geographic_distribution[Fully Relevant]people.ethnicity.includes_groups[Partially Relevant]',\n",
       "  'pred': '[Retrieve Entity]<paragraph>(Flemish people, people.ethnicity.people, Juliette Binoche);(Flemish people, people.ethnicity.people, Adam Willaerts);(Flemish people, people.ethnicity.people, Eric Van Rompuy);(Flemish people, people.ethnicity.people, Ulla Werbrouck);(Flemish people, people.ethnicity.people, Jan Brueghel the Elder, attributed to)</paragraph>Juliette Binoche[Partially Relevant]Adam Willaerts[Partially Relevant]Eric Van Rompuy[Partially Relevant]Ulla Werbrouck[Partially Relevant]Jan Brueghel the Elder, attributed to[Partially Relevant][Unreasonable][No Retrieval]Answer: Belgium;France;South Africa;Canada;Australia<|eot_id|>',\n",
       "  'context': '',\n",
       "  'score': 2.7182818284590455,\n",
       "  'parent': 1,\n",
       "  'topic_entity': ['m.018hlv'],\n",
       "  'income': 'people.ethnicity.people',\n",
       "  'verbose': {'path': ['(Flemish people, people.ethnicity.people, Juliette Binoche)',\n",
       "    '(Flemish people, people.ethnicity.people, Adam Willaerts)',\n",
       "    '(Flemish people, people.ethnicity.people, Eric Van Rompuy)',\n",
       "    '(Flemish people, people.ethnicity.people, Ulla Werbrouck)',\n",
       "    '(Flemish people, people.ethnicity.people, Jan Brueghel the Elder, attributed to)',\n",
       "    '(Flemish people, people.ethnicity.people, Henry van de Velde)',\n",
       "    '(Flemish people, people.ethnicity.people, Yves Leterme)',\n",
       "    '(Flemish people, people.ethnicity.people, Fons Borginon)',\n",
       "    '(Flemish people, people.ethnicity.people, Lodewijk de Vadder)',\n",
       "    '(Flemish people, people.ethnicity.people, Stijn Streuvels)',\n",
       "    '(Flemish people, people.ethnicity.people, Anthony van Dyck)',\n",
       "    '(Flemish people, people.ethnicity.people, Jean-Marie Dedecker)',\n",
       "    '(Flemish people, people.ethnicity.people, Isabella Brant)',\n",
       "    '(Flemish people, people.ethnicity.people, Herman Van Rompuy)',\n",
       "    '(Flemish people, people.ethnicity.people, Jacques Rogge)',\n",
       "    '(Flemish people, people.ethnicity.people, Marcel Storme)',\n",
       "    '(Flemish people, people.ethnicity.people, Tine Van Rompuy)',\n",
       "    '(Flemish people, people.ethnicity.people, Matthias Storme)',\n",
       "    '(Flemish people, people.ethnicity.people, Boudewijn Bouckaert)',\n",
       "    '(Flemish people, people.ethnicity.people, Jan Baptist van Helmont)',\n",
       "    '(Flemish people, people.ethnicity.people, Geert Bourgeois)',\n",
       "    '(Flemish people, people.ethnicity.people, Jules Boedt)',\n",
       "    '(Flemish people, people.ethnicity.people, Ward Hermans)',\n",
       "    '(Flemish people, people.ethnicity.people, Jules de Saint-Genois)',\n",
       "    '(Flemish people, people.ethnicity.people, Jef François)',\n",
       "    '(Flemish people, people.ethnicity.people, Chris van den Wyngaert)',\n",
       "    '(Flemish people, people.ethnicity.people, René Lagrou)',\n",
       "    '(Flemish people, people.ethnicity.people, Sabine Poleyn)',\n",
       "    '(Flemish people, people.ethnicity.people, Bart Laeremans)',\n",
       "    \"(Flemish people, people.ethnicity.people, Louis D'Haeseleer)\",\n",
       "    '(Flemish people, people.ethnicity.people, Caroline Gennez)',\n",
       "    '(Flemish people, people.ethnicity.people, Bert Van Hoorick)'],\n",
       "   'e_match': {'Juliette Binoche': 'Partially Relevant',\n",
       "    'Adam Willaerts': 'Partially Relevant',\n",
       "    'Eric Van Rompuy': 'Partially Relevant',\n",
       "    'Ulla Werbrouck': 'Partially Relevant',\n",
       "    'Jan Brueghel the Elder, attributed to': 'Partially Relevant'},\n",
       "   'r_match': {'people.ethnicity.people': 'Partially Relevant'},\n",
       "   'reason_score': 0},\n",
       "  'processed_pred': '[Retrieve Entity]<paragraph>(Flemish people, people.ethnicity.people, Juliette Binoche);(Flemish people, people.ethnicity.people, Adam Willaerts);(Flemish people, people.ethnicity.people, Eric Van Rompuy);(Flemish people, people.ethnicity.people, Ulla Werbrouck);(Flemish people, people.ethnicity.people, Jan Brueghel the Elder, attributed to)</paragraph>Juliette Binoche[Partially Relevant]Adam Willaerts[Partially Relevant]Eric Van Rompuy[Partially Relevant]Ulla Werbrouck[Partially Relevant]Jan Brueghel the Elder, attributed to[Partially Relevant][Unreasonable][No Retrieval]Answer: Belgium;France;South Africa;Canada;Australia<|eot_id|>'},\n",
       " 4: {'prompt': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nwhere did flemish people come from<|eot_id|><|start_header_id|>assistant<|end_header_id|>[Retrieve Relation]<paragraph>people.ethnicity.people;people.ethnicity.population;people.ethnicity.included_in_group;people.ethnicity.geographic_distribution;people.ethnicity.includes_groups</paragraph>people.ethnicity.people[Partially Relevant]people.ethnicity.population[Partially Relevant]people.ethnicity.included_in_group[Partially Relevant]people.ethnicity.geographic_distribution[Fully Relevant]people.ethnicity.includes_groups[Partially Relevant]',\n",
       "  'pred': '[Retrieve Entity]<paragraph>(Flemish people, people.ethnicity.population, m.0bmk2zp)</paragraph>Unknown Entity[Partially Relevant][Partially Reasonable][Retrieve Relation]',\n",
       "  'context': 'm.0bmk2zp',\n",
       "  'score': 4.481689070338065,\n",
       "  'parent': 1,\n",
       "  'topic_entity': ['m.0bmk2zp'],\n",
       "  'income': 'people.ethnicity.population',\n",
       "  'verbose': {'path': ['(Flemish people, people.ethnicity.population, m.0bmk2zp)'],\n",
       "   'e_match': {'Unknown Entity': 'Partially Relevant'},\n",
       "   'r_match': {'people.ethnicity.population': 'Partially Relevant'},\n",
       "   'reason_score': 0.5},\n",
       "  'processed_pred': '[Retrieve Entity]<paragraph>(Flemish people, people.ethnicity.population, m.0bmk2zp)</paragraph>Unknown Entity[Partially Relevant][Partially Reasonable][Retrieve Relation]'},\n",
       " 5: {'prompt': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nwhere did flemish people come from<|eot_id|><|start_header_id|>assistant<|end_header_id|>[Retrieve Relation]<paragraph>people.ethnicity.people;people.ethnicity.population;people.ethnicity.included_in_group;people.ethnicity.geographic_distribution;people.ethnicity.includes_groups</paragraph>people.ethnicity.people[Partially Relevant]people.ethnicity.population[Partially Relevant]people.ethnicity.included_in_group[Partially Relevant]people.ethnicity.geographic_distribution[Fully Relevant]people.ethnicity.includes_groups[Partially Relevant]',\n",
       "  'pred': '[Retrieve Entity]<paragraph>(Flemish people, people.ethnicity.included_in_group, Germanic peoples)</paragraph>Germanic peoples[Partially Relevant][Partially Reasonable][No Retrieval]Answer: Germanic peoples<|eot_id|>',\n",
       "  'context': '',\n",
       "  'score': 4.481689070338065,\n",
       "  'parent': 1,\n",
       "  'topic_entity': ['m.018hlv'],\n",
       "  'income': 'people.ethnicity.included_in_group',\n",
       "  'verbose': {'path': ['(Flemish people, people.ethnicity.included_in_group, Germanic peoples)'],\n",
       "   'e_match': {'Germanic peoples': 'Partially Relevant'},\n",
       "   'r_match': {'people.ethnicity.included_in_group': 'Partially Relevant'},\n",
       "   'reason_score': 0.5},\n",
       "  'processed_pred': '[Retrieve Entity]<paragraph>(Flemish people, people.ethnicity.included_in_group, Germanic peoples)</paragraph>Germanic peoples[Partially Relevant][Partially Reasonable][No Retrieval]Answer: Germanic peoples<|eot_id|>'},\n",
       " 6: {'prompt': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nwhere did flemish people come from<|eot_id|><|start_header_id|>assistant<|end_header_id|>[Retrieve Relation]<paragraph>people.ethnicity.people;people.ethnicity.population;people.ethnicity.included_in_group;people.ethnicity.geographic_distribution;people.ethnicity.includes_groups</paragraph>people.ethnicity.people[Partially Relevant]people.ethnicity.population[Partially Relevant]people.ethnicity.included_in_group[Partially Relevant]people.ethnicity.geographic_distribution[Fully Relevant]people.ethnicity.includes_groups[Partially Relevant]',\n",
       "  'pred': '[Retrieve Entity]<paragraph>(Flemish people, people.ethnicity.geographic_distribution, Canada);(Flemish people, people.ethnicity.geographic_distribution, France);(Flemish people, people.ethnicity.geographic_distribution, Brazil);(Flemish people, people.ethnicity.geographic_distribution, United States of America);(Flemish people, people.ethnicity.geographic_distribution, Australia)</paragraph>Canada[Partially Relevant]France[Partially Relevant]Brazil[Partially Relevant]United States of America[Partially Relevant]Australia[Partially Relevant][Unreasonable][No Retrieval]Answer: Belgium<|eot_id|>',\n",
       "  'context': '',\n",
       "  'score': 4.4816890703380645,\n",
       "  'parent': 1,\n",
       "  'topic_entity': ['m.018hlv'],\n",
       "  'income': 'people.ethnicity.geographic_distribution',\n",
       "  'verbose': {'path': ['(Flemish people, people.ethnicity.geographic_distribution, Canada)',\n",
       "    '(Flemish people, people.ethnicity.geographic_distribution, France)',\n",
       "    '(Flemish people, people.ethnicity.geographic_distribution, Brazil)',\n",
       "    '(Flemish people, people.ethnicity.geographic_distribution, United States of America)',\n",
       "    '(Flemish people, people.ethnicity.geographic_distribution, Australia)',\n",
       "    '(Flemish people, people.ethnicity.geographic_distribution, Belgium)',\n",
       "    '(Flemish people, people.ethnicity.geographic_distribution, South Africa)'],\n",
       "   'e_match': {'Canada': 'Partially Relevant',\n",
       "    'France': 'Partially Relevant',\n",
       "    'Brazil': 'Partially Relevant',\n",
       "    'United States of America': 'Partially Relevant',\n",
       "    'Australia': 'Partially Relevant'},\n",
       "   'r_match': {'people.ethnicity.geographic_distribution': 'Fully Relevant'},\n",
       "   'reason_score': 0},\n",
       "  'processed_pred': '[Retrieve Entity]<paragraph>(Flemish people, people.ethnicity.geographic_distribution, Canada);(Flemish people, people.ethnicity.geographic_distribution, France);(Flemish people, people.ethnicity.geographic_distribution, Brazil);(Flemish people, people.ethnicity.geographic_distribution, United States of America);(Flemish people, people.ethnicity.geographic_distribution, Australia)</paragraph>Canada[Partially Relevant]France[Partially Relevant]Brazil[Partially Relevant]United States of America[Partially Relevant]Australia[Partially Relevant][Unreasonable][No Retrieval]Answer: Belgium<|eot_id|>'},\n",
       " 7: {'prompt': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nwhere did flemish people come from<|eot_id|><|start_header_id|>assistant<|end_header_id|>[Retrieve Relation]<paragraph>people.ethnicity.people;people.ethnicity.population;people.ethnicity.included_in_group;people.ethnicity.geographic_distribution;people.ethnicity.includes_groups</paragraph>people.ethnicity.people[Partially Relevant]people.ethnicity.population[Partially Relevant]people.ethnicity.included_in_group[Partially Relevant]people.ethnicity.geographic_distribution[Fully Relevant]people.ethnicity.includes_groups[Partially Relevant]',\n",
       "  'pred': '[Retrieve Entity]<paragraph>(Flemish people, people.ethnicity.includes_groups, Germanic peoples)</paragraph>Germanic peoples[Partially Relevant][Partially Reasonable][No Retrieval]Answer: Germanic peoples<|eot_id|>',\n",
       "  'context': '',\n",
       "  'score': 4.481689070338065,\n",
       "  'parent': 1,\n",
       "  'topic_entity': ['m.018hlv'],\n",
       "  'income': 'people.ethnicity.includes_groups',\n",
       "  'verbose': {'path': ['(Flemish people, people.ethnicity.includes_groups, Germanic peoples)'],\n",
       "   'e_match': {'Germanic peoples': 'Partially Relevant'},\n",
       "   'r_match': {'people.ethnicity.includes_groups': 'Partially Relevant'},\n",
       "   'reason_score': 0.5},\n",
       "  'processed_pred': '[Retrieve Entity]<paragraph>(Flemish people, people.ethnicity.includes_groups, Germanic peoples)</paragraph>Germanic peoples[Partially Relevant][Partially Reasonable][No Retrieval]Answer: Germanic peoples<|eot_id|>'},\n",
       " 8: {'prompt': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nwhere did flemish people come from<|eot_id|><|start_header_id|>assistant<|end_header_id|>[Retrieve Relation]<paragraph>people.person.ethnicity</paragraph>people.person.ethnicity[Partially Relevant]people.ethnicity.includes_groups[Partially Relevant]people.ethnicity.included_in_group[Partially Relevant]people.ethnicity.languages_spoken[Partially Relevant]people.ethnicity.geographic_distribution</paragraph>people.person.ethnicity[Partially Relevant]people.ethnicity.includes_groups[Partially Relevant]people.ethnicity.included_in_group[Partially Relevant]people.ethnicity.languages_spoken[Partially Relevant]people.ethnicity.geographic_distribution[Fully Relevant]',\n",
       "  'pred': '[Retrieve Entity]<paragraph>(Flemish people, people.person.ethnicity, Juliette Binoche);(Flemish people, people.person.ethnicity, Adam Willaerts);(Flemish people, people.person.ethnicity, Eric Van Rompuy);(Flemish people, people.person.ethnicity, Ulla Werbrouck);(Flemish people, people.person.ethnicity, Jan Brueghel the Elder, attributed to)</paragraph>Juliette Binoche[Unrelevant]Adam Willaerts[Unrelevant]Eric Van Rompuy[Unrelevant]Ulla Werbrouck[Unrelevant]Jan Brueghel the Elder, attributed to[Unrelevant][Unreasonable][No Retrieval]Answer: Belgium;France;South Africa;Canada;Australia<|eot_id|>',\n",
       "  'context': '',\n",
       "  'score': 1.6487212707001282,\n",
       "  'parent': 2,\n",
       "  'topic_entity': ['m.018hlv'],\n",
       "  'income': 'people.person.ethnicity',\n",
       "  'verbose': {'path': ['(Flemish people, people.person.ethnicity, Juliette Binoche)',\n",
       "    '(Flemish people, people.person.ethnicity, Adam Willaerts)',\n",
       "    '(Flemish people, people.person.ethnicity, Eric Van Rompuy)',\n",
       "    '(Flemish people, people.person.ethnicity, Ulla Werbrouck)',\n",
       "    '(Flemish people, people.person.ethnicity, Jan Brueghel the Elder, attributed to)',\n",
       "    '(Flemish people, people.person.ethnicity, Henry van de Velde)',\n",
       "    '(Flemish people, people.person.ethnicity, Yves Leterme)',\n",
       "    '(Flemish people, people.person.ethnicity, Fons Borginon)',\n",
       "    '(Flemish people, people.person.ethnicity, Lodewijk de Vadder)',\n",
       "    '(Flemish people, people.person.ethnicity, Stijn Streuvels)',\n",
       "    '(Flemish people, people.person.ethnicity, Anthony van Dyck)',\n",
       "    '(Flemish people, people.person.ethnicity, Jean-Marie Dedecker)',\n",
       "    '(Flemish people, people.person.ethnicity, Isabella Brant)',\n",
       "    '(Flemish people, people.person.ethnicity, Herman Van Rompuy)',\n",
       "    '(Flemish people, people.person.ethnicity, Jacques Rogge)',\n",
       "    '(Flemish people, people.person.ethnicity, Marcel Storme)',\n",
       "    '(Flemish people, people.person.ethnicity, Tine Van Rompuy)',\n",
       "    '(Flemish people, people.person.ethnicity, Matthias Storme)',\n",
       "    '(Flemish people, people.person.ethnicity, Boudewijn Bouckaert)',\n",
       "    '(Flemish people, people.person.ethnicity, Jan Baptist van Helmont)',\n",
       "    '(Flemish people, people.person.ethnicity, Geert Bourgeois)',\n",
       "    '(Flemish people, people.person.ethnicity, Jules Boedt)',\n",
       "    '(Flemish people, people.person.ethnicity, Ward Hermans)',\n",
       "    '(Flemish people, people.person.ethnicity, Jules de Saint-Genois)',\n",
       "    '(Flemish people, people.person.ethnicity, Jef François)',\n",
       "    '(Flemish people, people.person.ethnicity, Chris van den Wyngaert)',\n",
       "    '(Flemish people, people.person.ethnicity, René Lagrou)',\n",
       "    '(Flemish people, people.person.ethnicity, Sabine Poleyn)',\n",
       "    '(Flemish people, people.person.ethnicity, Bart Laeremans)',\n",
       "    \"(Flemish people, people.person.ethnicity, Louis D'Haeseleer)\",\n",
       "    '(Flemish people, people.person.ethnicity, Caroline Gennez)',\n",
       "    '(Flemish people, people.person.ethnicity, Bert Van Hoorick)'],\n",
       "   'e_match': {'Juliette Binoche': 'Unrelevant',\n",
       "    'Adam Willaerts': 'Unrelevant',\n",
       "    'Eric Van Rompuy': 'Unrelevant',\n",
       "    'Ulla Werbrouck': 'Unrelevant',\n",
       "    'Jan Brueghel the Elder, attributed to': 'Unrelevant'},\n",
       "   'r_match': {'people.person.ethnicity': 'Partially Relevant'},\n",
       "   'reason_score': 0},\n",
       "  'processed_pred': '[Retrieve Entity]<paragraph>(Flemish people, people.person.ethnicity, Juliette Binoche);(Flemish people, people.person.ethnicity, Adam Willaerts);(Flemish people, people.person.ethnicity, Eric Van Rompuy);(Flemish people, people.person.ethnicity, Ulla Werbrouck);(Flemish people, people.person.ethnicity, Jan Brueghel the Elder, attributed to)</paragraph>Juliette Binoche[Unrelevant]Adam Willaerts[Unrelevant]Eric Van Rompuy[Unrelevant]Ulla Werbrouck[Unrelevant]Jan Brueghel the Elder, attributed to[Unrelevant][Unreasonable][No Retrieval]Answer: Belgium;France;South Africa;Canada;Australia<|eot_id|>'},\n",
       " 9: {'prompt': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nwhere did flemish people come from<|eot_id|><|start_header_id|>assistant<|end_header_id|>[Retrieve Relation]<paragraph>people.person.ethnicity</paragraph>people.person.ethnicity[Partially Relevant]people.ethnicity.includes_groups[Partially Relevant]people.ethnicity.included_in_group[Partially Relevant]people.ethnicity.languages_spoken[Partially Relevant]people.ethnicity.geographic_distribution</paragraph>people.person.ethnicity[Partially Relevant]people.ethnicity.includes_groups[Partially Relevant]people.ethnicity.included_in_group[Partially Relevant]people.ethnicity.languages_spoken[Partially Relevant]people.ethnicity.geographic_distribution[Fully Relevant]',\n",
       "  'pred': '[Retrieve Entity]<paragraph>(Flemish people, people.ethnicity.includes_groups, Germanic peoples)</paragraph>Germanic peoples[Partially Relevant][Partially Reasonable][No Retrieval]Answer: Germanic peoples<|eot_id|>',\n",
       "  'context': '',\n",
       "  'score': 4.481689070338065,\n",
       "  'parent': 2,\n",
       "  'topic_entity': ['m.018hlv'],\n",
       "  'income': 'people.ethnicity.includes_groups',\n",
       "  'verbose': {'path': ['(Flemish people, people.ethnicity.includes_groups, Germanic peoples)'],\n",
       "   'e_match': {'Germanic peoples': 'Partially Relevant'},\n",
       "   'r_match': {'people.ethnicity.includes_groups': 'Partially Relevant'},\n",
       "   'reason_score': 0.5},\n",
       "  'processed_pred': '[Retrieve Entity]<paragraph>(Flemish people, people.ethnicity.includes_groups, Germanic peoples)</paragraph>Germanic peoples[Partially Relevant][Partially Reasonable][No Retrieval]Answer: Germanic peoples<|eot_id|>'},\n",
       " 10: {'prompt': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nwhere did flemish people come from<|eot_id|><|start_header_id|>assistant<|end_header_id|>[Retrieve Relation]<paragraph>people.person.ethnicity</paragraph>people.person.ethnicity[Partially Relevant]people.ethnicity.includes_groups[Partially Relevant]people.ethnicity.included_in_group[Partially Relevant]people.ethnicity.languages_spoken[Partially Relevant]people.ethnicity.geographic_distribution</paragraph>people.person.ethnicity[Partially Relevant]people.ethnicity.includes_groups[Partially Relevant]people.ethnicity.included_in_group[Partially Relevant]people.ethnicity.languages_spoken[Partially Relevant]people.ethnicity.geographic_distribution[Fully Relevant]',\n",
       "  'pred': '[Retrieve Entity]<paragraph>(Flemish people, people.ethnicity.included_in_group, Germanic peoples)</paragraph>Germanic peoples[Partially Relevant][Partially Reasonable][No Retrieval]Answer: Germanic peoples<|eot_id|>',\n",
       "  'context': '',\n",
       "  'score': 4.481689070338065,\n",
       "  'parent': 2,\n",
       "  'topic_entity': ['m.018hlv'],\n",
       "  'income': 'people.ethnicity.included_in_group',\n",
       "  'verbose': {'path': ['(Flemish people, people.ethnicity.included_in_group, Germanic peoples)'],\n",
       "   'e_match': {'Germanic peoples': 'Partially Relevant'},\n",
       "   'r_match': {'people.ethnicity.included_in_group': 'Partially Relevant'},\n",
       "   'reason_score': 0.5},\n",
       "  'processed_pred': '[Retrieve Entity]<paragraph>(Flemish people, people.ethnicity.included_in_group, Germanic peoples)</paragraph>Germanic peoples[Partially Relevant][Partially Reasonable][No Retrieval]Answer: Germanic peoples<|eot_id|>'},\n",
       " 11: {'prompt': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nwhere did flemish people come from<|eot_id|><|start_header_id|>assistant<|end_header_id|>[Retrieve Relation]<paragraph>people.person.ethnicity</paragraph>people.person.ethnicity[Partially Relevant]people.ethnicity.includes_groups[Partially Relevant]people.ethnicity.included_in_group[Partially Relevant]people.ethnicity.languages_spoken[Partially Relevant]people.ethnicity.geographic_distribution</paragraph>people.person.ethnicity[Partially Relevant]people.ethnicity.includes_groups[Partially Relevant]people.ethnicity.included_in_group[Partially Relevant]people.ethnicity.languages_spoken[Partially Relevant]people.ethnicity.geographic_distribution[Fully Relevant]',\n",
       "  'pred': '[Retrieve Entity]<paragraph>(Flemish people, people.ethnicity.languages_spoken, English Language);(Flemish people, people.ethnicity.languages_spoken, Dutch Language);(Flemish people, people.ethnicity.languages_spoken, French)</paragraph>English Language[Partially Relevant]Dutch Language[Fully Relevant]French[Partially Relevant][Unreasonable][No Retrieval]Answer: France;Belgium;South Africa;Canada;Australia<|eot_id|>',\n",
       "  'context': '',\n",
       "  'score': 3.211270543153561,\n",
       "  'parent': 2,\n",
       "  'topic_entity': ['m.018hlv'],\n",
       "  'income': 'people.ethnicity.languages_spoken',\n",
       "  'verbose': {'path': ['(Flemish people, people.ethnicity.languages_spoken, English Language)',\n",
       "    '(Flemish people, people.ethnicity.languages_spoken, Dutch Language)',\n",
       "    '(Flemish people, people.ethnicity.languages_spoken, French)'],\n",
       "   'e_match': {'English Language': 'Partially Relevant',\n",
       "    'Dutch Language': 'Fully Relevant',\n",
       "    'French': 'Partially Relevant'},\n",
       "   'r_match': {'people.ethnicity.languages_spoken': 'Partially Relevant'},\n",
       "   'reason_score': 0},\n",
       "  'processed_pred': '[Retrieve Entity]<paragraph>(Flemish people, people.ethnicity.languages_spoken, English Language);(Flemish people, people.ethnicity.languages_spoken, Dutch Language);(Flemish people, people.ethnicity.languages_spoken, French)</paragraph>English Language[Partially Relevant]Dutch Language[Fully Relevant]French[Partially Relevant][Unreasonable][No Retrieval]Answer: France;Belgium;South Africa;Canada;Australia<|eot_id|>'},\n",
       " 12: {'prompt': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nwhere did flemish people come from<|eot_id|><|start_header_id|>assistant<|end_header_id|>[Retrieve Relation]<paragraph>people.person.ethnicity</paragraph>people.person.ethnicity[Partially Relevant]people.ethnicity.includes_groups[Partially Relevant]people.ethnicity.included_in_group[Partially Relevant]people.ethnicity.languages_spoken[Partially Relevant]people.ethnicity.geographic_distribution</paragraph>people.person.ethnicity[Partially Relevant]people.ethnicity.includes_groups[Partially Relevant]people.ethnicity.included_in_group[Partially Relevant]people.ethnicity.languages_spoken[Partially Relevant]people.ethnicity.geographic_distribution[Fully Relevant]',\n",
       "  'pred': '[Retrieve Entity]<paragraph>(Flemish people, people.ethnicity.geographic_distribution, Canada);(Flemish people, people.ethnicity.geographic_distribution, France);(Flemish people, people.ethnicity.geographic_distribution, Brazil);(Flemish people, people.ethnicity.geographic_distribution, United States of America);(Flemish people, people.ethnicity.geographic_distribution, Australia)</paragraph>Canada[Partially Relevant]France[Partially Relevant]Brazil[Partially Relevant]United States of America[Partially Relevant]Australia[Partially Relevant][Unreasonable][No Retrieval]Answer: Belgium;France;South Africa;Canada;Australia<|eot_id|>',\n",
       "  'context': '',\n",
       "  'score': 4.4816890703380645,\n",
       "  'parent': 2,\n",
       "  'topic_entity': ['m.018hlv'],\n",
       "  'income': 'people.ethnicity.geographic_distribution',\n",
       "  'verbose': {'path': ['(Flemish people, people.ethnicity.geographic_distribution, Canada)',\n",
       "    '(Flemish people, people.ethnicity.geographic_distribution, France)',\n",
       "    '(Flemish people, people.ethnicity.geographic_distribution, Brazil)',\n",
       "    '(Flemish people, people.ethnicity.geographic_distribution, United States of America)',\n",
       "    '(Flemish people, people.ethnicity.geographic_distribution, Australia)',\n",
       "    '(Flemish people, people.ethnicity.geographic_distribution, Belgium)',\n",
       "    '(Flemish people, people.ethnicity.geographic_distribution, South Africa)'],\n",
       "   'e_match': {'Canada': 'Partially Relevant',\n",
       "    'France': 'Partially Relevant',\n",
       "    'Brazil': 'Partially Relevant',\n",
       "    'United States of America': 'Partially Relevant',\n",
       "    'Australia': 'Partially Relevant'},\n",
       "   'r_match': {'people.ethnicity.geographic_distribution': 'Fully Relevant'},\n",
       "   'reason_score': 0},\n",
       "  'processed_pred': '[Retrieve Entity]<paragraph>(Flemish people, people.ethnicity.geographic_distribution, Canada);(Flemish people, people.ethnicity.geographic_distribution, France);(Flemish people, people.ethnicity.geographic_distribution, Brazil);(Flemish people, people.ethnicity.geographic_distribution, United States of America);(Flemish people, people.ethnicity.geographic_distribution, Australia)</paragraph>Canada[Partially Relevant]France[Partially Relevant]Brazil[Partially Relevant]United States of America[Partially Relevant]Australia[Partially Relevant][Unreasonable][No Retrieval]Answer: Belgium;France;South Africa;Canada;Australia<|eot_id|>'},\n",
       " 13: {'prompt': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nwhere did flemish people come from<|eot_id|><|start_header_id|>assistant<|end_header_id|>[Retrieve Relation]<paragraph>people.ethnicity.people;people.ethnicity.population;people.ethnicity.included_in_group;people.ethnicity.geographic_distribution;people.ethnicity.includes_groups</paragraph>people.ethnicity.people[Partially Relevant]people.ethnicity.population[Partially Relevant]people.ethnicity.included_in_group[Partially Relevant]people.ethnicity.geographic_distribution[Fully Relevant]people.ethnicity.includes_groups[Partially Relevant][Retrieve Entity]<paragraph>(Flemish people, people.ethnicity.population, m.0bmk2zp)</paragraph>Unknown Entity[Partially Relevant][Partially Reasonable][Retrieve Relation]',\n",
       "  'pred': '[Retrieve Relation]<paragraph>measurement_unit.dated_integer.year;type.type.instance;measurement_unit.dated_integer.number;type.object.type</paragraph>measurement_unit.dated_integer.year[Unrelevant]type.type.instance[Unrelevant]measurement_unit.dated_integer.number[Unrelevant]type.object.type[Unrelevant][Retrieve Entity]',\n",
       "  'score': 4.481689070338065,\n",
       "  'parent': 4,\n",
       "  'topic_entity': ['m.0bmk2zp'],\n",
       "  'context': 'measurement_unit.dated_integer.year[Unrelevant]type.type.instance[Unrelevant]measurement_unit.dated_integer.number[Unrelevant]type.object.type[Unrelevant]',\n",
       "  'income': 'people.ethnicity.population',\n",
       "  'processed_pred': '[Retrieve Relation]<paragraph>measurement_unit.dated_integer.year;type.type.instance;measurement_unit.dated_integer.number;type.object.type</paragraph>measurement_unit.dated_integer.year[Unrelevant]type.type.instance[Unrelevant]measurement_unit.dated_integer.number[Unrelevant]type.object.type[Unrelevant]'}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_nodes = []\n",
    "for n_ind, node in prediction_tree.items():\n",
    "    if 'Answer' in node['processed_pred']:\n",
    "        end_nodes.append(n_ind)\n",
    "\n",
    "queues = []\n",
    "for n_ind in end_nodes:\n",
    "\n",
    "    queue = []\n",
    "    node = prediction_tree[n_ind]\n",
    "    answer = node['processed_pred'].split('Answer:')[-1]\n",
    "    score =  node['score']\n",
    "    while node:\n",
    "        parent = node['parent']\n",
    "        if 'verbose' in node:\n",
    "            queue.append(node['verbose'])\n",
    "        if parent == None:\n",
    "            queues.append({'verbose': queue, \"answer\": answer, \"score\": score})\n",
    "\n",
    "            break\n",
    "        node = prediction_tree[parent]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "queues.sort(key=lambda x: x['score'], reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'verbose': [{'path': ['(Flemish people, people.ethnicity.included_in_group, Germanic peoples)'],\n",
       "    'e_match': {'Germanic peoples': 'Partially Relevant'},\n",
       "    'r_match': {'people.ethnicity.included_in_group': 'Partially Relevant'},\n",
       "    'reason_score': 0.5}],\n",
       "  'answer': ' Germanic peoples<|eot_id|>',\n",
       "  'score': 4.481689070338065},\n",
       " {'verbose': [{'path': ['(Flemish people, people.ethnicity.includes_groups, Germanic peoples)'],\n",
       "    'e_match': {'Germanic peoples': 'Partially Relevant'},\n",
       "    'r_match': {'people.ethnicity.includes_groups': 'Partially Relevant'},\n",
       "    'reason_score': 0.5}],\n",
       "  'answer': ' Germanic peoples<|eot_id|>',\n",
       "  'score': 4.481689070338065},\n",
       " {'verbose': [{'path': ['(Flemish people, people.ethnicity.includes_groups, Germanic peoples)'],\n",
       "    'e_match': {'Germanic peoples': 'Partially Relevant'},\n",
       "    'r_match': {'people.ethnicity.includes_groups': 'Partially Relevant'},\n",
       "    'reason_score': 0.5}],\n",
       "  'answer': ' Germanic peoples<|eot_id|>',\n",
       "  'score': 4.481689070338065},\n",
       " {'verbose': [{'path': ['(Flemish people, people.ethnicity.included_in_group, Germanic peoples)'],\n",
       "    'e_match': {'Germanic peoples': 'Partially Relevant'},\n",
       "    'r_match': {'people.ethnicity.included_in_group': 'Partially Relevant'},\n",
       "    'reason_score': 0.5}],\n",
       "  'answer': ' Germanic peoples<|eot_id|>',\n",
       "  'score': 4.481689070338065},\n",
       " {'verbose': [{'path': ['(Flemish people, people.ethnicity.geographic_distribution, Canada)',\n",
       "     '(Flemish people, people.ethnicity.geographic_distribution, France)',\n",
       "     '(Flemish people, people.ethnicity.geographic_distribution, Brazil)',\n",
       "     '(Flemish people, people.ethnicity.geographic_distribution, United States of America)',\n",
       "     '(Flemish people, people.ethnicity.geographic_distribution, Australia)',\n",
       "     '(Flemish people, people.ethnicity.geographic_distribution, Belgium)',\n",
       "     '(Flemish people, people.ethnicity.geographic_distribution, South Africa)'],\n",
       "    'e_match': {'Canada': 'Partially Relevant',\n",
       "     'France': 'Partially Relevant',\n",
       "     'Brazil': 'Partially Relevant',\n",
       "     'United States of America': 'Partially Relevant',\n",
       "     'Australia': 'Partially Relevant'},\n",
       "    'r_match': {'people.ethnicity.geographic_distribution': 'Fully Relevant'},\n",
       "    'reason_score': 0}],\n",
       "  'answer': ' Belgium<|eot_id|>',\n",
       "  'score': 4.4816890703380645},\n",
       " {'verbose': [{'path': ['(Flemish people, people.ethnicity.geographic_distribution, Canada)',\n",
       "     '(Flemish people, people.ethnicity.geographic_distribution, France)',\n",
       "     '(Flemish people, people.ethnicity.geographic_distribution, Brazil)',\n",
       "     '(Flemish people, people.ethnicity.geographic_distribution, United States of America)',\n",
       "     '(Flemish people, people.ethnicity.geographic_distribution, Australia)',\n",
       "     '(Flemish people, people.ethnicity.geographic_distribution, Belgium)',\n",
       "     '(Flemish people, people.ethnicity.geographic_distribution, South Africa)'],\n",
       "    'e_match': {'Canada': 'Partially Relevant',\n",
       "     'France': 'Partially Relevant',\n",
       "     'Brazil': 'Partially Relevant',\n",
       "     'United States of America': 'Partially Relevant',\n",
       "     'Australia': 'Partially Relevant'},\n",
       "    'r_match': {'people.ethnicity.geographic_distribution': 'Fully Relevant'},\n",
       "    'reason_score': 0}],\n",
       "  'answer': ' Belgium;France;South Africa;Canada;Australia<|eot_id|>',\n",
       "  'score': 4.4816890703380645},\n",
       " {'verbose': [{'path': ['(Flemish people, people.ethnicity.languages_spoken, English Language)',\n",
       "     '(Flemish people, people.ethnicity.languages_spoken, Dutch Language)',\n",
       "     '(Flemish people, people.ethnicity.languages_spoken, French)'],\n",
       "    'e_match': {'English Language': 'Partially Relevant',\n",
       "     'Dutch Language': 'Fully Relevant',\n",
       "     'French': 'Partially Relevant'},\n",
       "    'r_match': {'people.ethnicity.languages_spoken': 'Partially Relevant'},\n",
       "    'reason_score': 0}],\n",
       "  'answer': ' France;Belgium;South Africa;Canada;Australia<|eot_id|>',\n",
       "  'score': 3.211270543153561},\n",
       " {'verbose': [{'path': ['(Flemish people, people.ethnicity.people, Juliette Binoche)',\n",
       "     '(Flemish people, people.ethnicity.people, Adam Willaerts)',\n",
       "     '(Flemish people, people.ethnicity.people, Eric Van Rompuy)',\n",
       "     '(Flemish people, people.ethnicity.people, Ulla Werbrouck)',\n",
       "     '(Flemish people, people.ethnicity.people, Jan Brueghel the Elder, attributed to)',\n",
       "     '(Flemish people, people.ethnicity.people, Henry van de Velde)',\n",
       "     '(Flemish people, people.ethnicity.people, Yves Leterme)',\n",
       "     '(Flemish people, people.ethnicity.people, Fons Borginon)',\n",
       "     '(Flemish people, people.ethnicity.people, Lodewijk de Vadder)',\n",
       "     '(Flemish people, people.ethnicity.people, Stijn Streuvels)',\n",
       "     '(Flemish people, people.ethnicity.people, Anthony van Dyck)',\n",
       "     '(Flemish people, people.ethnicity.people, Jean-Marie Dedecker)',\n",
       "     '(Flemish people, people.ethnicity.people, Isabella Brant)',\n",
       "     '(Flemish people, people.ethnicity.people, Herman Van Rompuy)',\n",
       "     '(Flemish people, people.ethnicity.people, Jacques Rogge)',\n",
       "     '(Flemish people, people.ethnicity.people, Marcel Storme)',\n",
       "     '(Flemish people, people.ethnicity.people, Tine Van Rompuy)',\n",
       "     '(Flemish people, people.ethnicity.people, Matthias Storme)',\n",
       "     '(Flemish people, people.ethnicity.people, Boudewijn Bouckaert)',\n",
       "     '(Flemish people, people.ethnicity.people, Jan Baptist van Helmont)',\n",
       "     '(Flemish people, people.ethnicity.people, Geert Bourgeois)',\n",
       "     '(Flemish people, people.ethnicity.people, Jules Boedt)',\n",
       "     '(Flemish people, people.ethnicity.people, Ward Hermans)',\n",
       "     '(Flemish people, people.ethnicity.people, Jules de Saint-Genois)',\n",
       "     '(Flemish people, people.ethnicity.people, Jef François)',\n",
       "     '(Flemish people, people.ethnicity.people, Chris van den Wyngaert)',\n",
       "     '(Flemish people, people.ethnicity.people, René Lagrou)',\n",
       "     '(Flemish people, people.ethnicity.people, Sabine Poleyn)',\n",
       "     '(Flemish people, people.ethnicity.people, Bart Laeremans)',\n",
       "     \"(Flemish people, people.ethnicity.people, Louis D'Haeseleer)\",\n",
       "     '(Flemish people, people.ethnicity.people, Caroline Gennez)',\n",
       "     '(Flemish people, people.ethnicity.people, Bert Van Hoorick)'],\n",
       "    'e_match': {'Juliette Binoche': 'Partially Relevant',\n",
       "     'Adam Willaerts': 'Partially Relevant',\n",
       "     'Eric Van Rompuy': 'Partially Relevant',\n",
       "     'Ulla Werbrouck': 'Partially Relevant',\n",
       "     'Jan Brueghel the Elder, attributed to': 'Partially Relevant'},\n",
       "    'r_match': {'people.ethnicity.people': 'Partially Relevant'},\n",
       "    'reason_score': 0}],\n",
       "  'answer': ' Belgium;France;South Africa;Canada;Australia<|eot_id|>',\n",
       "  'score': 2.7182818284590455},\n",
       " {'verbose': [{'path': ['(Flemish people, people.person.ethnicity, Juliette Binoche)',\n",
       "     '(Flemish people, people.person.ethnicity, Adam Willaerts)',\n",
       "     '(Flemish people, people.person.ethnicity, Eric Van Rompuy)',\n",
       "     '(Flemish people, people.person.ethnicity, Ulla Werbrouck)',\n",
       "     '(Flemish people, people.person.ethnicity, Jan Brueghel the Elder, attributed to)',\n",
       "     '(Flemish people, people.person.ethnicity, Henry van de Velde)',\n",
       "     '(Flemish people, people.person.ethnicity, Yves Leterme)',\n",
       "     '(Flemish people, people.person.ethnicity, Fons Borginon)',\n",
       "     '(Flemish people, people.person.ethnicity, Lodewijk de Vadder)',\n",
       "     '(Flemish people, people.person.ethnicity, Stijn Streuvels)',\n",
       "     '(Flemish people, people.person.ethnicity, Anthony van Dyck)',\n",
       "     '(Flemish people, people.person.ethnicity, Jean-Marie Dedecker)',\n",
       "     '(Flemish people, people.person.ethnicity, Isabella Brant)',\n",
       "     '(Flemish people, people.person.ethnicity, Herman Van Rompuy)',\n",
       "     '(Flemish people, people.person.ethnicity, Jacques Rogge)',\n",
       "     '(Flemish people, people.person.ethnicity, Marcel Storme)',\n",
       "     '(Flemish people, people.person.ethnicity, Tine Van Rompuy)',\n",
       "     '(Flemish people, people.person.ethnicity, Matthias Storme)',\n",
       "     '(Flemish people, people.person.ethnicity, Boudewijn Bouckaert)',\n",
       "     '(Flemish people, people.person.ethnicity, Jan Baptist van Helmont)',\n",
       "     '(Flemish people, people.person.ethnicity, Geert Bourgeois)',\n",
       "     '(Flemish people, people.person.ethnicity, Jules Boedt)',\n",
       "     '(Flemish people, people.person.ethnicity, Ward Hermans)',\n",
       "     '(Flemish people, people.person.ethnicity, Jules de Saint-Genois)',\n",
       "     '(Flemish people, people.person.ethnicity, Jef François)',\n",
       "     '(Flemish people, people.person.ethnicity, Chris van den Wyngaert)',\n",
       "     '(Flemish people, people.person.ethnicity, René Lagrou)',\n",
       "     '(Flemish people, people.person.ethnicity, Sabine Poleyn)',\n",
       "     '(Flemish people, people.person.ethnicity, Bart Laeremans)',\n",
       "     \"(Flemish people, people.person.ethnicity, Louis D'Haeseleer)\",\n",
       "     '(Flemish people, people.person.ethnicity, Caroline Gennez)',\n",
       "     '(Flemish people, people.person.ethnicity, Bert Van Hoorick)'],\n",
       "    'e_match': {'Juliette Binoche': 'Unrelevant',\n",
       "     'Adam Willaerts': 'Unrelevant',\n",
       "     'Eric Van Rompuy': 'Unrelevant',\n",
       "     'Ulla Werbrouck': 'Unrelevant',\n",
       "     'Jan Brueghel the Elder, attributed to': 'Unrelevant'},\n",
       "    'r_match': {'people.person.ethnicity': 'Partially Relevant'},\n",
       "    'reason_score': 0}],\n",
       "  'answer': ' Belgium;France;South Africa;Canada;Australia<|eot_id|>',\n",
       "  'score': 1.6487212707001282}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queues[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/media/disk1/chatgpt/zh/graph_data/output/inference/webqsp_0122model_code_0123_res.json', 'r', encoding='utf-8') as f:\n",
    "    logging_res_0123 = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "207\n",
      "218\n",
      "219\n",
      "222\n",
      "228\n",
      "241\n",
      "249\n",
      "251\n",
      "264\n",
      "277\n",
      "286\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "\n",
    "for index in range(200, 300):\n",
    "    hit_1 = 0\n",
    "    hit_2 = 0\n",
    "    max_score = 0\n",
    "    labels = [get_label(ans) if ans.startswith('m.') else ans for ans in test_data[index]['answer']]\n",
    "    for ind, tree_node in enumerate(logging_res_0123[index]['tree'].values()):\n",
    "        if 'Answer' in tree_node['processed_pred']:\n",
    "            if tree_node['score'] > max_score:\n",
    "                max_score = tree_node['score']\n",
    "                answer_1 = tree_node['processed_pred'].split('Answer:')[-1]\n",
    "            answer_2 = tree_node['processed_pred'].split('Answer:')[-1]\n",
    "            for label in labels:\n",
    "                if label in answer_2:\n",
    "                    hit_2 = 1\n",
    "    for label in labels:\n",
    "        if label in answer_1:\n",
    "            hit_1 = 1\n",
    "    if hit_1 == 0 and hit_2 == 1:\n",
    "        print(index)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Comedian', 'Actor', 'Television producer', 'Singer', 'Model']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'0': {'prompt': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nwhat was lucille ball<|eot_id|><|start_header_id|>assistant<|end_header_id|>',\n",
       "  'pred': '[Retrieve Relation]',\n",
       "  'processed_pred': '',\n",
       "  'score': None,\n",
       "  'topic_entity': ['m.04nw9'],\n",
       "  'parent': None,\n",
       "  'context': ''},\n",
       " '1': {'prompt': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nwhat was lucille ball<|eot_id|><|start_header_id|>assistant<|end_header_id|>',\n",
       "  'pred': '[Retrieve Relation]<paragraph>base.americancomedy.double_act.comic;people.person.places_lived;people.person.profession;base.popstra.celebrity.religion;people.person.religion</paragraph>base.americancomedy.double_act.comic[Partially Relevant]people.person.places_lived[Unrelevant]people.person.profession[Partially Relevant]base.popstra.celebrity.religion[Unrelevant]people.person.religion[Unrelevant][Retrieve Entity]',\n",
       "  'score': 0,\n",
       "  'parent': 0,\n",
       "  'topic_entity': ['m.04nw9'],\n",
       "  'context': 'base.americancomedy.double_act.comic[Partially Relevant]people.person.places_lived[Unrelevant]people.person.profession[Partially Relevant]base.popstra.celebrity.religion[Unrelevant]people.person.religion[Unrelevant]',\n",
       "  'processed_pred': '[Retrieve Relation]<paragraph>base.americancomedy.double_act.comic;people.person.places_lived;people.person.profession;base.popstra.celebrity.religion;people.person.religion</paragraph>base.americancomedy.double_act.comic[Partially Relevant]people.person.places_lived[Unrelevant]people.person.profession[Partially Relevant]base.popstra.celebrity.religion[Unrelevant]people.person.religion[Unrelevant]'},\n",
       " '2': {'prompt': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nwhat was lucille ball<|eot_id|><|start_header_id|>assistant<|end_header_id|>',\n",
       "  'pred': '[Retrieve Relation]<paragraph>people.person.place_of_birth;people.person.nationality;people.person.date_of_birth;people.profession.people_with_this_profession</paragraph>people.person.place_of_birth[Partially Relevant]people.person.nationality[Partially Relevant]people.person.date_of_birth[Unrelevant]people.profession.people_with_this_profession[Partially Relevant][Retrieve Entity]',\n",
       "  'score': 0,\n",
       "  'parent': 0,\n",
       "  'topic_entity': ['m.04nw9'],\n",
       "  'context': 'people.person.place_of_birth[Partially Relevant]people.person.nationality[Partially Relevant]people.person.date_of_birth[Unrelevant]people.profession.people_with_this_profession[Partially Relevant]',\n",
       "  'processed_pred': '[Retrieve Relation]<paragraph>people.person.place_of_birth;people.person.nationality;people.person.date_of_birth;people.profession.people_with_this_profession</paragraph>people.person.place_of_birth[Partially Relevant]people.person.nationality[Partially Relevant]people.person.date_of_birth[Unrelevant]people.profession.people_with_this_profession[Partially Relevant]'},\n",
       " '3': {'prompt': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nwhat was lucille ball<|eot_id|><|start_header_id|>assistant<|end_header_id|>[Retrieve Relation]<paragraph>base.americancomedy.double_act.comic;people.person.places_lived;people.person.profession;base.popstra.celebrity.religion;people.person.religion</paragraph>base.americancomedy.double_act.comic[Partially Relevant]people.person.places_lived[Unrelevant]people.person.profession[Partially Relevant]base.popstra.celebrity.religion[Unrelevant]people.person.religion[Unrelevant]',\n",
       "  'pred': '[Retrieve Entity]<paragraph>(Lucille Ball, base.americancomedy.double_act.comic, Desi Arnaz & Lucille Ball)</paragraph>Desi Arnaz & Lucille Ball[Partially Relevant][Partially Reasonable][Retrieve Relation]',\n",
       "  'context': 'Desi Arnaz & Lucille Ball',\n",
       "  'score': 4.481689070338065,\n",
       "  'parent': 1,\n",
       "  'topic_entity': ['m.01wnvbg'],\n",
       "  'processed_pred': '[Retrieve Entity]<paragraph>(Lucille Ball, base.americancomedy.double_act.comic, Desi Arnaz & Lucille Ball)</paragraph>Desi Arnaz & Lucille Ball[Partially Relevant][Partially Reasonable]'},\n",
       " '4': {'prompt': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nwhat was lucille ball<|eot_id|><|start_header_id|>assistant<|end_header_id|>[Retrieve Relation]<paragraph>base.americancomedy.double_act.comic;people.person.places_lived;people.person.profession;base.popstra.celebrity.religion;people.person.religion</paragraph>base.americancomedy.double_act.comic[Partially Relevant]people.person.places_lived[Unrelevant]people.person.profession[Partially Relevant]base.popstra.celebrity.religion[Unrelevant]people.person.religion[Unrelevant]',\n",
       "  'pred': '[Retrieve Entity]<paragraph>(Lucille Ball, people.person.profession, Actor);(Lucille Ball, people.person.profession, Singer);(Lucille Ball, people.person.profession, Model);(Lucille Ball, people.person.profession, Comedian);(Lucille Ball, people.person.profession, Television producer)</paragraph>Actor[Fully Relevant]Singer[Partially Relevant]Model[Partially Relevant]Comedian[Partially Relevant]Television producer[Partially Relevant][Partially Reasonable][No Retrieval]Answer: Actor;Comedian;Television producer;Singer;Model<|eot_id|>',\n",
       "  'context': '',\n",
       "  'score': 4.953032424395115,\n",
       "  'parent': 1,\n",
       "  'topic_entity': ['m.04nw9'],\n",
       "  'processed_pred': '[Retrieve Entity]<paragraph>(Lucille Ball, people.person.profession, Actor);(Lucille Ball, people.person.profession, Singer);(Lucille Ball, people.person.profession, Model);(Lucille Ball, people.person.profession, Comedian);(Lucille Ball, people.person.profession, Television producer)</paragraph>Actor[Fully Relevant]Singer[Partially Relevant]Model[Partially Relevant]Comedian[Partially Relevant]Television producer[Partially Relevant][Partially Reasonable][No Retrieval]Answer: Actor;Comedian;Television producer;Singer;Model<|eot_id|>'},\n",
       " '5': {'prompt': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nwhat was lucille ball<|eot_id|><|start_header_id|>assistant<|end_header_id|>[Retrieve Relation]<paragraph>people.person.place_of_birth;people.person.nationality;people.person.date_of_birth;people.profession.people_with_this_profession</paragraph>people.person.place_of_birth[Partially Relevant]people.person.nationality[Partially Relevant]people.person.date_of_birth[Unrelevant]people.profession.people_with_this_profession[Partially Relevant]',\n",
       "  'pred': '[Retrieve Entity]<paragraph>(Lucille Ball, people.person.place_of_birth, Jamestown)</paragraph>Jamestown[Partially Relevant][Partially Reasonable][No Retrieval]Answer: Jamestown<|eot_id|>',\n",
       "  'context': '',\n",
       "  'score': 4.481689070338065,\n",
       "  'parent': 2,\n",
       "  'topic_entity': ['m.04nw9'],\n",
       "  'processed_pred': '[Retrieve Entity]<paragraph>(Lucille Ball, people.person.place_of_birth, Jamestown)</paragraph>Jamestown[Partially Relevant][Partially Reasonable][No Retrieval]Answer: Jamestown<|eot_id|>'},\n",
       " '6': {'prompt': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nwhat was lucille ball<|eot_id|><|start_header_id|>assistant<|end_header_id|>[Retrieve Relation]<paragraph>people.person.place_of_birth;people.person.nationality;people.person.date_of_birth;people.profession.people_with_this_profession</paragraph>people.person.place_of_birth[Partially Relevant]people.person.nationality[Partially Relevant]people.person.date_of_birth[Unrelevant]people.profession.people_with_this_profession[Partially Relevant]',\n",
       "  'pred': '[Retrieve Entity]<paragraph>(Lucille Ball, people.person.nationality, United States of America)</paragraph>United States of America[Partially Relevant][Partially Reasonable][No Retrieval]Answer: United States of America<|eot_id|>',\n",
       "  'context': '',\n",
       "  'score': 4.481689070338065,\n",
       "  'parent': 2,\n",
       "  'topic_entity': ['m.04nw9'],\n",
       "  'processed_pred': '[Retrieve Entity]<paragraph>(Lucille Ball, people.person.nationality, United States of America)</paragraph>United States of America[Partially Relevant][Partially Reasonable][No Retrieval]Answer: United States of America<|eot_id|>'},\n",
       " '7': {'prompt': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nwhat was lucille ball<|eot_id|><|start_header_id|>assistant<|end_header_id|>[Retrieve Relation]<paragraph>people.person.place_of_birth;people.person.nationality;people.person.date_of_birth;people.profession.people_with_this_profession</paragraph>people.person.place_of_birth[Partially Relevant]people.person.nationality[Partially Relevant]people.person.date_of_birth[Unrelevant]people.profession.people_with_this_profession[Partially Relevant]',\n",
       "  'pred': '[Retrieve Entity]<paragraph>(Lucille Ball, people.profession.people_with_this_profession, Actor);(Lucille Ball, people.profession.people_with_this_profession, Singer);(Lucille Ball, people.profession.people_with_this_profession, Model);(Lucille Ball, people.profession.people_with_this_profession, Comedian);(Lucille Ball, people.profession.people_with_this_profession, Television producer)</paragraph>Actor[Fully Relevant]Singer[Partially Relevant]Model[Partially Relevant]Comedian[Partially Relevant]Television producer[Partially Relevant][Partially Reasonable][No Retrieval]Answer: Actor;Comedian;Television producer;Singer;Model<|eot_id|>',\n",
       "  'context': '',\n",
       "  'score': 4.953032424395115,\n",
       "  'parent': 2,\n",
       "  'topic_entity': ['m.04nw9'],\n",
       "  'processed_pred': '[Retrieve Entity]<paragraph>(Lucille Ball, people.profession.people_with_this_profession, Actor);(Lucille Ball, people.profession.people_with_this_profession, Singer);(Lucille Ball, people.profession.people_with_this_profession, Model);(Lucille Ball, people.profession.people_with_this_profession, Comedian);(Lucille Ball, people.profession.people_with_this_profession, Television producer)</paragraph>Actor[Fully Relevant]Singer[Partially Relevant]Model[Partially Relevant]Comedian[Partially Relevant]Television producer[Partially Relevant][Partially Reasonable][No Retrieval]Answer: Actor;Comedian;Television producer;Singer;Model<|eot_id|>'},\n",
       " '8': {'prompt': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nwhat was lucille ball<|eot_id|><|start_header_id|>assistant<|end_header_id|>[Retrieve Relation]<paragraph>base.americancomedy.double_act.comic;people.person.places_lived;people.person.profession;base.popstra.celebrity.religion;people.person.religion</paragraph>base.americancomedy.double_act.comic[Partially Relevant]people.person.places_lived[Unrelevant]people.person.profession[Partially Relevant]base.popstra.celebrity.religion[Unrelevant]people.person.religion[Unrelevant][Retrieve Entity]<paragraph>(Lucille Ball, base.americancomedy.double_act.comic, Desi Arnaz & Lucille Ball)</paragraph>Desi Arnaz & Lucille Ball[Partially Relevant][Partially Reasonable]',\n",
       "  'pred': '[Retrieve Relation]<paragraph>base.americancomedy.double_act.comic;base.americancomedy.double_act_comic.comic_in_double_act;base.americancomedy.double_act_straight_man.straight_man_in_double_act;base.americancomedy.double_act.straight_man;common.topic.alias</paragraph>base.americancomedy.double_act.comic[Fully Relevant]base.americancomedy.double_act_comic.comic_in_double_act[Partially Relevant]base.americancomedy.double_act_straight_man.straight_man_in_double_act[Partially Relevant]base.americancomedy.double_act.straight_man[Unrelevant]common.topic.alias[Unrelevant][Retrieve Entity]',\n",
       "  'score': 4.481689070338065,\n",
       "  'parent': 3,\n",
       "  'topic_entity': ['m.01wnvbg'],\n",
       "  'context': 'base.americancomedy.double_act.comic[Fully Relevant]base.americancomedy.double_act_comic.comic_in_double_act[Partially Relevant]base.americancomedy.double_act_straight_man.straight_man_in_double_act[Partially Relevant]base.americancomedy.double_act.straight_man[Unrelevant]common.topic.alias[Unrelevant]',\n",
       "  'processed_pred': '[Retrieve Relation]<paragraph>base.americancomedy.double_act.comic;base.americancomedy.double_act_comic.comic_in_double_act;base.americancomedy.double_act_straight_man.straight_man_in_double_act;base.americancomedy.double_act.straight_man;common.topic.alias</paragraph>base.americancomedy.double_act.comic[Fully Relevant]base.americancomedy.double_act_comic.comic_in_double_act[Partially Relevant]base.americancomedy.double_act_straight_man.straight_man_in_double_act[Partially Relevant]base.americancomedy.double_act.straight_man[Unrelevant]common.topic.alias[Unrelevant]'},\n",
       " '9': {'prompt': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nwhat was lucille ball<|eot_id|><|start_header_id|>assistant<|end_header_id|>[Retrieve Relation]<paragraph>base.americancomedy.double_act.comic;people.person.places_lived;people.person.profession;base.popstra.celebrity.religion;people.person.religion</paragraph>base.americancomedy.double_act.comic[Partially Relevant]people.person.places_lived[Unrelevant]people.person.profession[Partially Relevant]base.popstra.celebrity.religion[Unrelevant]people.person.religion[Unrelevant][Retrieve Entity]<paragraph>(Lucille Ball, base.americancomedy.double_act.comic, Desi Arnaz & Lucille Ball)</paragraph>Desi Arnaz & Lucille Ball[Partially Relevant][Partially Reasonable]',\n",
       "  'pred': '[Retrieve Relation]<paragraph>type.type.instance</paragraph>type.type.instance[Unrelevant][Retrieve Entity]',\n",
       "  'score': 4.481689070338065,\n",
       "  'parent': 3,\n",
       "  'topic_entity': ['m.01wnvbg'],\n",
       "  'context': 'type.type.instance[Unrelevant]',\n",
       "  'processed_pred': '[Retrieve Relation]<paragraph>type.type.instance</paragraph>type.type.instance[Unrelevant]'},\n",
       " '10': {'prompt': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nwhat was lucille ball<|eot_id|><|start_header_id|>assistant<|end_header_id|>[Retrieve Relation]<paragraph>base.americancomedy.double_act.comic;people.person.places_lived;people.person.profession;base.popstra.celebrity.religion;people.person.religion</paragraph>base.americancomedy.double_act.comic[Partially Relevant]people.person.places_lived[Unrelevant]people.person.profession[Partially Relevant]base.popstra.celebrity.religion[Unrelevant]people.person.religion[Unrelevant][Retrieve Entity]<paragraph>(Lucille Ball, base.americancomedy.double_act.comic, Desi Arnaz & Lucille Ball)</paragraph>Desi Arnaz & Lucille Ball[Partially Relevant][Partially Reasonable][Retrieve Relation]<paragraph>base.americancomedy.double_act.comic;base.americancomedy.double_act_comic.comic_in_double_act;base.americancomedy.double_act_straight_man.straight_man_in_double_act;base.americancomedy.double_act.straight_man;common.topic.alias</paragraph>base.americancomedy.double_act.comic[Fully Relevant]base.americancomedy.double_act_comic.comic_in_double_act[Partially Relevant]base.americancomedy.double_act_straight_man.straight_man_in_double_act[Partially Relevant]base.americancomedy.double_act.straight_man[Unrelevant]common.topic.alias[Unrelevant]',\n",
       "  'pred': '[Retrieve Entity]<paragraph>(Desi Arnaz & Lucille Ball, base.americancomedy.double_act.comic, Lucille Ball)</paragraph>Lucille Ball[Fully Relevant][Unreasonable][No Retrieval]Answer: Lucille Désirée Ball;Desiderio Arnaz<|eot_id|>',\n",
       "  'context': '',\n",
       "  'score': 5.935372584634358,\n",
       "  'parent': 8,\n",
       "  'topic_entity': ['m.01wnvbg'],\n",
       "  'processed_pred': '[Retrieve Entity]<paragraph>(Desi Arnaz & Lucille Ball, base.americancomedy.double_act.comic, Lucille Ball)</paragraph>Lucille Ball[Fully Relevant][Unreasonable][No Retrieval]Answer: Lucille Désirée Ball;Desiderio Arnaz<|eot_id|>'},\n",
       " '11': {'prompt': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nwhat was lucille ball<|eot_id|><|start_header_id|>assistant<|end_header_id|>[Retrieve Relation]<paragraph>base.americancomedy.double_act.comic;people.person.places_lived;people.person.profession;base.popstra.celebrity.religion;people.person.religion</paragraph>base.americancomedy.double_act.comic[Partially Relevant]people.person.places_lived[Unrelevant]people.person.profession[Partially Relevant]base.popstra.celebrity.religion[Unrelevant]people.person.religion[Unrelevant][Retrieve Entity]<paragraph>(Lucille Ball, base.americancomedy.double_act.comic, Desi Arnaz & Lucille Ball)</paragraph>Desi Arnaz & Lucille Ball[Partially Relevant][Partially Reasonable][Retrieve Relation]<paragraph>base.americancomedy.double_act.comic;base.americancomedy.double_act_comic.comic_in_double_act;base.americancomedy.double_act_straight_man.straight_man_in_double_act;base.americancomedy.double_act.straight_man;common.topic.alias</paragraph>base.americancomedy.double_act.comic[Fully Relevant]base.americancomedy.double_act_comic.comic_in_double_act[Partially Relevant]base.americancomedy.double_act_straight_man.straight_man_in_double_act[Partially Relevant]base.americancomedy.double_act.straight_man[Unrelevant]common.topic.alias[Unrelevant]',\n",
       "  'pred': '[Retrieve Entity]<paragraph>(Desi Arnaz & Lucille Ball, base.americancomedy.double_act_comic.comic_in_double_act, Lucille Ball)</paragraph>Lucille Ball[Fully Relevant][Unreasonable][No Retrieval]Answer: Lucille Désirée Ball;Desiderio Arnaz<|eot_id|>',\n",
       "  'context': '',\n",
       "  'score': 4.481689070338065,\n",
       "  'parent': 8,\n",
       "  'topic_entity': ['m.01wnvbg'],\n",
       "  'processed_pred': '[Retrieve Entity]<paragraph>(Desi Arnaz & Lucille Ball, base.americancomedy.double_act_comic.comic_in_double_act, Lucille Ball)</paragraph>Lucille Ball[Fully Relevant][Unreasonable][No Retrieval]Answer: Lucille Désirée Ball;Desiderio Arnaz<|eot_id|>'},\n",
       " '12': {'prompt': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nwhat was lucille ball<|eot_id|><|start_header_id|>assistant<|end_header_id|>[Retrieve Relation]<paragraph>base.americancomedy.double_act.comic;people.person.places_lived;people.person.profession;base.popstra.celebrity.religion;people.person.religion</paragraph>base.americancomedy.double_act.comic[Partially Relevant]people.person.places_lived[Unrelevant]people.person.profession[Partially Relevant]base.popstra.celebrity.religion[Unrelevant]people.person.religion[Unrelevant][Retrieve Entity]<paragraph>(Lucille Ball, base.americancomedy.double_act.comic, Desi Arnaz & Lucille Ball)</paragraph>Desi Arnaz & Lucille Ball[Partially Relevant][Partially Reasonable][Retrieve Relation]<paragraph>base.americancomedy.double_act.comic;base.americancomedy.double_act_comic.comic_in_double_act;base.americancomedy.double_act_straight_man.straight_man_in_double_act;base.americancomedy.double_act.straight_man;common.topic.alias</paragraph>base.americancomedy.double_act.comic[Fully Relevant]base.americancomedy.double_act_comic.comic_in_double_act[Partially Relevant]base.americancomedy.double_act_straight_man.straight_man_in_double_act[Partially Relevant]base.americancomedy.double_act.straight_man[Unrelevant]common.topic.alias[Unrelevant]',\n",
       "  'pred': '[Retrieve Entity]<paragraph>(Desi Arnaz & Lucille Ball, base.americancomedy.double_act_straight_man.straight_man_in_double_act, Desi Arnaz)</paragraph>Desi Arnaz[Partially Relevant][Unreasonable][No Retrieval]Answer: Comedian;Actor;Television producer;Singer;Model<|eot_id|>',\n",
       "  'context': '',\n",
       "  'score': 3.5999854493985555,\n",
       "  'parent': 8,\n",
       "  'topic_entity': ['m.01wnvbg'],\n",
       "  'processed_pred': '[Retrieve Entity]<paragraph>(Desi Arnaz & Lucille Ball, base.americancomedy.double_act_straight_man.straight_man_in_double_act, Desi Arnaz)</paragraph>Desi Arnaz[Partially Relevant][Unreasonable][No Retrieval]Answer: Comedian;Actor;Television producer;Singer;Model<|eot_id|>'}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(labels)\n",
    "logging_res[106]['tree']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_nodes = []\n",
    "for n_ind, node in prediction_tree.values():\n",
    "    if 'Answer' in node['processed_pred']:\n",
    "        end_nodes.append(n_ind)\n",
    "for n_ind, node in end_nodes:\n",
    "    while node != 0:\n",
    "        parent = prediction_tree[node]['parent']\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [None, 0, 2, 3], 1: [None, 0, 2, 4], 2: [None, 0, 1, 5]}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def backtracking_prediction_tree(levels: dict[int,list[int]], curr_depth: int, prediction_tree: dict[int, dict]) -> dict[int,list[int]]:\n",
    "    '''\n",
    "    get best tracking from prediction_tree base on levels\n",
    "    '''\n",
    "    parent = 0 \n",
    "    best_selections = {}\n",
    "    # Traverse from the bottom \n",
    "    levels = {k: v for k, v in levels.items() if len(v) > 0 and k != 0} # remove empty list in levels\n",
    "    \n",
    "    for path_i, node in enumerate(levels[len(levels)]): # beam search \n",
    "        if node == 0:\n",
    "            break\n",
    "        best_selections[path_i] = [node] \n",
    "        current_node = node \n",
    "        current_level = curr_depth \n",
    "        if current_node is None:\n",
    "            continue\n",
    "        while current_level > 0 and current_node is not None:\n",
    "            parent = prediction_tree[current_node][\"parent\"]\n",
    "            best_selections[path_i] = [parent] + best_selections[path_i] \n",
    "            current_node = parent \n",
    "            current_level -= 1\n",
    "    return best_selections\n",
    "backtracking_prediction_tree(levels, 5, prediction_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_eval_metric(best_pred, preds, answers):\n",
    "    correct, total = 0.0, 0.0\n",
    "    for entity in preds:\n",
    "        if entity in answers:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "    if len(answers) == 0:\n",
    "        if total == 0:\n",
    "            return 1.0, 1.0, 1.0, 1.0 # precision, recall, f1, hits\n",
    "        else:\n",
    "            return 0.0, 1.0, 0.0, 1.0 # precision, recall, f1, hits\n",
    "    else:\n",
    "        hits = float(best_pred in answers)\n",
    "        if total == 0:\n",
    "            return 1.0, 0.0, 0.0, hits # precision, recall, f1, hits\n",
    "        else:\n",
    "            precision, recall = correct / total, correct / len(answers)\n",
    "            f1 = 2.0 / (1.0 / precision + 1.0 / recall) if precision != 0 and recall != 0 else 0.0\n",
    "            return precision, recall, f1, hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_eval_metric(preds, answers):\n",
    "    correct, total = 0.0, 0.0\n",
    "    for entity in preds:\n",
    "        if entity in answers:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "    # if len(answers) == 0:\n",
    "    #     if total == 0:\n",
    "    #         return 1.0, 1.0, 1.0, 1.0 # precision, recall, f1, hits\n",
    "    #     else:\n",
    "    #         return 0.0, 1.0, 0.0, 1.0 # precision, recall, f1, hits\n",
    "    # else:\n",
    "    if total != 0:\n",
    "        hits = 1\n",
    "    else: \n",
    "        hits = 0\n",
    "    if total == 0:\n",
    "        return 1.0, 0.0, 0.0, hits # precision, recall, f1, hits\n",
    "    else:\n",
    "        precision, recall = correct / total, correct / len(answers)\n",
    "        f1 = 2.0 / (1.0 / precision + 1.0 / recall) if precision != 0 and recall != 0 else 0.0\n",
    "        return precision, recall, f1, hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_eval_metric(preds, answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "correct_ids = []\n",
    "logging_res = []\n",
    "for index in range(0, len(test_data)):\n",
    "# index = 483\n",
    "    hit = 0\n",
    "    print(f'Process {index}')\n",
    "    data_input = test_data[index]['question']\n",
    "    prompt = PROMPT_DICT['llama3'].format(input= data_input)\n",
    "    max_depth = 5\n",
    "    # topic_entity = list(test_data[index]['topic_entity'].keys())\n",
    "    topic_entity = list(test_data[index]['gold_entity_map'].keys())\n",
    "    # pred = model.generate([prompt], sampling_params)[0]\n",
    "    # pred_text = pred.outputs[0].text\n",
    "    # if '[New Retrieval]' in pred_text:\n",
    "    curr_depth = 1\n",
    "    terminated = False\n",
    "    node_id = 0\n",
    "    prediction_tree = {}\n",
    "    levels = {}\n",
    "    prediction_tree[node_id] = {\"prompt\": prompt, \"pred\": \"[New Retrieval]\",\n",
    "                                \"processed_pred\": \"\", \"score\": None, \"topic_entity\": topic_entity, \"parent\": None, \"context\": ''}\n",
    "    levels[0] = [0]\n",
    "    while curr_depth < max_depth:\n",
    "        levels[curr_depth] = []\n",
    "        if curr_depth-1 in levels:\n",
    "            for node in levels[curr_depth-1]:\n",
    "                curr_pred = prediction_tree[node][\"pred\"]\n",
    "                if \"<|eot_id|>\" in curr_pred:\n",
    "                    continue\n",
    "                prompt = prediction_tree[node][\"prompt\"]\n",
    "                prev_generation = prediction_tree[node][\"processed_pred\"]\n",
    "                score = prediction_tree[node][\"score\"]\n",
    "                topic_entity = prediction_tree[node][\"topic_entity\"]\n",
    "                context = prediction_tree[node]['context']\n",
    "                cur_prompt = prompt + prev_generation\n",
    "                if \"Retrieve Entity\" in curr_pred.split('[')[-1]:\n",
    "                    retrieval_results = {}\n",
    "                    preds, scores, next_entities, contexts = run_entity_generation_batch(\n",
    "                        model, cur_prompt, topic_entity, context)\n",
    "                    for i, (pred, p_score,next_topic, context) in enumerate(zip(preds, scores, next_entities, contexts)):\n",
    "                        retrieval_results[i] = {\n",
    "                            \"pred\": pred, \"score\": p_score, \"next_topic\": next_topic, \"context\": context}\n",
    "\n",
    "                    for i, result in retrieval_results.items():\n",
    "                        node_id += 1\n",
    "                        node_score = result[\"score\"] * \\\n",
    "                            score if score is not None else result[\"score\"]\n",
    "                        pred = result[\"pred\"]\n",
    "                        next_entity = result['next_topic']\n",
    "                        if len(next_entity) == 0:\n",
    "                            next_entity = topic_entity\n",
    "                        prediction_tree[node_id] = {\"prompt\": cur_prompt, \"pred\": pred, \"context\": result['context'],\n",
    "                                                    \"score\": node_score, \"parent\": node,\n",
    "                                                    \"topic_entity\": next_entity}\n",
    "                        if \"[Continue to Retrieve Evidence]\" in pred:\n",
    "                            gen_result_index = pred.index(\"[Continue to Retrieve Evidence]\")\n",
    "                            prev_generation = pred[:gen_result_index]\n",
    "                        else:\n",
    "                            prev_generation = pred\n",
    "                        prediction_tree[node_id][\"processed_pred\"] = prev_generation\n",
    "                        levels[curr_depth].append(node_id)\n",
    "                #存在前后逻辑粘连   \n",
    "                if \"New Retrieval\" in curr_pred.split('[')[-1] or \"Continue to Retrieve Evidence\" in curr_pred.split('[')[-1]:\n",
    "                    retrieval_results = {}\n",
    "                    preds, scores, contexts = run_relation_generation_batch(\n",
    "                        model, cur_prompt, new_retrieval=True if (\"[New Retrieval]\" in curr_pred) else False, context=context, topic_entity=topic_entity, hypo=True)\n",
    "                    for i, (pred, p_score, context) in enumerate(zip(preds, scores, contexts)):\n",
    "                        retrieval_results[i] = {\n",
    "                            \"pred\": pred, \"score\": p_score, \"context\": context}\n",
    "\n",
    "                    for i, result in retrieval_results.items():\n",
    "                        node_id += 1\n",
    "                        node_score = result[\"score\"] * \\\n",
    "                            score if score is not None else result[\"score\"]\n",
    "                        pred = result[\"pred\"]\n",
    "                        context = result[\"context\"]\n",
    "                        prediction_tree[node_id] = {\"prompt\": cur_prompt, \"pred\": pred,\n",
    "                                                    \"score\": node_score, \"parent\": node,\n",
    "                                                    \"topic_entity\": topic_entity, \"context\": context}\n",
    "                        if \"[Retrieve Entity]\" in pred:\n",
    "                            gen_result_index = pred.index(\"[Retrieve Entity]\")\n",
    "                            prev_generation = pred[:gen_result_index]\n",
    "                        else:\n",
    "                            prev_generation = pred\n",
    "                        prediction_tree[node_id][\"processed_pred\"] = prev_generation\n",
    "                        levels[curr_depth].append(node_id)\n",
    "            current_rank = levels[curr_depth]\n",
    "            node2score = {\n",
    "                node_id: prediction_tree[node_id][\"score\"] for node_id in current_rank}\n",
    "            top_nodes = sorted(node2score.items(), key=lambda x: x[1], reverse=True)[\n",
    "                :3]\n",
    "            levels[curr_depth] = [node[0] for node in top_nodes]\n",
    "            curr_depth += 1\n",
    "        else:\n",
    "            break\n",
    "    labels = [get_label(ans) if ans.startswith('m.') else ans for ans in test_data[index]['answer']]\n",
    "    # labels = [ans['entity_name'] for ans in test_data[index]['answer']]\n",
    "    # print(labels)\n",
    "    for tree_node in prediction_tree.values():\n",
    "        if 'Answer' in tree_node['processed_pred']:\n",
    "            answer = tree_node['processed_pred'].split('Answer:')[-1]\n",
    "            for label in labels:\n",
    "                if label and label in answer:\n",
    "                    hit = 1\n",
    "    if hit == 1:\n",
    "        print('Correct')\n",
    "        count += 1\n",
    "        correct_ids.append(index)\n",
    "        break\n",
    "    # else:\n",
    "        # break\n",
    "\n",
    "        # logging_res.append({\"index\": index, \"tree\": prediction_tree})\n",
    "    # if len(logging_res) == 20:\n",
    "        # save_to_json(logging_res, './output/inference/cwq_test_res_1217.json')\n",
    "    #     logging_res = []\n",
    "#1217修改了relation-1hop,修改了score\n",
    "# save_to_json(logging_res, './output/inference/cwq_test_res_1217.json')\n",
    "    #注意有value标签, e.g. WebQTest-31\n",
    "    # for label in labels:\n",
    "    #     if label and label in prediction_tree[len(prediction_tree)-1]['processed_pred']:\n",
    "    #         count += 1\n",
    "    #         break\n",
    "    # except:\n",
    "    #     print(f'{index} Error')\n",
    "    #     continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('./output/inference/cwq_test_res_1217.json', 'r', encoding='utf-8') as f:\n",
    "    cwq_result = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, error in enumerate(result):\n",
    "    if error['index'] == 14:\n",
    "        print(i, error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwq_result[18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_another_entity('m.0y496z9', 'location.location.time_zones')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "self-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
