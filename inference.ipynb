{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1736929/2109291646.py:13: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n",
      "/media/disk1/chatgpt/miniconda3/envs/self-rag/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain.embeddings import (\n",
    "    HuggingFaceEmbeddings,\n",
    ")\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "                model_name=\"BAAI/bge-large-en-v1.5\",\n",
    "                # model_kwargs={'device': 'cpu'},\n",
    "                encode_kwargs={'normalize_embeddings': False},\n",
    "            )\n",
    "from langchain.storage import LocalFileStore, RedisStore\n",
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "store = RedisStore(redis_url=\"redis://localhost:6379\")\n",
    "cached_embedder = CacheBackedEmbeddings.from_bytes_store(\n",
    "embeddings, store, namespace=\"bge-large\"\n",
    ")\n",
    "row_string = []\n",
    "with open('./data/clean_relations', 'r') as f:\n",
    "    r_data = [line.strip() for line in f]\n",
    "all_db = FAISS.from_texts(r_data, cached_embedder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run_long_form answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-20 07:17:10 cuda.py:22] You are using a deprecated `pynvml` package. Please install `nvidia-ml-py` instead, and make sure to uninstall `pynvml`. When both of them are installed, `pynvml` will take precedence and cause errors. See https://pypi.org/project/pynvml for more information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 07:17:10,546\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-20 07:17:14 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='/media/disk2/llama_factory/generation_0202_uti_no_mask', speculative_config=None, tokenizer='/media/disk2/llama_factory/generation_0202_uti_no_mask', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/media/disk2/llama_factory/generation_0202_uti_no_mask, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "INFO 05-20 07:17:15 model_runner.py:1056] Starting to load model /media/disk2/llama_factory/generation_0202_uti_no_mask...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:00,  4.56it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.59it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.26it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.16it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.29it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-20 07:17:19 model_runner.py:1067] Loading model weights took 14.9605 GB\n",
      "INFO 05-20 07:17:20 gpu_executor.py:122] # GPU blocks: 9656, # CPU blocks: 2048\n",
      "INFO 05-20 07:17:20 gpu_executor.py:126] Maximum concurrency for 8192 tokens per request: 18.86x\n",
      "INFO 05-20 07:17:21 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 05-20 07:17:21 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 05-20 07:17:39 model_runner.py:1523] Graph capturing finished in 18 secs.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "model = LLM(model='/media/disk2/llama_factory/generation_0202_uti_no_mask', trust_remote_code=True, tensor_parallel_size=1)\n",
    "tokenizer = AutoTokenizer.from_pretrained('/media/disk2/llama_factory/generation_0202_uti_no_mask')\n",
    "sampling_params = SamplingParams(\n",
    "            temperature=0.0, top_p=1.0,max_tokens=100, logprobs=10, skip_special_tokens=False, include_stop_str_in_output=True)\n",
    "PROMPT_DICT = {\"llama3\": '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n{input}<|eot_id|><|start_header_id|>assistant<|end_header_id|>', \"llama2\": \"<s>[INST] {input} [/INST]\"}\n",
    "test = model.generate([PROMPT_DICT[\"llama3\"].format(input=\"你好\")], sampling_params)[0].outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_special_tokens(tokenizer, use_grounding=False, use_utility=False):\n",
    "    rel_tokens = {}\n",
    "    for token in ['[Unrelevant]','[Partially Relevant]','[Fully Relevant]']:\n",
    "        rel_tokens[token] = tokenizer.convert_tokens_to_ids(token)\n",
    "    reason_tokens = {}\n",
    "    for token in ['[Fully Reasonable]', '[Partially Reasonable]', '[Unreasonable]']:\n",
    "        reason_tokens[token] = tokenizer.convert_tokens_to_ids(token)\n",
    "    ut_tokens = {}\n",
    "    for token in ['[Utility:5]','[Utility:4]','[Utility:3]','[Utility:2]','[Utility:1]']:\n",
    "        ut_tokens[token] = tokenizer.convert_tokens_to_ids(token)\n",
    "\n",
    "    return rel_tokens, reason_tokens, ut_tokens\n",
    "rel_tokens, reason_tokens, utility_tokens = load_special_tokens(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "with open('/media/disk1/chatgpt/zh/ToG/data/graliqa.json', 'r',encoding='utf-8') as f:\n",
    "    test_data = json.load(f)\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1639\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('./data/merged/WebQSP_test.json', 'r',encoding='utf-8') as f:\n",
    "    test_data = json.load(f)\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3531\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('./data/merged/CWQ_test.json', 'r', encoding='utf-8') as f:\n",
    "    test_data = json.load(f)\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import random\n",
    "def save_to_json(data: List, data_path=''):\n",
    "    if not os.path.isfile(data_path):\n",
    "        # 文件不存在，创建新列表并写入文件\n",
    "        with open(data_path, 'w', encoding='utf-8') as file:\n",
    "            json.dump(data, file, ensure_ascii=False, indent=4)\n",
    "        return\n",
    "    try:\n",
    "        # 尝试读取现有文件\n",
    "        with open(data_path, 'r', encoding='utf-8') as file:\n",
    "            # 加载现有的JSON数据\n",
    "            existing_data = json.load(file)\n",
    "            existing_data.extend(data)\n",
    "    except json.JSONDecodeError:\n",
    "        # 文件不是有效的JSON，打印错误信息并退出\n",
    "        print(f\"文件 {data_path} 不是有效的JSON格式。\")\n",
    "        return\n",
    "    except ValueError as e:\n",
    "        # 打印错误信息并退出\n",
    "        print(e)\n",
    "        return\n",
    "    # 将更新后的数据写回文件\n",
    "    with open(data_path, 'w', encoding='utf-8') as file:\n",
    "        json.dump(existing_data, file, ensure_ascii=False, indent=4)\n",
    "def random_sample(lst, k=3):\n",
    "    return random.sample(lst, min(k, len(lst)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from src.sparql_utils import *\n",
    "import random\n",
    "#维护一个relation 池\n",
    "def run_relation_generation_batch(model, prompt, llama, context, topic_entity, hypo=True, use_1hop=True, income=None):\n",
    "    # 初始化关系得分字典、最终预测列表、总体得分列表、最终上下文列表、段落集合\n",
    "    final_preds = []\n",
    "    overall_scores = []\n",
    "    final_contexts = []\n",
    "    paragraph = set()\n",
    "    # 如果是新检索，则设置检索标记为\"[Retrieve Relation]\"，否则设置为\"[Retrieve Relation]\"\n",
    "    \n",
    "    retrieval_token = \"[Retrieve Relation]\"\n",
    "    # 如果使用1跳关系，则获取1跳关系，否则使用所有关系\n",
    "    if use_1hop:\n",
    "        candidate_relations = set()\n",
    "        for entity in topic_entity:\n",
    "            try:\n",
    "                candidate_relations.update(get_1hop_relations_with_odbc(entity))\n",
    "            except:\n",
    "                continue\n",
    "        if len(list(candidate_relations)):\n",
    "            vec_db = FAISS.from_texts(list(candidate_relations), cached_embedder)\n",
    "        else:\n",
    "            vec_db = all_db\n",
    "        retriever = vec_db.as_retriever(search_kwargs={\"k\": 10})\n",
    "    else:\n",
    "        retriever = all_db.as_retriever(search_kwargs={\"k\": 10})\n",
    "    # 获取段落内容\n",
    "    try:\n",
    "        if llama == 'llama3':\n",
    "            paragraph.update([page.page_content.strip() for page in retriever.invoke(prompt.split('\\n\\n')[1].split('<|eot_id|>')[0] + ' '+ context)])\n",
    "        else:\n",
    "            paragraph.update([page.page_content.strip() for page in retriever.invoke(prompt.split('[INST]')[1].split('[/INST]')[0].strip() + ' '+ context)])\n",
    "    except:\n",
    "        pass\n",
    "    # 如果使用假设，则生成假设关系\n",
    "    if hypo:\n",
    "        hypo_rel = model.generate(prompt + retrieval_token, sampling_params)[0].outputs[0].text\n",
    "        pattern = r'(\\w+\\.\\w+\\.\\w+)\\[(.*?)\\]'\n",
    "        if '[Retrieve Entity]' in hypo_rel:\n",
    "            hypo_rel = hypo_rel.split('[Retrieve Entity]')[0]\n",
    "        matches =  dict(re.findall(pattern, hypo_rel))\n",
    "        string = ''\n",
    "        for k,v in matches.items():\n",
    "            if v in ['Fully Relevant', 'Partially Relevant']:\n",
    "                string += k + ' '\n",
    "        paragraph.update([page.page_content.strip() for page in retriever.invoke(string)])\n",
    "    paragraph.discard(income)\n",
    "    aug_prompts =  [\"<paragraph>{}</paragraph>\".format(';'.join(p))  for p in [list(paragraph)[i: i+5] for i in range(0, len(paragraph), 5)]]\n",
    "    \n",
    "    preds = model.generate([prompt + retrieval_token + aug for aug in aug_prompts], sampling_params)\n",
    "    for p_id, pred in enumerate(preds):\n",
    "        score_dict = dict()\n",
    "        pred_token_ids = pred.outputs[0].token_ids\n",
    "        pred_text_1 = pred.outputs[0].text\n",
    "        pred_log_probs = pred.outputs[0].logprobs\n",
    "        seq_score = pred.outputs[0].cumulative_logprob / \\\n",
    "            max(len(pred.outputs[0].token_ids), 1)\n",
    "        if '[Retrieve Entity]' in pred_text_1:\n",
    "            processed_pred = pred_text_1.split('[Retrieve Entity]')[0] + '[Retrieve Entity]'\n",
    "        else:\n",
    "            processed_pred = pred_text_1\n",
    "\n",
    "        context_pattern = r'(\\w+\\.\\w+\\.\\w+)\\[(.*?)\\]'\n",
    "        context_matches = re.findall(context_pattern, pred_text_1.split('[Retrieve Entity]')[0])\n",
    "        relevance_indices = []\n",
    "        for tok_idx, tok in enumerate(pred_token_ids):\n",
    "            if tok in rel_tokens.values():\n",
    "                relevance_indices.append(tok_idx)\n",
    "        if len(relevance_indices) > 0:\n",
    "            for i, idx in enumerate(relevance_indices[:len(context_matches)]):\n",
    "                rel_score_dict = {}\n",
    "                for token, token_id in rel_tokens.items():\n",
    "                    prob = pred_log_probs[idx][token_id].logprob if token_id in pred_log_probs[idx] else -100\n",
    "                    rel_score_dict[token] = np.exp(prob)\n",
    "                score_dict[context_matches[i][0]] = {\"relevance\":context_matches[i][1],  \"score\":(rel_score_dict['[Fully Relevant]']+ 0.5 * rel_score_dict['[Partially Relevant]']) / np.sum(list(rel_score_dict.values()))}\n",
    "        \n",
    "        final_preds.append(retrieval_token + aug_prompts[p_id] + processed_pred)\n",
    "        overall_scores.append(0)\n",
    "        final_contexts.append(score_dict)\n",
    "\n",
    "    return final_preds, overall_scores, final_contexts\n",
    "\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def run_entity_generation_batch(model, prompt, topic_entities, context):\n",
    "    # 初始化变量\n",
    "    final_predictions, final_entities, final_contexts = [], [], []\n",
    "    entity_prompts, income_rel = [], []\n",
    "    overall_scores, name_to_id = {}, {}\n",
    "    effective_count = 0\n",
    "    context_pattern = r'(.+?)\\[(.*?)\\]'\n",
    "    # 构建实体提示\n",
    "    for entity in topic_entities:\n",
    "        for key, relevance in context.items():\n",
    "            if relevance['relevance'] in ['Fully Relevant', 'Partially Relevant']:\n",
    "                try:\n",
    "                    related_entities = get_another_entity(entity, key, return_label=True)\n",
    "                except Exception:\n",
    "                    related_entities = {}\n",
    "\n",
    "                if related_entities:\n",
    "                    # 初始化分数和实体映射\n",
    "                    income_rel.append(key)\n",
    "                    name_to_id[effective_count] = related_entities\n",
    "                    overall_scores[effective_count] = {\n",
    "                        'r_relevance': relevance['score']\n",
    "                    }\n",
    "                    # 构造实体提示\n",
    "                    entities = [\n",
    "                        f'({get_label(entity)}, {key}, {rel_entity})'\n",
    "                        for rel_entity in related_entities.keys()\n",
    "                    ]\n",
    "                    overall_scores[effective_count]['path'] = entities[:5]\n",
    "                    entity_prompts.append(\n",
    "                        f\"<paragraph>{';'.join(entities[:5])}</paragraph>\"\n",
    "                    )\n",
    "                    effective_count += 1\n",
    "\n",
    "    # 生成模型预测\n",
    "    augmented_prompts = [\n",
    "        f\"{prompt}[Retrieve Entity]{entity_prompt}\"\n",
    "        for entity_prompt in entity_prompts\n",
    "    ]\n",
    "    predictions = model.generate(augmented_prompts, sampling_params)\n",
    "\n",
    "    # 解析预测结果\n",
    "    for idx, prediction in enumerate(predictions):  \n",
    "        pred_output = prediction.outputs[0]\n",
    "        pred_text = pred_output.text\n",
    "        pred_log_probs = pred_output.logprobs\n",
    "        pred_token_ids = pred_output.token_ids\n",
    "        seq_score = pred_output.cumulative_logprob / max(len(pred_token_ids), 1)\n",
    "\n",
    "        processed_pred, rel_score, reason_score, utility_score, return_entities, matches, rationality, utility = process_prediction(\n",
    "            idx, pred_text, pred_log_probs, pred_token_ids, context_pattern, name_to_id, overall_scores\n",
    "        )\n",
    "\n",
    "        # 更新分数和最终结果\n",
    "        overall_scores[idx].update({\n",
    "            \"r_match\": {income_rel[idx]: context[income_rel[idx]]},\n",
    "            \"e_match\": matches,\n",
    "            \"reason_score\": reason_score,\n",
    "            \"rationality\": rationality, \n",
    "            \"utility\": utility,\n",
    "            \"utility_score\": utility_score,\n",
    "            \"seq_score\": seq_score,\n",
    "            \"final_score\":  rel_score * overall_scores[idx]['r_relevance'] * reason_score\n",
    "        })\n",
    "\n",
    "        final_predictions.append(f\"[Retrieve Entity]{entity_prompts[idx]}{processed_pred}\")\n",
    "        final_entities.append(list(return_entities.values()))\n",
    "        final_contexts.append(' '.join(return_entities.keys()))\n",
    "\n",
    "    # 返回最终结果\n",
    "    return final_predictions, [\n",
    "        overall_scores[idx]['final_score'] for idx in overall_scores\n",
    "    ], final_entities, final_contexts, overall_scores, income_rel\n",
    "\n",
    "\n",
    "def process_prediction(idx, pred_text, pred_log_probs, pred_token_ids,pattern, name_to_id, overall_scores):\n",
    "    \"\"\"处理单条预测结果，计算相关性分数和返回实体。\"\"\"\n",
    "    rel_score = 0\n",
    "    reason_score = 0\n",
    "    utility_score = 0\n",
    "    rationality = ''\n",
    "    utility = ''\n",
    "    return_entities = {}\n",
    "    count = 0\n",
    "    relevance_indices = []\n",
    "    reason_indices = []\n",
    "    utility_indices = []\n",
    "    \n",
    "    if '[Retrieve Relation]' in pred_text:\n",
    "        processed_pred = pred_text.split('[Retrieve Relation]')[0].strip()\n",
    "        matches = dict(re.findall(pattern, processed_pred))\n",
    "        for key, relevance in matches.items():\n",
    "            if relevance.strip() in ['Fully Relevant', 'Partially Relevant']:\n",
    "                if key in name_to_id[idx]:\n",
    "                    return_entities[key] = name_to_id[idx][key]\n",
    "                elif key.strip() == 'Unknown Entity':\n",
    "                    # random_key = random.choice(list(name_to_id[idx].keys()))\n",
    "                    random_keys = random_sample(list(name_to_id[idx].keys()))\n",
    "                    for random_key in random_keys:\n",
    "                        return_entities[random_key] = name_to_id[idx][random_key]\n",
    "        processed_pred += '[Retrieve Relation]'\n",
    "\n",
    "    elif '[No Retrieval]' in pred_text:\n",
    "        processed_pred = pred_text\n",
    "        matches = dict(re.findall(pattern, pred_text.split('[No Retrieval]')[0]))\n",
    "        for tok_idx, tok in enumerate(pred_token_ids):\n",
    "            if tok in utility_tokens.values():\n",
    "                utility_indices.append(tok_idx)\n",
    "        if len(utility_indices):\n",
    "            uti_score_dict = {}\n",
    "            for token, token_id in utility_tokens.items():\n",
    "                prob = pred_log_probs[utility_indices[0]][token_id].logprob if token_id in pred_log_probs[utility_indices[0]] else -100\n",
    "                uti_score_dict[token] = np.exp(prob)\n",
    "            if len(uti_score_dict) == 5:\n",
    "                ut_sum = np.sum(list(uti_score_dict.values()))\n",
    "                ut_scores = [-1, -0.5, 0, 0.5, 1]\n",
    "                utility_score = np.sum([ut_scores[i] * (uti_score_dict[\"[Utility:{}]\".format(i+1)] / ut_sum)\n",
    "                                    if \"[Utility:{}]\".format(i+1) in uti_score_dict else 0.0 for i in range(0, 5)])\n",
    "            utility = tokenizer.convert_ids_to_tokens(pred_token_ids[utility_indices[0]])\n",
    "    else:\n",
    "        matches = dict()\n",
    "        processed_pred = '[No Retrieval]Answer: None'\n",
    "        rel_score = 0\n",
    "    for tok_idx, tok in enumerate(pred_token_ids):\n",
    "        if tok in rel_tokens.values():\n",
    "            relevance_indices.append(tok_idx)\n",
    "        if tok in reason_tokens.values():\n",
    "            reason_indices.append(tok_idx)\n",
    "    if len(relevance_indices) > 0:\n",
    "        for i, idx in enumerate(relevance_indices[:len(matches)]):\n",
    "            rel_score_dict = {}\n",
    "            for token, token_id in rel_tokens.items():\n",
    "                prob = pred_log_probs[idx][token_id].logprob if token_id in pred_log_probs[idx] else -100\n",
    "                rel_score_dict[token] = np.exp(prob)\n",
    "            if len(rel_score_dict) == 3:\n",
    "                rl_sum =  np.sum(list(rel_score_dict.values()))\n",
    "            rel_score = max((rel_score_dict['[Fully Relevant]'] / rl_sum)+ 0.5* (rel_score_dict['[Partially Relevant]'] / rl_sum), rel_score)\n",
    "    if len(reason_indices): \n",
    "        reason_score_dict = {}\n",
    "        for token, token_id in reason_tokens.items():\n",
    "            prob = pred_log_probs[reason_indices[0]][token_id].logprob if token_id in pred_log_probs[reason_indices[0]] else -100\n",
    "            reason_score_dict[token] = np.exp(prob)\n",
    "        if len(reason_score_dict) == 3:\n",
    "            rs_sum = np.sum(list(reason_score_dict.values()))\n",
    "            reason_score = (reason_score_dict['[Fully Reasonable]'] / rs_sum) + 0.5 * (reason_score_dict['[Partially Reasonable]'] / rs_sum)\n",
    "            rationality = tokenizer.convert_ids_to_tokens(pred_token_ids[reason_indices[0]])\n",
    "    return processed_pred, rel_score, reason_score, utility_score, return_entities, matches, rationality, utility\n",
    " \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_eval_metric(preds, answers):\n",
    "    correct, total = 0.0, 0.0\n",
    "    for entity in preds:\n",
    "        if entity in answers:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "    if len(answers) == 0:\n",
    "        if total == 0:\n",
    "            return 1.0, 1.0, 1.0, 1.0 # precision, recall, f1, hits\n",
    "        else:\n",
    "            return 0.0, 1.0, 0.0, 1.0 # precision, recall, f1, hits\n",
    "    if correct != 0:\n",
    "        hits = 1\n",
    "    else: \n",
    "        hits = 0\n",
    "    if total == 0:\n",
    "        return 1.0, 0.0, 0.0, hits # precision, recall, f1, hits\n",
    "    else:\n",
    "        precision, recall = correct / total, correct / len(answers)\n",
    "        f1 = 2.0 / (1.0 / precision + 1.0 / recall) if precision != 0 and recall != 0 else 0.0\n",
    "        return precision, recall, f1, hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.54s/it, est. speed input: 13.64 toks/s, output: 64.97 toks/s]\n",
      "Processed prompts: 100%|██████████| 3/3 [00:01<00:00,  1.96it/s, est. speed input: 101.20 toks/s, output: 142.98 toks/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:01<00:00,  1.32it/s, est. speed input: 258.89 toks/s, output: 74.82 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.52s/it, est. speed input: 85.24 toks/s, output: 66.08 toks/s]\n",
      "Processed prompts: 0it [00:00, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.52s/it, est. speed input: 87.96 toks/s, output: 65.64 toks/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:01<00:00,  1.30it/s, est. speed input: 208.16 toks/s, output: 111.23 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.47s/it, est. speed input: 110.06 toks/s, output: 67.94 toks/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:01<00:00,  1.44it/s, est. speed input: 270.97 toks/s, output: 105.21 toks/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00,  4.83it/s, est. speed input: 1172.49 toks/s, output: 104.16 toks/s]\n",
      "Processed prompts: 100%|██████████| 3/3 [00:00<00:00, 12.06it/s, est. speed input: 2264.39 toks/s, output: 144.78 toks/s]\n",
      "Processed prompts: 100%|██████████| 4/4 [00:00<00:00,  8.64it/s, est. speed input: 2291.58 toks/s, output: 149.30 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# add reasonable score\n",
    "import time\n",
    "count = 0\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1s = []\n",
    "hitss = []\n",
    "correct_ids = []\n",
    "logging_res = []\n",
    "context_pattern = r'(.+?)\\[(.*?)\\]'\n",
    "for index in range(0, len(test_data)):\n",
    "    print(f'Process {index}')\n",
    "    all_relations = []\n",
    "    data_input = test_data[index]['question']\n",
    "    prompt = PROMPT_DICT['llama3'].format(input= data_input)\n",
    "    max_depth = 5\n",
    "    # topic_entity = list(test_data[index]['topic_entity'].keys())\n",
    "    topic_entity = list(test_data[index]['gold_entity_map'].keys())\n",
    "    curr_depth = 1\n",
    "    node_id = 0\n",
    "    prediction_tree = {\n",
    "        node_id: {\n",
    "            \"prompt\": prompt,\n",
    "            \"pred\": \"[Retrieve Relation]\",\n",
    "            \"processed_pred\": \"\",\n",
    "            \"score\": None,\n",
    "            \"topic_entity\": topic_entity,\n",
    "            \"parent\": None,\n",
    "            \"context\": \"\",\n",
    "            \"income\": \"\"\n",
    "        }\n",
    "    }\n",
    "    levels = {0: [node_id]}\n",
    "    start_time = time.time()\n",
    "    while curr_depth < max_depth:\n",
    "        levels[curr_depth] = []\n",
    "        if curr_depth-1 in levels:\n",
    "            for node in levels[curr_depth-1]:\n",
    "                curr_pred = prediction_tree[node][\"pred\"]\n",
    "                if \"<|eot_id|>\" in curr_pred or 'No Retrieval' in curr_pred:\n",
    "                    continue\n",
    "                cur_prompt = prediction_tree[node][\"prompt\"] + prediction_tree[node][\"processed_pred\"]\n",
    "                score = prediction_tree[node][\"score\"] or 0\n",
    "                topic_entity = prediction_tree[node][\"topic_entity\"]\n",
    "                context = prediction_tree[node][\"context\"]\n",
    "                income_rel = prediction_tree[node][\"income\"]\n",
    "                if \"Retrieve Entity\" in curr_pred.split('[')[-1]:\n",
    "                    retrieval_results = {}\n",
    "                    preds, scores, next_entities, contexts, overall_scores, income_rel = run_entity_generation_batch(\n",
    "                        model, cur_prompt, topic_entity, context)\n",
    "                    for i, (pred, p_score,next_topic, context, rel) in enumerate(zip(preds, scores, next_entities, contexts,  income_rel)):\n",
    "                        retrieval_results[i] = {\n",
    "                            \"pred\": pred, \"score\": p_score, \"next_topic\": next_topic, \"context\": context, \"income\": rel, \"verbose\": {\"path\": overall_scores[i]['path'], \"e_match\": overall_scores[i]['e_match'], \"r_match\": overall_scores[i]['r_match'], 'reason_score': overall_scores[i]['reason_score'],'rationality': overall_scores[i]['rationality'], 'utility': overall_scores[i]['utility'], 'utility_score': overall_scores[i]['utility_score'], 'seq_score': overall_scores[i]['seq_score']}}\n",
    "                    for i, result in retrieval_results.items():\n",
    "                        node_id += 1\n",
    "                        node_score = (result[\"score\"] + score) / (curr_depth // 2)\n",
    "                        pred = result[\"pred\"]\n",
    "                        next_entity = result['next_topic']\n",
    "                        if len(next_entity) == 0:\n",
    "                            next_entity = topic_entity  \n",
    "                        prediction_tree[node_id] = {\"prompt\": cur_prompt, \"pred\": pred, \"context\": result['context'],\n",
    "                                                    \"score\": node_score, \"parent\": node,\n",
    "                                                    \"topic_entity\": next_entity, \"income\": result['income'], \"verbose\": result['verbose'],'depth': curr_depth}\n",
    "                        if \"[Retrieve Relation]\" in pred:\n",
    "                            gen_result_index = pred.index(\"[Retrieve Relation]\")\n",
    "                            prev_generation = pred[:gen_result_index]\n",
    "                        else:\n",
    "                            prev_generation = pred\n",
    "                        prediction_tree[node_id][\"processed_pred\"] = prev_generation\n",
    "                        levels[curr_depth].append(node_id)\n",
    "                #存在前后逻辑粘连   \n",
    "                if \"Retrieve Relation\" in curr_pred.split('[')[-1]:\n",
    "                    retrieval_results = {}\n",
    "                    preds, scores, contexts = run_relation_generation_batch(\n",
    "                        model, cur_prompt, llama='llama3', context=context, topic_entity=topic_entity, hypo=True, income=income_rel)\n",
    "                    \n",
    "                    for i, (pred, p_score, context) in enumerate(zip(preds, scores, contexts)):\n",
    "                        retrieval_results[i] = {\n",
    "                            \"pred\": pred, \"score\": p_score, \"context\": context}\n",
    "                        all_relations.extend(list(context.keys()))\n",
    "                    for i, result in retrieval_results.items():\n",
    "                        node_id += 1\n",
    "                        #计算score\n",
    "                        node_score = result[\"score\"] + score if score is not None else result[\"score\"]\n",
    "                        # node_score = result['score']\n",
    "                        pred = result[\"pred\"]\n",
    "                        context = result[\"context\"]\n",
    "                        prediction_tree[node_id] = {\"prompt\": cur_prompt, \"pred\": pred,\n",
    "                                                    \"score\": node_score, \"parent\": node,\n",
    "                                                \"topic_entity\": topic_entity, \"context\": context, 'income': income_rel, 'depth': curr_depth}\n",
    "                        if \"[Retrieve Entity]\" in pred:\n",
    "                            gen_result_index = pred.index(\"[Retrieve Entity]\")\n",
    "                            prev_generation = pred[:gen_result_index]\n",
    "                        else:\n",
    "                            prev_generation = pred\n",
    "                        prediction_tree[node_id][\"processed_pred\"] = prev_generation\n",
    "                        levels[curr_depth].append(node_id)\n",
    "            current_rank = levels[curr_depth]\n",
    "            node2score = {\n",
    "                node_id: prediction_tree[node_id][\"score\"] for node_id in current_rank}\n",
    "            top_nodes = sorted(node2score.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "            levels[curr_depth] = [node[0] for node in top_nodes]\n",
    "            curr_depth += 1\n",
    "    labels = [get_label(ans) if ans.startswith('m.') else ans for ans in test_data[index]['answer']]\n",
    "    # labels = [ans['entity_name'] if 'entity_name' in ans else ans['answer_argument'] for ans in test_data[index]['answer']  ]\n",
    "    ellapse = time.time() - start_time\n",
    "    end_nodes = []\n",
    "    for n_ind, node in prediction_tree.items():\n",
    "        if 'Answer' in node['processed_pred']:\n",
    "            end_nodes.append(n_ind)\n",
    "\n",
    "    queues = []\n",
    "    for n_ind in end_nodes:\n",
    "\n",
    "        queue = []\n",
    "        node = prediction_tree[n_ind]\n",
    "        answer = node['processed_pred'].split('Answer:')[-1]\n",
    "        if 'verbose' in node:\n",
    "            score =  node['score'] + 0.5 * (node['verbose']['utility_score']+ 1) + node['verbose']['seq_score']\n",
    "        else:\n",
    "            score = node['score']\n",
    "        while node:\n",
    "            parent = node['parent']\n",
    "            if 'verbose' in node:\n",
    "                queue.append(node['verbose'])\n",
    "            if parent == None:\n",
    "                queues.append({'verbose': queue, \"answer\": answer, \"score\": score, 'query': data_input})\n",
    "\n",
    "                break\n",
    "            node = prediction_tree[parent]\n",
    "    queues.sort(key=lambda x: x['score'], reverse=True)\n",
    "    answers = set()\n",
    "    for q in queues[:3]:\n",
    "        answer = q['answer']\n",
    "        candidate =  answer.strip().split(';')\n",
    "        answers.update([re.findall(context_pattern, candidate[i])[0][0]  for i in range(len(candidate)) if len(re.findall(context_pattern, candidate[i]))])\n",
    "    precision, recall, f1, hits =cal_eval_metric(list(answers), labels)\n",
    "    if hits == 1:\n",
    "        print(f'correct {index}')\n",
    "    # else:\n",
    "    #     break\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    f1s.append(f1)\n",
    "    hitss.append(hits)\n",
    "    logging_res.append({\"index\": index, \"tree\": prediction_tree})  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import json\n",
    "from typing import Dict, List\n",
    "from graphviz import Digraph\n",
    "\n",
    "class TreeNode:\n",
    "    # 静态字典，存储 (level, name) 到 id 的映射\n",
    "    _level_name_to_id = {}\n",
    "\n",
    "    def __init__(self, name: str, is_entity: bool = False, level: int = 0, r_relevance: str = None, e_relevance: str = None):\n",
    "        self.name = name\n",
    "        self.is_entity = is_entity\n",
    "        self.level = level\n",
    "        self.r_relevance = r_relevance  # Relevance for relation (for edges to this node if relation)\n",
    "        self.e_relevance = e_relevance  # Relevance for entity (for edges from this node if entity)\n",
    "        # 为实体分配 ID，同一 level 的同名实体共享 ID\n",
    "        if is_entity:\n",
    "            key = (level, name)\n",
    "            if key in TreeNode._level_name_to_id:\n",
    "                self.id = TreeNode._level_name_to_id[key]\n",
    "            else:\n",
    "                self.id = str(uuid.uuid4())\n",
    "                TreeNode._level_name_to_id[key] = self.id\n",
    "        else:\n",
    "            self.id = name  # 关系节点使用名称作为 ID\n",
    "        self.children = {}\n",
    "\n",
    "    def add_child(self, name: str, is_entity: bool = False, r_relevance: str = None, e_relevance: str = None) -> 'TreeNode':\n",
    "        # 计算子节点的 level\n",
    "        child_level = self.level + 1\n",
    "        # 为关系节点使用名称作为 key，为实体节点使用 ID 作为 key\n",
    "        key = name if not is_entity else str(TreeNode._get_entity_id(child_level, name))\n",
    "        if key not in self.children:\n",
    "            self.children[key] = TreeNode(name, is_entity, child_level, r_relevance, e_relevance)\n",
    "        else:\n",
    "            # 更新现有节点的 relevance（如果提供）\n",
    "            if r_relevance and not is_entity:\n",
    "                self.children[key].r_relevance = r_relevance\n",
    "            if e_relevance and is_entity:\n",
    "                self.children[key].e_relevance = e_relevance\n",
    "        return self.children[key]\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_entity_id(level: int, name: str) -> str:\n",
    "        \"\"\"获取或生成指定 level 和 name 的实体 ID。\"\"\"\n",
    "        key = (level, name)\n",
    "        if key not in TreeNode._level_name_to_id:\n",
    "            TreeNode._level_name_to_id[key] = str(uuid.uuid4())\n",
    "        return TreeNode._level_name_to_id[key]\n",
    "\n",
    "    def __repr__(self, level=0):\n",
    "        ret = \"  \" * level + f\"{self.name} (id={self.id}, level={self.level}\"\n",
    "        if self.r_relevance:\n",
    "            ret += f\", r_relevance={self.r_relevance}\"\n",
    "        if self.e_relevance:\n",
    "            ret += f\", e_relevance={self.e_relevance}\"\n",
    "        ret += \")\\n\"\n",
    "        for child in self.children.values():\n",
    "            ret += child.__repr__(level + 1)\n",
    "        return ret\n",
    "\n",
    "def parse_triple(triple: str) -> tuple[str, str, str]:\n",
    "    \"\"\"Parse a triple string into (entity1, relation, entity2).\"\"\"\n",
    "    triple = triple.strip('()').split(',')\n",
    "    return triple[0].strip(), triple[1].strip(), triple[2].strip()\n",
    "\n",
    "def find_node(node: TreeNode, target_name: str, is_entity: bool, path: List[str] = None) -> TreeNode:\n",
    "    \"\"\"Recursively find the deepest node by name, type, and level, avoiding cycles.\"\"\"\n",
    "    if path is None:\n",
    "        path = []\n",
    "    result = None\n",
    "    if node.name == target_name and node.is_entity == is_entity and node.id not in path:\n",
    "        result = node\n",
    "    path.append(node.id)\n",
    "    for child in node.children.values():\n",
    "        child_result = find_node(child, target_name, is_entity, path.copy())\n",
    "        if child_result:\n",
    "            result = child_result  # Keep the deepest match\n",
    "    return result\n",
    "\n",
    "def add_path_to_tree(tree: TreeNode, entity1: str, relation: str, entity2: str, r_match: Dict, e_match: Dict):\n",
    "    \"\"\"Add a path (entity1 -> relation -> entity2) to the tree with relevance.\"\"\"\n",
    "    parent = find_node(tree, entity1, is_entity=True)\n",
    "    if not parent:\n",
    "        return  # Skip if entity1 not found\n",
    "\n",
    "    # 获取关系和实体的 relevance\n",
    "    r_relevance = r_match.get(relation, {}).get('relevance', None)\n",
    "    e_relevance = e_match.get(entity2, None)\n",
    "\n",
    "    relation_node = parent.add_child(relation, is_entity=False, r_relevance=r_relevance)\n",
    "    relation_node.add_child(entity2, is_entity=True, e_relevance=e_relevance)\n",
    "\n",
    "def build_tree(data: Dict) -> TreeNode:\n",
    "    \"\"\"Build a tree from the verbose list with query as root.\"\"\"\n",
    "    verbose = data['verbose']\n",
    "    query = data.get('query', 'Unknown Query')\n",
    "\n",
    "    # 清空 ID 映射\n",
    "    TreeNode._level_name_to_id.clear()\n",
    "    \n",
    "    # Initialize tree with query as root\n",
    "    tree = TreeNode(query, is_entity=True, level=0)\n",
    "\n",
    "    # Process verbose[-1] to get connecting entities\n",
    "    last_path_data = verbose[-1]\n",
    "    last_path = last_path_data['path']\n",
    "    r_match = last_path_data.get('r_match', {})\n",
    "    e_match = last_path_data.get('e_match', {})\n",
    "    for triple_str in last_path:\n",
    "        entity1, relation, entity2 = parse_triple(triple_str)\n",
    "        entity_node = tree.add_child(entity1, is_entity=True)\n",
    "        relation_node = entity_node.add_child(\n",
    "            relation, \n",
    "            is_entity=False, \n",
    "            r_relevance=r_match.get(relation, {}).get('relevance', None)\n",
    "        )\n",
    "        relation_node.add_child(\n",
    "            entity2, \n",
    "            is_entity=True, \n",
    "            e_relevance=e_match.get(entity2, None)\n",
    "        )\n",
    "\n",
    "    # Process remaining paths in reverse order (excluding verbose[-1])\n",
    "    for path_data in verbose[:-1][::-1]:\n",
    "        r_match = path_data.get('r_match', {})\n",
    "        e_match = path_data.get('e_match', {})\n",
    "        for triple_str in path_data['path']:\n",
    "            entity1, relation, entity2 = parse_triple(triple_str)\n",
    "            add_path_to_tree(tree, entity1, relation, entity2, r_match, e_match)\n",
    "\n",
    "    return tree\n",
    "\n",
    "def visualize_tree(root: TreeNode, verbose: List[Dict], output_file: str = 'tree', utility_score=0, ):\n",
    "    \"\"\"Visualize the tree using Graphviz with relevance on edges, rationality on entity nodes, and underline for root.\"\"\"\n",
    "    dot = Digraph(comment='Tree Visualization', format='png')\n",
    "    dot.attr(rankdir='TB')\n",
    "    if utility_score != 0:\n",
    "        dot.attr(label=f\"Overall Uility: {utility_score}\")\n",
    "    def get_rationality(entity_name: str) -> str:\n",
    "        \"\"\"Retrieve rationality score for an entity from verbose data.\"\"\"\n",
    "        for path_data in verbose:\n",
    "            e_match = path_data.get('e_match', {})\n",
    "            if entity_name in e_match and 'rationality' in path_data:\n",
    "                return path_data['rationality']\n",
    "        return \"\"\n",
    "\n",
    "    def add_nodes_edges(node: TreeNode, parent_id: str = None, is_root: bool = False):\n",
    "        node_id = node.id\n",
    "        if is_root:\n",
    "            # 为根节点设置下划线样式\n",
    "            dot.node(node_id, \n",
    "                     label=f'{node.name}',\n",
    "                     shape='underline')\n",
    "        else:\n",
    "            # 为实体节点添加 rationality 分数\n",
    "            label = node.name\n",
    "            if node.is_entity:\n",
    "                rationality = get_rationality(node.name)\n",
    "                if rationality:\n",
    "                    label += f\"\\n{rationality}\"\n",
    "                    # label = f'{node.name}<BR/><FONT COLOR=\"#FF0000\">{rationality}</FONT>'\n",
    "            dot.node(node_id, \n",
    "                     label=label, \n",
    "                     shape='box' if not node.is_entity else 'ellipse')\n",
    "        \n",
    "        if parent_id:\n",
    "            # 根据边类型添加 relevance 标签\n",
    "            label = \"\"\n",
    "            if node.is_entity and node.e_relevance:\n",
    "                label = node.e_relevance  # 关系到实体的边\n",
    "            elif not node.is_entity and node.r_relevance:\n",
    "                label = node.r_relevance  # 实体到关系的边\n",
    "            dot.edge(parent_id, node_id, label=label)\n",
    "        \n",
    "        for child in node.children.values():\n",
    "            add_nodes_edges(child, node_id, is_root=False)\n",
    "\n",
    "    # 为根节点传入 is_root=True\n",
    "    add_nodes_edges(root, is_root=True)\n",
    "    dot.render(output_file, view=False, cleanup=True)\n",
    "    print(f\"Tree visualization saved as {output_file}.png\")\n",
    "    return dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree visualization saved as test.png\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"526pt\" height=\"514pt\"\n",
       " viewBox=\"0.00 0.00 526.36 514.48\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 510.48)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-510.48 522.36,-510.48 522.36,4 -4,4\"/>\n",
       "<!-- 5645a644&#45;9233&#45;4a61&#45;89f7&#45;9de6cf4fd12e -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>5645a644&#45;9233&#45;4a61&#45;89f7&#45;9de6cf4fd12e</title>\n",
       "<polygon fill=\"none\" stroke=\"transparent\" points=\"355.5,-506.48 140.5,-506.48 140.5,-470.48 355.5,-470.48 355.5,-506.48\"/>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"140.5,-470.48 355.5,-470.48 \"/>\n",
       "<text text-anchor=\"middle\" x=\"248\" y=\"-484.78\" font-family=\"Times,serif\" font-size=\"14.00\">who is niall ferguson &#39;s wife</text>\n",
       "</g>\n",
       "<!-- Marriage -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>Marriage</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"207.5,-410.61 124.5,-410.61 124.5,-374.61 207.5,-374.61 207.5,-410.61\"/>\n",
       "<text text-anchor=\"middle\" x=\"166\" y=\"-388.91\" font-family=\"Times,serif\" font-size=\"14.00\">Marriage</text>\n",
       "</g>\n",
       "<!-- 5645a644&#45;9233&#45;4a61&#45;89f7&#45;9de6cf4fd12e&#45;&gt;Marriage -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>5645a644&#45;9233&#45;4a61&#45;89f7&#45;9de6cf4fd12e&#45;&gt;Marriage</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M221.1,-470.27C214.08,-465.06 206.86,-458.98 201,-452.48 192.26,-442.78 184.57,-430.57 178.65,-419.78\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"181.67,-418 173.92,-410.77 175.47,-421.25 181.67,-418\"/>\n",
       "<text text-anchor=\"middle\" x=\"253\" y=\"-441.28\" font-family=\"Times,serif\" font-size=\"14.00\">Fully Relevant</text>\n",
       "</g>\n",
       "<!-- 091763ff&#45;20d3&#45;44fc&#45;8eb6&#45;e91e280992cb -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>091763ff&#45;20d3&#45;44fc&#45;8eb6&#45;e91e280992cb</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"339\" cy=\"-392.61\" rx=\"105.22\" ry=\"26.74\"/>\n",
       "<text text-anchor=\"middle\" x=\"339\" y=\"-396.41\" font-family=\"Times,serif\" font-size=\"14.00\">Niall Ferguson</text>\n",
       "<text text-anchor=\"middle\" x=\"339\" y=\"-381.41\" font-family=\"Times,serif\" font-size=\"14.00\">[Fully Reasonable]</text>\n",
       "</g>\n",
       "<!-- 5645a644&#45;9233&#45;4a61&#45;89f7&#45;9de6cf4fd12e&#45;&gt;091763ff&#45;20d3&#45;44fc&#45;8eb6&#45;e91e280992cb -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>5645a644&#45;9233&#45;4a61&#45;89f7&#45;9de6cf4fd12e&#45;&gt;091763ff&#45;20d3&#45;44fc&#45;8eb6&#45;e91e280992cb</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M282,-470.41C290.12,-465.35 298.36,-459.3 305,-452.48 311.77,-445.52 317.7,-436.96 322.63,-428.54\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"325.83,-429.99 327.58,-419.54 319.69,-426.62 325.83,-429.99\"/>\n",
       "</g>\n",
       "<!-- people.marriage.type_of_union -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>people.marriage.type_of_union</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"236,-314.74 0,-314.74 0,-278.74 236,-278.74 236,-314.74\"/>\n",
       "<text text-anchor=\"middle\" x=\"118\" y=\"-293.04\" font-family=\"Times,serif\" font-size=\"14.00\">people.marriage.type_of_union</text>\n",
       "</g>\n",
       "<!-- Marriage&#45;&gt;people.marriage.type_of_union -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>Marriage&#45;&gt;people.marriage.type_of_union</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M157.2,-374.39C149.96,-360.23 139.58,-339.95 131.33,-323.8\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"134.42,-322.16 126.75,-314.85 128.18,-325.35 134.42,-322.16\"/>\n",
       "<text text-anchor=\"middle\" x=\"196\" y=\"-336.54\" font-family=\"Times,serif\" font-size=\"14.00\">Fully Relevant</text>\n",
       "</g>\n",
       "<!-- e7056c88&#45;50d6&#45;4561&#45;929c&#45;41dee54f795f -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>e7056c88&#45;50d6&#45;4561&#45;929c&#45;41dee54f795f</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"118\" cy=\"-209.74\" rx=\"55.79\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"118\" y=\"-206.04\" font-family=\"Times,serif\" font-size=\"14.00\">m.0j4tj43</text>\n",
       "</g>\n",
       "<!-- people.marriage.type_of_union&#45;&gt;e7056c88&#45;50d6&#45;4561&#45;929c&#45;41dee54f795f -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>people.marriage.type_of_union&#45;&gt;e7056c88&#45;50d6&#45;4561&#45;929c&#45;41dee54f795f</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M118,-278.54C118,-266.9 118,-251.29 118,-237.98\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"121.5,-237.92 118,-227.92 114.5,-237.92 121.5,-237.92\"/>\n",
       "<text text-anchor=\"middle\" x=\"170\" y=\"-249.54\" font-family=\"Times,serif\" font-size=\"14.00\">Fully Relevant</text>\n",
       "</g>\n",
       "<!-- people.person.spouse_s -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>people.person.spouse_s</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"439.5,-314.74 254.5,-314.74 254.5,-278.74 439.5,-278.74 439.5,-314.74\"/>\n",
       "<text text-anchor=\"middle\" x=\"347\" y=\"-293.04\" font-family=\"Times,serif\" font-size=\"14.00\">people.person.spouse_s</text>\n",
       "</g>\n",
       "<!-- 091763ff&#45;20d3&#45;44fc&#45;8eb6&#45;e91e280992cb&#45;&gt;people.person.spouse_s -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>091763ff&#45;20d3&#45;44fc&#45;8eb6&#45;e91e280992cb&#45;&gt;people.person.spouse_s</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M341.21,-365.71C342.28,-353.08 343.58,-337.88 344.67,-325.06\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"348.17,-325.22 345.53,-314.96 341.2,-324.63 348.17,-325.22\"/>\n",
       "<text text-anchor=\"middle\" x=\"396\" y=\"-336.54\" font-family=\"Times,serif\" font-size=\"14.00\">Fully Relevant</text>\n",
       "</g>\n",
       "<!-- 4f77cfd1&#45;dfb8&#45;4c4c&#45;bd89&#45;89b34e84ab9f -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>4f77cfd1&#45;dfb8&#45;4c4c&#45;bd89&#45;89b34e84ab9f</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"298\" cy=\"-209.74\" rx=\"57.69\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"298\" y=\"-206.04\" font-family=\"Times,serif\" font-size=\"14.00\">m.0j4jq57</text>\n",
       "</g>\n",
       "<!-- people.person.spouse_s&#45;&gt;4f77cfd1&#45;dfb8&#45;4c4c&#45;bd89&#45;89b34e84ab9f -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>people.person.spouse_s&#45;&gt;4f77cfd1&#45;dfb8&#45;4c4c&#45;bd89&#45;89b34e84ab9f</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M337.08,-278.54C330.13,-266.48 320.71,-250.14 312.87,-236.53\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"315.76,-234.54 307.73,-227.63 309.7,-238.04 315.76,-234.54\"/>\n",
       "</g>\n",
       "<!-- dfc84a85&#45;2c83&#45;4526&#45;a75f&#45;13d63b910a61 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>dfc84a85&#45;2c83&#45;4526&#45;a75f&#45;13d63b910a61</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"432\" cy=\"-209.74\" rx=\"58.49\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"432\" y=\"-206.04\" font-family=\"Times,serif\" font-size=\"14.00\">m.0pdtjg5</text>\n",
       "</g>\n",
       "<!-- people.person.spouse_s&#45;&gt;dfc84a85&#45;2c83&#45;4526&#45;a75f&#45;13d63b910a61 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>people.person.spouse_s&#45;&gt;dfc84a85&#45;2c83&#45;4526&#45;a75f&#45;13d63b910a61</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M364.2,-278.54C376.89,-265.85 394.31,-248.43 408.31,-234.43\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"411.09,-236.6 415.69,-227.05 406.14,-231.65 411.09,-236.6\"/>\n",
       "</g>\n",
       "<!-- people.marriage.spouse -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>people.marriage.spouse</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"392,-140.74 204,-140.74 204,-104.74 392,-104.74 392,-140.74\"/>\n",
       "<text text-anchor=\"middle\" x=\"298\" y=\"-119.04\" font-family=\"Times,serif\" font-size=\"14.00\">people.marriage.spouse</text>\n",
       "</g>\n",
       "<!-- 4f77cfd1&#45;dfb8&#45;4c4c&#45;bd89&#45;89b34e84ab9f&#45;&gt;people.marriage.spouse -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>4f77cfd1&#45;dfb8&#45;4c4c&#45;bd89&#45;89b34e84ab9f&#45;&gt;people.marriage.spouse</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M298,-191.54C298,-179.9 298,-164.29 298,-150.98\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"301.5,-150.92 298,-140.92 294.5,-150.92 301.5,-150.92\"/>\n",
       "<text text-anchor=\"middle\" x=\"350\" y=\"-162.54\" font-family=\"Times,serif\" font-size=\"14.00\">Fully Relevant</text>\n",
       "</g>\n",
       "<!-- f94d6e05&#45;faf5&#45;450e&#45;973c&#45;252c3e5934f2 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>f94d6e05&#45;faf5&#45;450e&#45;973c&#45;252c3e5934f2</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"184\" cy=\"-26.87\" rx=\"105.22\" ry=\"26.74\"/>\n",
       "<text text-anchor=\"middle\" x=\"184\" y=\"-30.67\" font-family=\"Times,serif\" font-size=\"14.00\">Ayaan Hirsi Ali</text>\n",
       "<text text-anchor=\"middle\" x=\"184\" y=\"-15.67\" font-family=\"Times,serif\" font-size=\"14.00\">[Fully Reasonable]</text>\n",
       "</g>\n",
       "<!-- people.marriage.spouse&#45;&gt;f94d6e05&#45;faf5&#45;450e&#45;973c&#45;252c3e5934f2 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>people.marriage.spouse&#45;&gt;f94d6e05&#45;faf5&#45;450e&#45;973c&#45;252c3e5934f2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M277.09,-104.52C261.82,-91.95 240.67,-74.53 222.39,-59.48\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"224.17,-56.41 214.23,-52.76 219.72,-61.82 224.17,-56.41\"/>\n",
       "<text text-anchor=\"middle\" x=\"307\" y=\"-75.54\" font-family=\"Times,serif\" font-size=\"14.00\">Fully Relevant</text>\n",
       "</g>\n",
       "<!-- ccbc67c9&#45;12b1&#45;403f&#45;a6c1&#45;290883a51e8c -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>ccbc67c9&#45;12b1&#45;403f&#45;a6c1&#45;290883a51e8c</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"413\" cy=\"-26.87\" rx=\"105.22\" ry=\"26.74\"/>\n",
       "<text text-anchor=\"middle\" x=\"413\" y=\"-30.67\" font-family=\"Times,serif\" font-size=\"14.00\">Niall Ferguson</text>\n",
       "<text text-anchor=\"middle\" x=\"413\" y=\"-15.67\" font-family=\"Times,serif\" font-size=\"14.00\">[Fully Reasonable]</text>\n",
       "</g>\n",
       "<!-- people.marriage.spouse&#45;&gt;ccbc67c9&#45;12b1&#45;403f&#45;a6c1&#45;290883a51e8c -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>people.marriage.spouse&#45;&gt;ccbc67c9&#45;12b1&#45;403f&#45;a6c1&#45;290883a51e8c</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M335.6,-104.56C344.99,-99.46 354.73,-93.43 363,-86.74 372.02,-79.44 380.65,-70.35 388.13,-61.52\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"391.04,-63.49 394.65,-53.53 385.62,-59.06 391.04,-63.49\"/>\n",
       "<text text-anchor=\"middle\" x=\"419.5\" y=\"-75.54\" font-family=\"Times,serif\" font-size=\"14.00\">Unrelevant</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7f647614c970>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_tree(queues[0])\n",
    "# utility_score = queues[1]['verbose'][0]['utility']\n",
    "visualize_tree(build_tree(queues[0]), queues[0]['verbose'] ,  'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "precisions = []\n",
    "recalls = []\n",
    "f1s = []\n",
    "hitss = []\n",
    "for index in range(27, len(test_res)):\n",
    "    true_index = test_res[index]['index']\n",
    "    labels = [get_label(ans) if ans.startswith('m.') else ans for ans in test_data[true_index]['answer']]\n",
    "    # labels = [get_label(ans['answer_argument']) if ans['answer_argument'].startswith('m.') else ans['answer_argument'] for ans in test_data[index]['answer']]\n",
    "    prediction_tree = test_res[index]['tree']\n",
    "    end_nodes = []\n",
    "    for n_ind, node in prediction_tree.items():\n",
    "        if 'Answer' in node['processed_pred']:\n",
    "            end_nodes.append(n_ind)\n",
    "    queues = []\n",
    "    context_pattern = r'(.+?)\\[(.*?)\\]'\n",
    "    for n_ind in end_nodes:\n",
    "        queue = []\n",
    "        node = prediction_tree[str(n_ind)]\n",
    "        answer = node['processed_pred'].split('Answer:')[-1]\n",
    "        if 'verbose' in node:\n",
    "            # score = node['verbose']['seq_score']\n",
    "            score =  node['score'] + 0.5 * (node['verbose']['utility_score']+ 1) \n",
    "        else:\n",
    "            score = node['score']\n",
    "        while node:\n",
    "            parent = node['parent']\n",
    "            if 'verbose' in node:\n",
    "                queue.append(node['verbose'])\n",
    "            if parent == None:\n",
    "                queues.append({'verbose': queue, \"answer\": answer, \"score\": score})\n",
    "\n",
    "                break\n",
    "            node = prediction_tree[str(parent)]\n",
    "    queues.sort(key=lambda x: x['score'], reverse=True)\n",
    "    answers = set()\n",
    "    for q in queues[:3]:\n",
    "        answer = q['answer']\n",
    "        candidate =  answer.strip().split(';')\n",
    "        answers.update([re.findall(context_pattern, candidate[i])[0][0]  for i in range(len(candidate)) if len(re.findall(context_pattern, candidate[i]))])\n",
    "\n",
    "    precision, recall, f1, hits =cal_eval_metric(list(answers), labels)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    f1s.append(f1)\n",
    "    hitss.append(hits)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "self-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
