{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"2b219db0d2984f9dae28b651ab8ab3d9\"\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://smsh.openai.azure.com/\"\n",
    "os.environ[\"AZURE_OPENAI_API_VERSION\"] = \"2024-03-01-preview\"\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",\n",
    ")\n",
    "\n",
    "from langchain.storage import LocalFileStore, RedisStore\n",
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "store = RedisStore(redis_url=\"redis://localhost:6379\")\n",
    "cached_embedder = CacheBackedEmbeddings.from_bytes_store(\n",
    "embeddings, store, namespace=\"openai\"\n",
    ")\n",
    "row_string = []\n",
    "with open('./data/clean_relations', 'r') as f:\n",
    "    data = [line.strip() for line in f]\n",
    "db = FAISS.from_texts(data, cached_embedder)\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create with GPT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "import os\n",
    "\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"2b219db0d2984f9dae28b651ab8ab3d9\"\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://smsh.openai.azure.com/\"\n",
    "os.environ[\"AZURE_OPENAI_API_VERSION\"] = \"2024-02-01\"\n",
    "os.environ[\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"] = \"gpt-35-turbo\"\n",
    "\n",
    "model = AzureChatOpenAI(\n",
    "    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    "    azure_deployment=os.environ[\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"],\n",
    "    temperature=1,\n",
    "    n = 3,\n",
    "    max_retries=5, request_timeout=600\n",
    ")\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The name of Justin Bieber's brother is Jaxon Bieber. This is based on the reasoning path that connects Justin Bieber to Jaxon Bieber through the relationship of sibling. The other paths that connect Justin Bieber to Jazmyn Bieber or back to Justin Bieber himself are incorrect in this context.\n",
    "few_shot_intepretable_prompt = FewShotPromptTemplate(\n",
    "        examples=[{\n",
    "            \"query\": \"Who was the prime minister of Japan in 2011, that served in the New Party Sakigake?\",\n",
    "            \"evidence\": \"Relations retrieved: language.human_language.countries_spoken_in\\n Entities retrieved: Japan\",\n",
    "            \"preceding_sentences\": \"\",\n",
    "            \"output\": \"Naoto Kan\",\n",
    "            \"rating\": \"[Continue to Retrieve Evidence]\"\n",
    "        }],\n",
    "        example_prompt=PromptTemplate.from_template(\"\"\"Query: {query}\n",
    "Preceding sentences: {preceding_sentences}\n",
    "Evidence: {evidence}\n",
    "Output: {target_output}\n",
    "Rating: {rating}\"\"\"),\n",
    "        prefix=\n",
    "        \"You will be provided with a query, evidence, output sentence, and preceding sentences (optional). Your task is to determine whether the information in the output sentence can be fully verified by the evidence or if it requires further external verification. There are three cases:\\n\" \n",
    "        \"- If the output sentence can be verified solely with the evidence and the preceding sentences, then respond with [No Retrieval]. \\n\"\n",
    "        \"- If additional information about the tail entity in evidence is needed to verify the output sentence, respond with [Continue to Retrieve Evidence] \\n\"\n",
    "        \"- If more information unrelated to the evidence is needed, e.g. totally new relationship or new entity, reponse with [New Retrieval].\\n\\n\",\n",
    "        suffix=\n",
    "        \"Query: {query}\\n\"\n",
    "        \"Preceding sentences: {preceding_sentences}\\n\"\n",
    "        \"Evidence: {evidence}\\n\"\n",
    "        \"Output: {target_output}\",\n",
    "        input_variables=[\"query\", \"evidence\", \"preceding_sentences\", \"topic\"],\n",
    ")\n",
    "\n",
    "llm_chain = LLMChain(llm=model, prompt=few_shot_intepretable_prompt, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relationship relevance\n",
    "graph_intepretable =  \"\"\"You will receive a query, evidence and optional preceding sentences containing history information. The evidence contains graph relationships or entities may be useful to answer the query. Your task is to filters out valid information from the evidence to answer the given query, evaluate your output and provide explanations on your result.\n",
    "\n",
    "###\n",
    "Query: Name the president of the country whose main spoken language was Brahui in 1980?\n",
    "Topic Entity: Brahui Language\n",
    "Evidence: language.human_language.main_country; language.human_language.language_family; language.human_language.iso_639_3_code; base.rosetta.languoid.parent; language.human_language.writing_system; base.rosetta.languoid.languoid_class; language.human_language.countries_spoken_in; kg.object_profile.prominent_type; base.rosetta.languoid.document; base.ontologies.ontology_instance.equivalent_instances; base.rosetta.languoid.local_name; language.human_language.region\n",
    "Preceding sentences: \n",
    "Output: \n",
    "1. {{language.human_language.main_country (Score: Fully relavant))}}: This relation is highly relevant as it directly relates to the country whose president is being asked for, and the main country where Brahui language is spoken in 1980.\n",
    "2. {{language.human_language.countries_spoken_in (Score: Fully relavant)}}: This relation is also relevant as it provides information on the countries where Brahui language is spoken, which could help narrow down the search for the president.\n",
    "3. {{base.rosetta.languoid.parent (Score: Partially relevant)}}: This relation is less relevant but still provides some context on the language family to which Brahui belongs, which could be useful in understanding the linguistic and cultural background of the country in question.\n",
    "\n",
    "###\n",
    "Query: {query}\n",
    "Topic Entity: {topic}\n",
    "Evidence: {evidence}\n",
    "Preceding sentences: {preceding_sentences}\n",
    "Output: \n",
    "\"\"\"\n",
    "# The name of Justin Bieber's brother is Jaxon Bieber. This is based on the reasoning path that connects Justin Bieber to Jaxon Bieber through the relationship of sibling. The other paths that connect Justin Bieber to Jazmyn Bieber or back to Justin Bieber himself are incorrect in this context.\n",
    "few_shot_intepretable_prompt = FewShotPromptTemplate(\n",
    "        examples=[{\n",
    "            \"query\": \"Name the president of the country whose main spoken language was Brahui in 1980?\",\n",
    "            \"topic\": \"Brahui Language\",\n",
    "            \"evidence\": \"language.human_language.main_country; language.human_language.language_family; language.human_language.iso_639_3_code; base.rosetta.languoid.parent; language.human_language.writing_system; base.rosetta.languoid.languoid_class; language.human_language.countries_spoken_in; kg.object_profile.prominent_type; base.rosetta.languoid.document; base.ontologies.ontology_instance.equivalent_instances; base.rosetta.languoid.local_name; language.human_language.region\",\n",
    "            \"preceding_sentences\": \"\",\n",
    "            \"output\": \"\"\"1. {{language.human_language.main_country (Score: [Fully Relavant])}}: This relation is highly relevant as it directly relates to the country whose president is being asked for, and the main country where Brahui language is spoken in 1980.\n",
    "2. {{language.human_language.countries_spoken_in (Score: [Fully Relavant])}}: This relation is also relevant as it provides information on the countries where Brahui language is spoken, which could help narrow down the search for the president.\n",
    "3. {{base.rosetta.languoid.parent (Score: [Partially Relevant])}}: This relation is less relevant but still provides some context on the language family to which Brahui belongs, which could be useful in understanding the linguistic and cultural background of the country in question.\"\"\"\n",
    "        }],\n",
    "        example_prompt=PromptTemplate.from_template(\"\"\"###\n",
    "Query: {query}\n",
    "Topic Entity: {topic}\n",
    "Evidence: {evidence}\n",
    "Preceding sentences: {preceding_sentences}\n",
    "Output: {output}\n",
    "\"\"\"),\n",
    "        prefix=\n",
    "        \"\"\"You will receive a query, topic entity, evidence and optional preceding sentences containing history information. The evidence contains graph relationships or entities may be useful to answer the query. Your task is to filters out 3 valid information from the evidence that contribute to answering the query and provide a relevance score for each output, output your explanations for the score.\n",
    "The score of relevance range from [Fully Relavant], [Partially Relevant] to [Unrelevant].\"\"\",\n",
    "        suffix=\n",
    "        \"\"\"###\n",
    "Query: {query}\n",
    "Topic Entity: {topic}\n",
    "Evidence: {evidence}\n",
    "Preceding sentences: {preceding_sentences}\n",
    "Output: \"\"\",\n",
    "        input_variables=[\"query\", \"evidence\", \"preceding_sentences\", \"topic\"],\n",
    ")\n",
    "graph_intepretable_prompt = PromptTemplate(input_variables=[\"query\", \"evidence\", \"preceding_sentences\", \"topic\"], template=\n",
    "graph_intepretable)\n",
    "llm_chain = LLMChain(llm=model, prompt=few_shot_intepretable_prompt, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "few_shot_intepretable_prompt = FewShotPromptTemplate(\n",
    "        examples=[{\n",
    "            \"query\": \"what is the name of justin bieber brother?\",\n",
    "            \"output\": \"[Retreive New Relationship]<paragraph>people.sibling_relationship.sibling;fictional_universe.fictional_character.siblings;fictional_universe.sibling_relationship_of_fictional_characters.siblings;people.person.sibling_s;people.family.members;people.person.parents</paragraph>Retrieved relationship: people.person.parents[Fully Relevant][Retrieve Entity]<paragraph>(Justin Bieber, people.person.parents, Pattie Mallette);(Justin Bieber, people.person.parents, Jeremy Bieber);(Justin Bieber, people.person.parents, Jeremy Bieber)</paragraph>Retrieved triplet: (Justin Bieber, people.person.parents, Jeremy Bieber)[Fully Relevant][Continue to Retrieve Evidence]<paragraph>people.sibling_relationship.sibling;people.person.sibling_s;people.person.parents;fictional_universe.fictional_character.siblings;people.family.members;people.person.children</paragraph>Retrieved relationship: people.person.children[Fully Relevant][Retrieve Entity]<paragraph>(Jeremy Bieber, people.person.children, Jazmyn Bieber);(Jeremy Bieber, people.person.children, Justin Bieber);(Jeremy Bieber, people.person.children, Jaxon Bieber);(Jeremy Bieber, people.person.children, Jaxon Bieber)</paragraph>Retrieved triplet: (Jeremy Bieber, people.person.children, Jaxon Bieber)[Fully Relevant][No Retrieval] Answer: Jaxon Bieber\",\n",
    "            \"explanation\": \"The output provides the name of justin bieber's brother. This is based on the reasoning path that connects Justin Bieber to Jaxon Bieber through the relationship of sibling. The other paths that connect Justin Bieber to Jazmyn Bieber or back to Justin Bieber himself are incorrect in this context.\",\n",
    "            \"rating\": \"[Confidence:5]\"\n",
    "        }],\n",
    "        example_prompt=PromptTemplate.from_template(\"\"\"\n",
    "Query: {query}\\n\n",
    "Output: {output}\n",
    "Explanation: {explanation}\n",
    "Rating: {rating}\n",
    "\"\"\"),\n",
    "        prefix=\"\"\"Given a query and an output, rate whether the response and the thought process appears to be a helpful and informative answer to the query, from 1 (lowest) - 5 (highest). We call this confidence score.\n",
    "[Confidence:5]: The response provides a complete and correct reasoning chain to the query, and the final answer is complete and logically correct.\n",
    "[Confidence:4]: The response mostly fulfills the need in the query and provides correct answers, while there can be some minor improvements such as shorter reasoning chain or less repetition.\n",
    "[Confidence:3]: The response is acceptable, but the answers may be not complete or needs minor improvement.\n",
    "[Confidence:2]: The reasoning process still addresses the main request, but the answers are not correct or not relevant to the query.\n",
    "[Confidence:1]: The reasoning is barely irrelevant or does not give an answer in the end.\"\"\",\n",
    "        suffix=\n",
    "        \"\"\"\n",
    "Query: {query}\\n\n",
    "Output: {output}\\n\"\"\",\n",
    "        input_variables=[\"query\", \"output\"],\n",
    ")\n",
    "# confidence_prompt = PromptTemplate(input_variables=[\"query\", \"output\"], template=\n",
    "# graph_intepretable)\n",
    "llm_chain = LLMChain(llm=model, prompt=few_shot_intepretable_prompt, verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run_long_form answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/disk1/chatgpt/miniconda3/envs/self-rag/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 12-05 01:15:27 cuda.py:22] You are using a deprecated `pynvml` package. Please install `nvidia-ml-py` instead, and make sure to uninstall `pynvml`. When both of them are installed, `pynvml` will take precedence and cause errors. See https://pypi.org/project/pynvml for more information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-05 01:15:29,280\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-05 01:15:33 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='/media/disk2/llama_factory/generation_1124_special', speculative_config=None, tokenizer='/media/disk2/llama_factory/generation_1124_special', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/media/disk2/llama_factory/generation_1124_special, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "INFO 12-05 01:15:34 model_runner.py:1056] Starting to load model /media/disk2/llama_factory/generation_1124_special...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.41s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:07<00:08,  4.06s/it]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:13<00:05,  5.05s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:20<00:00,  5.87s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:20<00:00,  5.17s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-05 01:15:55 model_runner.py:1067] Loading model weights took 14.9605 GB\n",
      "INFO 12-05 01:15:56 gpu_executor.py:122] # GPU blocks: 9656, # CPU blocks: 2048\n",
      "INFO 12-05 01:15:56 gpu_executor.py:126] Maximum concurrency for 8192 tokens per request: 18.86x\n",
      "INFO 12-05 01:15:58 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 12-05 01:15:58 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 12-05 01:16:09 model_runner.py:1523] Graph capturing finished in 12 secs.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "model = LLM(model='/media/disk2/llama_factory/generation_1124_special', trust_remote_code=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s, est. speed input: 10.58 toks/s, output: 60.45 toks/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[RequestOutput(request_id=0, prompt='Hello! Who are you?', prompt_token_ids=[128000, 9906, 0, 10699, 527, 499, 30], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=\" What brings you to our neck of the woods? We're a friendly bunch, so don't be shy! Say hello and let us know what's on your mind. We're all ears!<|eot_id|>\", token_ids=(3639, 12716, 499, 311, 1057, 13272, 315, 279, 33633, 30, 1226, 2351, 264, 11919, 15860, 11, 779, 1541, 956, 387, 33394, 0, 25961, 24748, 323, 1095, 603, 1440, 1148, 596, 389, 701, 4059, 13, 1226, 2351, 682, 25212, 0, 128009), cumulative_logprob=None, logprobs=None, finish_reason=stop, stop_reason=None)], finished=True, metrics=RequestMetrics(arrival_time=1733130983.477379, last_token_time=1733130983.477379, first_scheduled_time=1733130983.4866183, first_token_time=1733130983.5325885, time_in_queue=0.00923919677734375, finished_time=1733130984.1349523, scheduler_time=0.0043992819264531136, model_forward_time=None, model_execute_time=None), lora_request=None)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampling_params = SamplingParams(\n",
    "            temperature=0.5, top_p=0.8, max_tokens=2048, skip_special_tokens=False, include_stop_str_in_output=True)\n",
    "model.generate('Hello! Who are you?', sampling_params,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s, est. speed input: 106.52 toks/s, output: 51.96 toks/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletionOutput(index=0, text='finance.currency.countries_used[Partially Relevant]location.country.currency_used[Partially Relevant][Retrieve Entity][No Retrieval]Answer: Bahamian dollar<|eot_id|>', token_ids=(63775, 46518, 522, 20730, 28903, 128262, 2588, 34424, 46518, 28903, 128262, 128259, 128256, 16533, 25, 32429, 309, 1122, 18160, 128009), cumulative_logprob=-3.1009353531653687, logprobs=[{63775: Logprob(logprob=-3.0577995777130127, rank=2, decoded_token='finance'), 128256: Logprob(logprob=-0.057799555361270905, rank=1, decoded_token='[No Retrieval]'), 2588: Logprob(logprob=-4.807799339294434, rank=3, decoded_token='location'), 128258: Logprob(logprob=-6.932799339294434, rank=4, decoded_token='[Continue to Retrieve Evidence]'), 23175: Logprob(logprob=-11.307799339294434, rank=5, decoded_token='United')}, {46518: Logprob(logprob=0.0, rank=1, decoded_token='.currency'), 522: Logprob(logprob=-18.75, rank=2, decoded_token='.c'), 11667: Logprob(logprob=-21.75, rank=3, decoded_token=' currency'), 32733: Logprob(logprob=-23.0, rank=4, decoded_token='.language'), 13: Logprob(logprob=-23.75, rank=5, decoded_token='.')}, {522: Logprob(logprob=-0.005233872216194868, rank=1, decoded_token='.c'), 46518: Logprob(logprob=-5.2552337646484375, rank=2, decoded_token='.currency'), 34424: Logprob(logprob=-17.505233764648438, rank=3, decoded_token='.country'), 840: Logprob(logprob=-18.880233764648438, rank=4, decoded_token='.f'), 962: Logprob(logprob=-19.380233764648438, rank=5, decoded_token='.d')}, {20730: Logprob(logprob=0.0, rank=1, decoded_token='ountries'), 16753: Logprob(logprob=-22.5, rank=2, decoded_token='ourses'), 20180: Logprob(logprob=-23.875, rank=3, decoded_token='urrencies'), 1003: Logprob(logprob=-26.0, rank=4, decoded_token='ash'), 72828: Logprob(logprob=-26.125, rank=5, decoded_token='Countries')}, {28903: Logprob(logprob=0.0, rank=1, decoded_token='_used'), 128262: Logprob(logprob=-24.25, rank=2, decoded_token='[Partially Relevant]'), 12477: Logprob(logprob=-24.75, rank=3, decoded_token=' Used'), 53073: Logprob(logprob=-26.75, rank=4, decoded_token='_owned'), 2656: Logprob(logprob=-28.25, rank=5, decoded_token='used')}, {128262: Logprob(logprob=-1.9073468138230965e-06, rank=1, decoded_token='[Partially Relevant]'), 128261: Logprob(logprob=-13.250001907348633, rank=2, decoded_token='[Fully Relevant]'), 128260: Logprob(logprob=-15.750001907348633, rank=3, decoded_token='[Unrelevant]'), 128256: Logprob(logprob=-27.250001907348633, rank=4, decoded_token='[No Retrieval]'), 58: Logprob(logprob=-28.625001907348633, rank=5, decoded_token='[')}, {2588: Logprob(logprob=-2.3841830625315197e-06, rank=1, decoded_token='location'), 128259: Logprob(logprob=-13.250001907348633, rank=2, decoded_token='[Retrieve Entity]'), 63775: Logprob(logprob=-15.000001907348633, rank=3, decoded_token='finance'), 128256: Logprob(logprob=-16.125001907348633, rank=4, decoded_token='[No Retrieval]'), 100022: Logprob(logprob=-16.375001907348633, rank=5, decoded_token='Bah')}, {34424: Logprob(logprob=0.0, rank=1, decoded_token='.country'), 46518: Logprob(logprob=-25.0, rank=2, decoded_token='.currency'), 3224: Logprob(logprob=-25.25, rank=3, decoded_token=' country'), 29206: Logprob(logprob=-25.75, rank=4, decoded_token='_country'), 8383: Logprob(logprob=-29.625, rank=5, decoded_token='.location')}, {46518: Logprob(logprob=0.0, rank=1, decoded_token='.currency'), 11667: Logprob(logprob=-30.0, rank=2, decoded_token=' currency'), 91575: Logprob(logprob=-33.25, rank=3, decoded_token='(currency'), 27418: Logprob(logprob=-34.5, rank=4, decoded_token='Currency'), 16353: Logprob(logprob=-34.625, rank=5, decoded_token='currency')}, {28903: Logprob(logprob=0.0, rank=1, decoded_token='_used'), 12477: Logprob(logprob=-28.25, rank=2, decoded_token=' Used'), 128262: Logprob(logprob=-28.375, rank=3, decoded_token='[Partially Relevant]'), 84302: Logprob(logprob=-30.125, rank=4, decoded_token='.used'), 2656: Logprob(logprob=-30.25, rank=5, decoded_token='used')}, {128262: Logprob(logprob=-0.029750533401966095, rank=1, decoded_token='[Partially Relevant]'), 128261: Logprob(logprob=-3.5297505855560303, rank=2, decoded_token='[Fully Relevant]'), 128260: Logprob(logprob=-15.65475082397461, rank=3, decoded_token='[Unrelevant]'), 38173: Logprob(logprob=-24.40475082397461, rank=4, decoded_token='[F'), 128256: Logprob(logprob=-25.52975082397461, rank=5, decoded_token='[No Retrieval]')}, {128259: Logprob(logprob=-2.753696753643453e-05, rank=1, decoded_token='[Retrieve Entity]'), 63775: Logprob(logprob=-10.500027656555176, rank=2, decoded_token='finance'), 100022: Logprob(logprob=-20.75002670288086, rank=3, decoded_token='Bah'), 128256: Logprob(logprob=-21.75002670288086, rank=4, decoded_token='[No Retrieval]'), 69243: Logprob(logprob=-26.87502670288086, rank=5, decoded_token='bah')}, {128256: Logprob(logprob=-0.005301464814692736, rank=1, decoded_token='[No Retrieval]'), 100022: Logprob(logprob=-5.255301475524902, rank=2, decoded_token='Bah'), 128258: Logprob(logprob=-9.755301475524902, rank=3, decoded_token='[Continue to Retrieve Evidence]'), 128259: Logprob(logprob=-12.380301475524902, rank=4, decoded_token='[Retrieve Entity]'), 3923: Logprob(logprob=-12.505301475524902, rank=5, decoded_token='What')}, {16533: Logprob(logprob=0.0, rank=1, decoded_token='Answer'), 22559: Logprob(logprob=-26.75, rank=2, decoded_token=' Answer'), 96877: Logprob(logprob=-33.125, rank=3, decoded_token='-answer'), 9399: Logprob(logprob=-33.375, rank=4, decoded_token='answer'), 29634: Logprob(logprob=-33.875, rank=5, decoded_token='_answer')}, {25: Logprob(logprob=0.0, rank=1, decoded_token=':'), 128261: Logprob(logprob=-30.625, rank=2, decoded_token='[Fully Relevant]'), 26: Logprob(logprob=-30.75, rank=3, decoded_token=';'), 1473: Logprob(logprob=-30.875, rank=4, decoded_token=':\\n\\n'), 291: Logprob(logprob=-34.625, rank=5, decoded_token='ed')}, {32429: Logprob(logprob=-0.0003401654539629817, rank=1, decoded_token=' Bah'), 84229: Logprob(logprob=-8.000340461730957, rank=2, decoded_token=' Bahamas'), 43029: Logprob(logprob=-12.375340461730957, rank=3, decoded_token=' bah'), 100022: Logprob(logprob=-14.875340461730957, rank=4, decoded_token='Bah'), 8715: Logprob(logprob=-15.875340461730957, rank=5, decoded_token=' Bank')}, {309: Logprob(logprob=0.0, rank=1, decoded_token='am'), 336: Logprob(logprob=-32.0, rank=2, decoded_token='em'), 24697: Logprob(logprob=-32.25, rank=3, decoded_token='ам'), 13005: Logprob(logprob=-33.25, rank=4, decoded_token='aman'), 3105: Logprob(logprob=-33.5, rank=5, decoded_token='ama')}, {1122: Logprob(logprob=0.0, rank=1, decoded_token='ian'), 467: Logprob(logprob=-29.75, rank=2, decoded_token='ain'), 22774: Logprob(logprob=-33.5, rank=3, decoded_token='IAN'), 8503: Logprob(logprob=-34.75, rank=4, decoded_token='yan'), 5038: Logprob(logprob=-35.0, rank=5, decoded_token='iam')}, {18160: Logprob(logprob=0.0, rank=1, decoded_token=' dollar'), 54135: Logprob(logprob=-22.625, rank=2, decoded_token='-dollar'), 11667: Logprob(logprob=-24.125, rank=3, decoded_token=' currency'), 21999: Logprob(logprob=-25.5, rank=4, decoded_token='ollar'), 11441: Logprob(logprob=-25.625, rank=5, decoded_token=' dollars')}, {128009: Logprob(logprob=-0.0024779110681265593, rank=1, decoded_token='<|eot_id|>'), 26: Logprob(logprob=-6.002478122711182, rank=2, decoded_token=';'), 128261: Logprob(logprob=-13.002477645874023, rank=3, decoded_token='[Fully Relevant]'), 38173: Logprob(logprob=-20.627477645874023, rank=4, decoded_token='[F'), 128262: Logprob(logprob=-21.002477645874023, rank=5, decoded_token='[Partially Relevant]')}], finish_reason=stop, stop_reason=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#是否带special token 分开传入，用于检索和生成\n",
    "sampling_params = SamplingParams(\n",
    "            temperature=0.5, top_p=1.0,max_tokens=100, logprobs=5, skip_special_tokens=False, include_stop_str_in_output=True)\n",
    "PROMPT_DICT = {\"llama3\": '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n{input}<|eot_id|><|start_header_id|>assistant<|end_header_id|>'}\n",
    "model.generate([PROMPT_DICT[\"llama3\"].format(input=\"what kind of money to take to bahamas\") + \"finance.currency.countries_used[Fully Relevant]location.country.currency_used[Fully Relevant]finance.currency.currency_code[Partially Relevant][Retrieve Entity]Bahamian dollar[Fully Relevant]\"], sampling_params)[0].outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('/media/disk2/llama_factory/generation_1124_special')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_special_tokens(tokenizer, use_grounding=False, use_utility=False):\n",
    "    rel_tokens = {}\n",
    "    for token in ['[Unrelevant]','[Partially Relevant]','[Fully Relevant]']:\n",
    "        rel_tokens[token] = tokenizer.convert_tokens_to_ids(token)\n",
    "\n",
    "    # ut_tokens = None\n",
    "    # if use_utility is True:\n",
    "    #     ut_tokens = {}\n",
    "    #     for token in utility_tokens_names:\n",
    "    #         ut_tokens[token] = tokenizer.convert_tokens_to_ids(token)\n",
    "\n",
    "    return rel_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_tokens = load_special_tokens(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from src.sparql_utils import *\n",
    "def run_step_generation_batch(model, prompt, topic_entity,new_retrieval, beam_width=3):\n",
    "    pattern = r'(.*?)\\[(.*?)\\]'\n",
    "    rel_score_dict = {}\n",
    "    return_entities = []\n",
    "    final_preds = []\n",
    "    overall_scores = {}\n",
    "    paragraph = ';'.join([page.page_content.strip() for page in retriever.invoke(prompt.split('\\n\\n')[1].split('<|eot_id|>')[0])])\n",
    "    print(paragraph)\n",
    "    if new_retrieval:\n",
    "        retrieval_token = \"[New Retrieval]\"\n",
    "        aug_prompts =  [\"<paragraph>{}</paragraph>\".format(paragraph)]\n",
    "    else:\n",
    "        retrieval_token = \"[Continue to Retrieve Evidence]\"\n",
    "        aug_prompts =  [\"<paragraph>{}</paragraph>\".format(paragraph)]\n",
    "    \n",
    "    pred = model.generate(prompt + retrieval_token + aug_prompts[0], sampling_params)[0]\n",
    "    pred_token_ids = pred.outputs[0].token_ids\n",
    "    pred_text_1 = pred.outputs[0].text\n",
    "    pred_log_probs = pred.outputs[0].logprobs\n",
    "    seq_score = pred.outputs[0].cumulative_logprob / \\\n",
    "        max(len(pred.outputs[0].token_ids), 1)\n",
    "    relevance_indices = []\n",
    "    for tok_idx, tok in enumerate(pred_token_ids):\n",
    "        if tok in rel_tokens.values():\n",
    "            relevance_indices.append(tok_idx)\n",
    "    if len(relevance_indices) > 0:\n",
    "        for idx in relevance_indices:\n",
    "            for token, token_id in rel_tokens.items():\n",
    "                prob = pred_log_probs[idx][token_id].logprob if token_id in pred_log_probs[idx] else -100\n",
    "                rel_score_dict[token] = np.exp(prob)\n",
    "    relevance_score = rel_score_dict['[Fully Relevant]']+ rel_score_dict['[Partially Relevant]'] / np.sum(list(rel_score_dict.values()))\n",
    "    processed_pred = pred_text_1.split('[Retrieve Entity]')[0]\n",
    "    matches =  dict(re.findall(pattern,processed_pred))\n",
    "    \n",
    "    name2id = dict()\n",
    "    entity_prompts = []\n",
    "    for _, entity in enumerate(topic_entity):\n",
    "        entities = []\n",
    "        for k, v in matches.items():\n",
    "            if v in ['Fully Relevant', 'Partially Relevant']:\n",
    "                another_entities = get_another_entity(entity, k, return_label=True)\n",
    "                # print(another_entities)\n",
    "                name2id.update(another_entities)\n",
    "                entities.extend([f'({get_label(entity)}, {k}, {e})' for e in another_entities.values()])\n",
    "        entity_prompts.append(aug_prompts[0] + processed_pred +  '[Retrieve Entity]' + \"<paragraph>{}</paragraph>\".format(';'.join(entities[:10])))\n",
    "    # print(aug_prompts)\n",
    "    preds = model.generate([prompt + retrieval_token+ entity_prompts[i] for i in range(len(entity_prompts))], sampling_params)\n",
    "    \n",
    "    for p_idx, pred in enumerate(preds):\n",
    "        pred_token_ids = pred.outputs[0].token_ids\n",
    "        pred_text_2 = pred.outputs[0].text\n",
    "        pred_log_probs = pred.outputs[0].logprobs\n",
    "        rel_score_dict = {}\n",
    "        relevance_indices = []\n",
    "        for tok_idx, tok in enumerate(pred_token_ids):\n",
    "            if tok in rel_tokens.values():\n",
    "                relevance_indices.append(tok_idx)\n",
    "        if len(relevance_indices) > 0:\n",
    "            # print(relevance_indices)\n",
    "            for idx in relevance_indices:\n",
    "                for token, token_id in rel_tokens.items():\n",
    "                    prob = pred_log_probs[idx][token_id].logprob if token_id in pred_log_probs[idx] else -100\n",
    "                    rel_score_dict[token] = np.exp(prob)\n",
    "        overall_scores[p_idx] = relevance_score + rel_score_dict['[Fully Relevant]'] + rel_score_dict['[Partially Relevant]']/ np.sum(list(rel_score_dict.values()))\n",
    "        if '[Continue to Retrieve Evidence]' in pred_text_2:\n",
    "            processed_pred = pred_text_2.split('[Continue to Retrieve Evidence]')[0]\n",
    "            matches =  dict(re.findall(pattern, processed_pred))\n",
    "            for k, v in matches.items():\n",
    "                if v in ['Fully Relevant', 'Partially Relevant']:\n",
    "                    if k in name2id:\n",
    "                        return_entities.append(name2id[k])\n",
    "            processed_pred += '[Continue to Retrieve Evidence]'\n",
    "        else:\n",
    "            processed_pred = pred_text_2\n",
    "        final_preds.append(aug_prompts[0] + processed_pred)\n",
    "    return final_preds, [overall_scores[p_idx] for p_idx in overall_scores], return_entities\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1639\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('./data/merged/WebQSP_test.json', 'r') as f:\n",
    "    test_data = json.load(f)\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 4\n",
      "location.us_state.capital;people.family.country;people.ethnicity.geographic_distribution;fictional_universe.fictional_character.place_of_birth;people.marriage.from\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 55.59 toks/s, output: 63.68 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  2.54it/s, est. speed input: 193.99 toks/s, output: 63.81 toks/s]\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "# for i in range(len(test_data)):\n",
    "i=4\n",
    "print(f'Process {i}')\n",
    "data_input = test_data[i]['question']\n",
    "prompt = PROMPT_DICT['llama3'].format(input= data_input)\n",
    "max_depth = 3\n",
    "topic_entity = list(test_data[i]['gold_entity_map'].keys())\n",
    "# pred = model.generate([prompt], sampling_params)[0]\n",
    "# pred_text = pred.outputs[0].text\n",
    "# if '[New Retrieval]' in pred_text:\n",
    "curr_depth = 1\n",
    "terminated = False\n",
    "node_id = 0\n",
    "prediction_tree = {}\n",
    "levels = {}\n",
    "prediction_tree[node_id] = {\"prompt\": prompt, \"pred\": \"[New Retrieval]\",\n",
    "                            \"processed_pred\": \"\", \"score\": None, \"topic_entity\": topic_entity, \"parent\": None}\n",
    "levels[0] = [0]\n",
    "while curr_depth < max_depth:\n",
    "    levels[curr_depth] = []\n",
    "    if curr_depth-1 in levels and terminated is False:\n",
    "        for node in levels[curr_depth-1]:\n",
    "            pred = prediction_tree[node][\"pred\"]\n",
    "            if \"<|eot_id|>\" in pred:\n",
    "                terminated = True\n",
    "                continue\n",
    "            prompt = prediction_tree[node][\"prompt\"]\n",
    "            prev_generation = prediction_tree[node][\"processed_pred\"]\n",
    "            score = prediction_tree[node][\"score\"]\n",
    "            topic_entity = prediction_tree[node][\"topic_entity\"]\n",
    "            if \"[New Retrieval]\" in pred or \"[Continue to Retrieve Evidence]\" in pred:\n",
    "                retrieval_results = {}\n",
    "                preds, scores, next_entities = run_step_generation_batch(\n",
    "                    model, prompt + prev_generation, topic_entity, new_retrieval=True if (\"[New Retrieval]\" in pred) else False)\n",
    "                for i, (pred, p_score) in enumerate(zip(preds, scores)):\n",
    "                    retrieval_results[i] = {\n",
    "                        \"pred\": pred, \"score\": p_score}\n",
    "\n",
    "                for i, result in retrieval_results.items():\n",
    "                    node_id += 1\n",
    "                    node_score = result[\"score\"] * \\\n",
    "                        score if score is not None else result[\"score\"]\n",
    "                    pred = result[\"pred\"]\n",
    "                    if len(next_entities) == 0:\n",
    "                        next_entities = [topic_entity]\n",
    "                    prediction_tree[node_id] = {\"prompt\": prompt + prev_generation, \"pred\": pred,\n",
    "                                                \"score\": node_score, \"parent\": node,\n",
    "                                                \"topic_entity\": next_entities[0]}\n",
    "                    if \"[Continue to Retrieve Evidence]\" in pred:\n",
    "                        gen_result_index = pred.index(\"[Continue to Retrieve Evidence]\")\n",
    "                        prev_generation = pred[:gen_result_index]\n",
    "                    else:\n",
    "                        prev_generation = pred\n",
    "                    prediction_tree[node_id][\"processed_pred\"] = prev_generation\n",
    "                    levels[curr_depth].append(node_id)\n",
    "            current_rank = levels[curr_depth]\n",
    "            node2score = {\n",
    "                node_id: prediction_tree[node_id][\"score\"] for node_id in current_rank}\n",
    "            top_nodes = sorted(node2score.items(), key=lambda x: x[1], reverse=True)[\n",
    "                :2]\n",
    "            levels[curr_depth] = [node[0] for node in top_nodes]\n",
    "            curr_depth += 1\n",
    "    else:\n",
    "        break\n",
    "labels = [get_label(ans) for ans in test_data[i]['answer']]\n",
    "for label in labels:\n",
    "    if label in prediction_tree[len(prediction_tree)-1]['processed_pred']:\n",
    "        print(f'correct {i}')\n",
    "        count += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['m.03_r3']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(test_data[i]['gold_entity_map'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'prompt': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nwhere was george washington carver from<|eot_id|><|start_header_id|>assistant<|end_header_id|>',\n",
       "  'pred': '[New Retrieval]',\n",
       "  'processed_pred': '',\n",
       "  'score': None,\n",
       "  'topic_entity': ['m.03djm'],\n",
       "  'parent': None},\n",
       " 1: {'prompt': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nwhere was george washington carver from<|eot_id|><|start_header_id|>assistant<|end_header_id|>',\n",
       "  'pred': '<paragraph>location.us_state.capital;people.family.country;people.ethnicity.geographic_distribution;fictional_universe.fictional_character.place_of_birth;people.marriage.from</paragraph>people.family.country[Fully Relevant]people.ethnicity.geographic_distribution[Partially Relevant]location.us_state.capital[Unrelevant][Retrieve Entity]<paragraph>[No Retrieval]Answer: Minneapolis<|eot_id|>',\n",
       "  'score': 2.578040019318527e-07,\n",
       "  'parent': 0,\n",
       "  'topic_entity': ['m.03djm'],\n",
       "  'processed_pred': '<paragraph>location.us_state.capital;people.family.country;people.ethnicity.geographic_distribution;fictional_universe.fictional_character.place_of_birth;people.marriage.from</paragraph>people.family.country[Fully Relevant]people.ethnicity.geographic_distribution[Partially Relevant]location.us_state.capital[Unrelevant][Retrieve Entity]<paragraph>[No Retrieval]Answer: Minneapolis<|eot_id|>'}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtracking_prediction_tree(levels: dict[int,list[int]], curr_depth: int, prediction_tree: dict[int, dict]) -> dict[int,list[int]]:\n",
    "    '''\n",
    "    get best tracking from prediction_tree base on levels\n",
    "    '''\n",
    "    parent = 0 \n",
    "    best_selections = {}\n",
    "    # Traverse from the bottom \n",
    "    levels = {k: v for k, v in levels.items() if len(v) > 0 and k != 0} # remove empty list in levels\n",
    "    for path_i, node in enumerate(levels[len(levels)]): # beam search \n",
    "        if node == 0:\n",
    "            break\n",
    "        best_selections[path_i] = [node] \n",
    "        current_node = node \n",
    "        current_level = curr_depth \n",
    "        if current_node is None:\n",
    "            continue\n",
    "        while current_level > 0 and current_node is not None:\n",
    "            parent = prediction_tree[current_node][\"parent\"]\n",
    "            best_selections[path_i] = [parent] + best_selections[path_i] \n",
    "            current_node = parent \n",
    "            current_level -= 1\n",
    "    return best_selections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_selections = backtracking_prediction_tree(levels, curr_depth, prediction_tree)\n",
    "final_prediction = {}\n",
    "splitted_sentences = {}\n",
    "original_splitted_sentences = {}\n",
    "ctxs = {}\n",
    "for path_i, nodes in best_selections.items():\n",
    "    final_prediction[path_i] = \" \".join([prediction_tree[node][\"processed_pred\"] for node in nodes if node is not None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: ' <paragraph>government.us_vice_president.to_president;government.us_president.vice_president;government.politician.government_positions_held;government.political_appointer.appointees;base.obamabase.cabinet_member.cabinet_position</paragraph>None[Fully Relevant]George M. Dallas[Unrelevant][No Retrieval]Answer: Speaker of the House of Representatives;President of the House of Representatives<|eot_id|>'}\n"
     ]
    }
   ],
   "source": [
    "print(final_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat response: Completion(id='cmpl-d7ccc07b74794709a9452c6f4f43865a', choices=[CompletionChoice(finish_reason='stop', index=0, logprobs=None, text=' Thank you for purchasing this Kindle edition of The Way Life Should Be, written by Irma Coriss. Irma is a lifelong resident of South Florida and currently resides in St. Lucie County. This is her third novel and her first book in her RUMBUNCH series. The idea of this series came about as Irma wished to write stories that would make her audience believe in the goodness of life, having faith that all is not invisible, and just believing that with careful attention to detail, you, too, live as if life\\'s moments are \"The Way Life Should Be\". Hope you enjoy Irma\\'s stories and feel she has allowed all of us to believe that God\\'s love goes and on and on and today belongs to you.\\nIrma Coriss is a graduate of one of the first degrees offered to a woman. While attending college, she fell in love with writing and took some of her professors\\' favorite books to write about. Although they said not to write from your own life experiences, those stories they had read about touched her soul and she personally knew the ways it felt when living them themselves.\\nIrma was raised knowing her values as she lived in a time when families sat together for some amazing dishes that included; strawberry shortcake, poached lobster, deep fried chicken, gouge cheese, fresh picked peas and LOTS of bread.\\nIrma starts her mornings with The Lord asking that all is well in her life. Almost immediately, she is flooded with ideas. Where the characters come from, and the story is, still stuns her. Someday she would like to write young adult fiction and mystery. Although, at the moment it is a dream, she would like the same best-seller status for a character in her Rumbunch Series as Maeve Binchey, and other great authors of young adult fiction achieved.\\nOver the years, Irma has greeted her fans, even dedicating\\nan entire book to a great, \"Auntie, M.\" While in New York, she was\\ntold it was the first Best Seller signed by an Amazon author, but that\\ndoesn\\'t mean it\\'s not on the New York Times Best Seller list. Perhaps for\\nthe newest book in the RUMBUNCH series.\\n1. MOMENT OF RENEWAL\\n2. OUR VACATION\\n8. HIS MIRACLE', stop_reason=None, prompt_logprobs=None)], created=1733112950, model='/media/disk1/chatgpt/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B/snapshots/8cde5ca8380496c9a6cc7ef3a8b46a0372a1d920', object='text_completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=475, prompt_tokens=3, total_tokens=478, completion_tokens_details=None, prompt_tokens_details=None))\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "# Set OpenAI's API key and API base to use vLLM's API server.\n",
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base = \"http://localhost:8000/v1\"\n",
    "prompt = 'Hello!' \n",
    "client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")\n",
    "\n",
    "chat_response = client.completions.create(\n",
    "    model=\"/media/disk1/chatgpt/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B/snapshots/8cde5ca8380496c9a6cc7ef3a8b46a0372a1d920\",\n",
    "    prompt=prompt,\n",
    "    max_tokens=2048,\n",
    "#     extra_body={\n",
    "#     \"skip_special_tokens\": \"True\",\n",
    "#   }\n",
    ")\n",
    "print(\"Chat response:\", chat_response)\n",
    "from openai import OpenAI\n",
    "# Set OpenAI's API key and API base to use vLLM's API server.\n",
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base = \"http://localhost:8001/v1\"\n",
    "prompt = 'what country is the grand bahama island in?' \n",
    "client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")\n",
    "\n",
    "chat_response = client.chat.completions.create(\n",
    "    model=\"/media/disk2/llama_factory/generation_1124_special/\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ],\n",
    "    extra_body={\n",
    "    \"skip_special_tokens\": \"False\",\n",
    "    #  \"logprobs\": \"True\",\n",
    "  }\n",
    ")\n",
    "print(\"Chat response:\", chat_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from src.sparql_utils import *\n",
    "def run_relation_generation_batch(model, prompt, new_retrieval, context):\n",
    "    rel_score_dict = {}\n",
    "    final_preds = []\n",
    "    overall_scores = {}\n",
    "    paragraph = ';'.join([page.page_content.strip() for page in retriever.invoke(prompt.split('\\n\\n')[1].split('<|eot_id|>')[0])])\n",
    "    if new_retrieval:\n",
    "        retrieval_token = \"[New Retrieval]\"\n",
    "        aug_prompts =  [\"<paragraph>{}</paragraph>\".format(paragraph)]\n",
    "    else:\n",
    "        retrieval_token = \"[Continue to Retrieve Evidence]\"\n",
    "        aug_prompts =  [\"<paragraph>{}</paragraph>\".format(paragraph)]\n",
    "    \n",
    "    pred = model.generate(prompt + retrieval_token + aug_prompts[0], sampling_params)[0]\n",
    "    pred_token_ids = pred.outputs[0].token_ids\n",
    "    pred_text_1 = pred.outputs[0].text\n",
    "    pred_log_probs = pred.outputs[0].logprobs\n",
    "    seq_score = pred.outputs[0].cumulative_logprob / \\\n",
    "        max(len(pred.outputs[0].token_ids), 1)\n",
    "    relevance_indices = []\n",
    "    for tok_idx, tok in enumerate(pred_token_ids):\n",
    "        if tok in rel_tokens.values():\n",
    "            relevance_indices.append(tok_idx)\n",
    "    if len(relevance_indices) > 0:\n",
    "        for idx in relevance_indices:\n",
    "            for token, token_id in rel_tokens.items():\n",
    "                prob = pred_log_probs[idx][token_id].logprob if token_id in pred_log_probs[idx] else -100\n",
    "                rel_score_dict[token] = np.exp(prob)\n",
    "    relevance_score = rel_score_dict['[Fully Relevant]']+ rel_score_dict['[Partially Relevant]'] / np.sum(list(rel_score_dict.values()))\n",
    "    assert '[Retrieve Entity]' in pred_text_1\n",
    "    processed_pred = pred_text_1.split('[Retrieve Entity]')[0] + '[Retrieve Entity]'\n",
    "    return [retrieval_token + aug_prompts[0] + processed_pred], [relevance_score], pred_text_1.split('[Retrieve Entity]')[0]\n",
    "\n",
    "def run_entity_generation_batch(model, prompt, topic_entity, context):\n",
    "    final_preds = []\n",
    "    overall_scores = {}\n",
    "    return_entities = []\n",
    "    pattern = r'(.*?)\\[(.*?)\\]'\n",
    "    matches =  dict(re.findall(pattern,context))\n",
    "    name2id = dict()\n",
    "    entity_prompts = []\n",
    "    for _, entity in enumerate(topic_entity):\n",
    "        entities = []\n",
    "        for k, v in matches.items():\n",
    "            if v in ['Fully Relevant', 'Partially Relevant']:\n",
    "                another_entities = get_another_entity(entity, k, return_label=True)\n",
    "                # print(another_entities)\n",
    "                name2id.update(another_entities)\n",
    "                entities.extend([f'({get_label(entity)}, {k}, {e})' for e in another_entities.values()])\n",
    "        \n",
    "        entity_prompts.append(prompt+  '[Retrieve Entity]' + \"<paragraph>{}</paragraph>\".format(';'.join(entities[:10]) if len(entities) else 'No triplets Received'))\n",
    "    # print(aug_prompts)\n",
    "    preds = model.generate([prompt + entity_prompts[i] for i in range(len(entity_prompts))], sampling_params)\n",
    "    \n",
    "    for p_idx, pred in enumerate(preds):\n",
    "        pred_token_ids = pred.outputs[0].token_ids\n",
    "        pred_text_2 = pred.outputs[0].text\n",
    "        pred_log_probs = pred.outputs[0].logprobs\n",
    "        rel_score_dict = {}\n",
    "        relevance_indices = []\n",
    "        for tok_idx, tok in enumerate(pred_token_ids):\n",
    "            if tok in rel_tokens.values():\n",
    "                relevance_indices.append(tok_idx)\n",
    "        if len(relevance_indices) > 0:\n",
    "            # print(relevance_indices)\n",
    "            for idx in relevance_indices:\n",
    "                for token, token_id in rel_tokens.items():\n",
    "                    prob = pred_log_probs[idx][token_id].logprob if token_id in pred_log_probs[idx] else -100\n",
    "                    rel_score_dict[token] = np.exp(prob)\n",
    "        overall_scores[p_idx] = rel_score_dict['[Fully Relevant]'] + rel_score_dict['[Partially Relevant]']/ np.sum(list(rel_score_dict.values()))\n",
    "        if '[Continue to Retrieve Evidence]' in pred_text_2:\n",
    "            processed_pred = pred_text_2.split('[Continue to Retrieve Evidence]')[0]\n",
    "            matches =  dict(re.findall(pattern, processed_pred))\n",
    "            for k, v in matches.items():\n",
    "                if v in ['Fully Relevant', 'Partially Relevant']:\n",
    "                    if k in name2id:\n",
    "                        return_entities.append(name2id[k])\n",
    "            processed_pred += '[Continue to Retrieve Evidence]'\n",
    "        else:\n",
    "            processed_pred = pred_text_2\n",
    "        final_preds.append(processed_pred)\n",
    "    return final_preds, [overall_scores[p_idx] for p_idx in overall_scores], return_entities, \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.55s/it, est. speed input: 42.65 toks/s, output: 64.61 toks/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:01<00:00,  1.26it/s, est. speed input: 347.04 toks/s, output: 126.19 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.51s/it, est. speed input: 98.42 toks/s, output: 66.50 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.73it/s, est. speed input: 133.39 toks/s, output: 65.83 toks/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:01<00:00,  1.33it/s, est. speed input: 278.54 toks/s, output: 112.08 toks/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:01<00:00,  1.26it/s, est. speed input: 552.91 toks/s, output: 125.95 toks/s]\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for index in range(len(test_data)):\n",
    "# index=7\n",
    "    print(f'Process {index}')\n",
    "    data_input = test_data[index]['question']\n",
    "    prompt = PROMPT_DICT['llama3'].format(input= data_input)\n",
    "    max_depth = 5\n",
    "    topic_entity = list(test_data[index]['gold_entity_map'].keys())\n",
    "    # pred = model.generate([prompt], sampling_params)[0]\n",
    "    # pred_text = pred.outputs[0].text\n",
    "    # if '[New Retrieval]' in pred_text:\n",
    "    curr_depth = 1\n",
    "    terminated = False\n",
    "    node_id = 0\n",
    "    prediction_tree = {}\n",
    "    levels = {}\n",
    "    prediction_tree[node_id] = {\"prompt\": prompt, \"pred\": \"[New Retrieval]\",\n",
    "                                \"processed_pred\": \"\", \"score\": None, \"topic_entity\": topic_entity, \"parent\": None, \"context\": None}\n",
    "    levels[0] = [0]\n",
    "    while curr_depth < max_depth:\n",
    "        levels[curr_depth] = []\n",
    "        if curr_depth-1 in levels and terminated is False:\n",
    "            for node in levels[curr_depth-1]:\n",
    "                curr_pred = prediction_tree[node][\"pred\"]\n",
    "                if \"<|eot_id|>\" in curr_pred:\n",
    "                    terminated = True\n",
    "                    continue\n",
    "                prompt = prediction_tree[node][\"prompt\"]\n",
    "                prev_generation = prediction_tree[node][\"processed_pred\"]\n",
    "                score = prediction_tree[node][\"score\"]\n",
    "                topic_entity = prediction_tree[node][\"topic_entity\"]\n",
    "                context = prediction_tree[node]['context']\n",
    "                if \"Retrieve Entity\" in curr_pred.split('[')[-1]:\n",
    "                    retrieval_results = {}\n",
    "                    preds, scores, next_entities = run_entity_generation_batch(\n",
    "                        model, prompt + prev_generation, topic_entity, context)\n",
    "                    for i, (pred, p_score) in enumerate(zip(preds, scores)):\n",
    "                        retrieval_results[i] = {\n",
    "                            \"pred\": pred, \"score\": p_score}\n",
    "\n",
    "                    for i, result in retrieval_results.items():\n",
    "                        node_id += 1\n",
    "                        node_score = result[\"score\"] * \\\n",
    "                            score if score is not None else result[\"score\"]\n",
    "                        pred = result[\"pred\"]\n",
    "                        if len(next_entities) == 0:\n",
    "                            next_entities = topic_entity\n",
    "                        prediction_tree[node_id] = {\"prompt\": prompt + prev_generation, \"pred\": pred, \"context\": next_entities,\n",
    "                                                    \"score\": node_score, \"parent\": node,\n",
    "                                                    \"topic_entity\": next_entities}\n",
    "                        if \"[Continue to Retrieve Evidence]\" in pred:\n",
    "                            gen_result_index = pred.index(\"[Continue to Retrieve Evidence]\")\n",
    "                            prev_generation = pred[:gen_result_index]\n",
    "                        else:\n",
    "                            prev_generation = pred\n",
    "                        prediction_tree[node_id][\"processed_pred\"] = prev_generation\n",
    "                        levels[curr_depth].append(node_id)\n",
    "                #存在前后逻辑粘连   \n",
    "                if \"New Retrieval\" in curr_pred.split('[')[-1] or \"Continue to Retrieve Evidence\" in curr_pred.split('[')[-1]:\n",
    "                    retrieval_results = {}\n",
    "                    preds, scores, context = run_relation_generation_batch(\n",
    "                        model, prompt + prev_generation, new_retrieval=True if (\"[New Retrieval]\" in curr_pred) else False, context=context)\n",
    "                    for i, (pred, p_score) in enumerate(zip(preds, scores)):\n",
    "                        retrieval_results[i] = {\n",
    "                            \"pred\": pred, \"score\": p_score}\n",
    "\n",
    "                    for i, result in retrieval_results.items():\n",
    "                        node_id += 1\n",
    "                        node_score = result[\"score\"] * \\\n",
    "                            score if score is not None else result[\"score\"]\n",
    "                        pred = result[\"pred\"]\n",
    "                        prediction_tree[node_id] = {\"prompt\": prompt + prev_generation, \"pred\": pred,\n",
    "                                                    \"score\": node_score, \"parent\": node,\n",
    "                                                    \"topic_entity\": topic_entity, \"context\": context}\n",
    "                        if \"[Retrieve Entity]\" in pred:\n",
    "                            gen_result_index = pred.index(\"[Retrieve Entity]\")\n",
    "                            prev_generation = pred[:gen_result_index]\n",
    "                        else:\n",
    "                            prev_generation = pred\n",
    "                        prediction_tree[node_id][\"processed_pred\"] = prev_generation\n",
    "                        levels[curr_depth].append(node_id)\n",
    "            current_rank = levels[curr_depth]\n",
    "            node2score = {\n",
    "                node_id: prediction_tree[node_id][\"score\"] for node_id in current_rank}\n",
    "            top_nodes = sorted(node2score.items(), key=lambda x: x[1], reverse=True)[\n",
    "                :2]\n",
    "            levels[curr_depth] = [node[0] for node in top_nodes]\n",
    "            curr_depth += 1\n",
    "        else:\n",
    "            break\n",
    "    labels = [get_label(ans) for ans in test_data[index]['answer']]\n",
    "    for label in labels:\n",
    "        if label in prediction_tree[len(prediction_tree)-1]['processed_pred']:\n",
    "            count += 1\n",
    "            break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [0], 1: [1], 2: [2, 4], 3: []}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'prompt': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nwho is governor of ohio 2011<|eot_id|><|start_header_id|>assistant<|end_header_id|>',\n",
       "  'pred': '[New Retrieval]',\n",
       "  'processed_pred': '',\n",
       "  'score': None,\n",
       "  'topic_entity': ['m.05kkh', 'm.0fkvn'],\n",
       "  'parent': None,\n",
       "  'context': None},\n",
       " 1: {'prompt': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nwho is governor of ohio 2011<|eot_id|><|start_header_id|>assistant<|end_header_id|>',\n",
       "  'pred': '[New Retrieval]<paragraph>government.us_vice_president.to_president;government.us_president.vice_president;government.government_position_held.district_represented;government.election.winner;government.politician.election_campaigns</paragraph>government.election.winner[Fully Relevant]government.government_position_held.district_represented[Partially Relevant]government.politician.election_campaigns[Partially Relevant][Retrieve Entity]',\n",
       "  'score': 0.999987954659157,\n",
       "  'parent': 0,\n",
       "  'topic_entity': ['m.05kkh', 'm.0fkvn'],\n",
       "  'context': 'government.election.winner[Fully Relevant]government.government_position_held.district_represented[Partially Relevant]government.politician.election_campaigns[Partially Relevant]',\n",
       "  'processed_pred': '[New Retrieval]<paragraph>government.us_vice_president.to_president;government.us_president.vice_president;government.government_position_held.district_represented;government.election.winner;government.politician.election_campaigns</paragraph>government.election.winner[Fully Relevant]government.government_position_held.district_represented[Partially Relevant]government.politician.election_campaigns[Partially Relevant]'},\n",
       " 2: {'prompt': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nwho is governor of ohio 2011<|eot_id|><|start_header_id|>assistant<|end_header_id|>[New Retrieval]<paragraph>government.us_vice_president.to_president;government.us_president.vice_president;government.government_position_held.district_represented;government.election.winner;government.politician.election_campaigns</paragraph>government.election.winner[Fully Relevant]government.government_position_held.district_represented[Partially Relevant]government.politician.election_campaigns[Partially Relevant]',\n",
       "  'pred': 'None[Unrelavant][Continue to Retrieve Evidence]',\n",
       "  'context': ['m.05kkh', 'm.0fkvn'],\n",
       "  'score': 0.9706760750329039,\n",
       "  'parent': 1,\n",
       "  'topic_entity': ['m.05kkh', 'm.0fkvn'],\n",
       "  'processed_pred': 'None[Unrelavant]'},\n",
       " 3: {'prompt': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nwho is governor of ohio 2011<|eot_id|><|start_header_id|>assistant<|end_header_id|>None[Unrelavant]',\n",
       "  'pred': 'No triplets[Unrelevant][Continue to Retrieve Evidence]',\n",
       "  'context': ['m.05kkh', 'm.0fkvn'],\n",
       "  'score': 3.7265942056393588e-06,\n",
       "  'parent': 1,\n",
       "  'topic_entity': ['m.05kkh', 'm.0fkvn'],\n",
       "  'processed_pred': 'No triplets[Unrelevant]'},\n",
       " 4: {'prompt': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nwho is governor of ohio 2011<|eot_id|><|start_header_id|>assistant<|end_header_id|>[New Retrieval]<paragraph>government.us_vice_president.to_president;government.us_president.vice_president;government.government_position_held.district_represented;government.election.winner;government.politician.election_campaigns</paragraph>government.election.winner[Fully Relevant]government.government_position_held.district_represented[Partially Relevant]government.politician.election_campaigns[Partially Relevant]None[Unrelavant]',\n",
       "  'pred': '[Continue to Retrieve Evidence]<paragraph>government.us_vice_president.to_president;government.us_president.vice_president;government.government_position_held.district_represented;government.election.winner;government.politician.election_campaigns</paragraph>government.election.winner[Fully Relevant]government.government_position_held.district_represented[Partially Relevant]government.politician.election_campaigns[Partially Relevant][Retrieve Entity]',\n",
       "  'score': 8.340210562762611e-10,\n",
       "  'parent': 2,\n",
       "  'topic_entity': ['m.05kkh', 'm.0fkvn'],\n",
       "  'context': 'government.election.winner[Fully Relevant]government.government_position_held.district_represented[Partially Relevant]government.politician.election_campaigns[Partially Relevant]',\n",
       "  'processed_pred': '[Continue to Retrieve Evidence]<paragraph>government.us_vice_president.to_president;government.us_president.vice_president;government.government_position_held.district_represented;government.election.winner;government.politician.election_campaigns</paragraph>government.election.winner[Fully Relevant]government.government_position_held.district_represented[Partially Relevant]government.politician.election_campaigns[Partially Relevant]'},\n",
       " 5: {'prompt': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nwho is governor of ohio 2011<|eot_id|><|start_header_id|>assistant<|end_header_id|>None[Unrelavant]No triplets[Unrelevant]',\n",
       "  'pred': '[Continue to Retrieve Evidence]<paragraph>government.us_vice_president.to_president;government.us_president.vice_president;government.government_position_held.district_represented;government.election.winner;government.politician.election_campaigns</paragraph>government.election.winner[Fully Relevant]government.us_vice_president.to_president[Partially Relevant]government.politician.election_campaigns[Partially Relevant][Retrieve Entity]',\n",
       "  'score': 3.7265930656654847e-06,\n",
       "  'parent': 3,\n",
       "  'topic_entity': ['m.05kkh', 'm.0fkvn'],\n",
       "  'context': 'government.election.winner[Fully Relevant]government.us_vice_president.to_president[Partially Relevant]government.politician.election_campaigns[Partially Relevant]',\n",
       "  'processed_pred': '[Continue to Retrieve Evidence]<paragraph>government.us_vice_president.to_president;government.us_president.vice_president;government.government_position_held.district_represented;government.election.winner;government.politician.election_campaigns</paragraph>government.election.winner[Fully Relevant]government.us_vice_president.to_president[Partially Relevant]government.politician.election_campaigns[Partially Relevant]'},\n",
       " 6: {'prompt': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nwho is governor of ohio 2011<|eot_id|><|start_header_id|>assistant<|end_header_id|>None[Unrelavant]No triplets[Unrelevant][Continue to Retrieve Evidence]<paragraph>government.us_vice_president.to_president;government.us_president.vice_president;government.government_position_held.district_represented;government.election.winner;government.politician.election_campaigns</paragraph>government.election.winner[Fully Relevant]government.us_vice_president.to_president[Partially Relevant]government.politician.election_campaigns[Partially Relevant]',\n",
       "  'pred': 'No triplets[Unrelevant][Continue to Retrieve Evidence]',\n",
       "  'context': ['m.05kkh', 'm.0fkvn'],\n",
       "  'score': 3.6173583025052717e-06,\n",
       "  'parent': 5,\n",
       "  'topic_entity': ['m.05kkh', 'm.0fkvn'],\n",
       "  'processed_pred': 'No triplets[Unrelevant]'},\n",
       " 7: {'prompt': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nwho is governor of ohio 2011<|eot_id|><|start_header_id|>assistant<|end_header_id|>None[Unrelavant]No triplets[Unrelevant]No triplets[Unrelevant]',\n",
       "  'pred': 'No triplets[Unrelevant][Continue to Retrieve Evidence]',\n",
       "  'context': ['m.05kkh', 'm.0fkvn'],\n",
       "  'score': 2.2028147914635697e-08,\n",
       "  'parent': 5,\n",
       "  'topic_entity': ['m.05kkh', 'm.0fkvn'],\n",
       "  'processed_pred': 'No triplets[Unrelevant]'},\n",
       " 8: {'prompt': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nwho is governor of ohio 2011<|eot_id|><|start_header_id|>assistant<|end_header_id|>[New Retrieval]<paragraph>government.us_vice_president.to_president;government.us_president.vice_president;government.government_position_held.district_represented;government.election.winner;government.politician.election_campaigns</paragraph>government.election.winner[Fully Relevant]government.government_position_held.district_represented[Partially Relevant]government.politician.election_campaigns[Partially Relevant]None[Unrelavant][Continue to Retrieve Evidence]<paragraph>government.us_vice_president.to_president;government.us_president.vice_president;government.government_position_held.district_represented;government.election.winner;government.politician.election_campaigns</paragraph>government.election.winner[Fully Relevant]government.government_position_held.district_represented[Partially Relevant]government.politician.election_campaigns[Partially Relevant]',\n",
       "  'pred': 'None[Unrelavant][Continue to Retrieve Evidence]',\n",
       "  'context': ['m.05kkh', 'm.0fkvn'],\n",
       "  'score': 8.340206356397284e-10,\n",
       "  'parent': 4,\n",
       "  'topic_entity': ['m.05kkh', 'm.0fkvn'],\n",
       "  'processed_pred': 'None[Unrelavant]'},\n",
       " 9: {'prompt': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nwho is governor of ohio 2011<|eot_id|><|start_header_id|>assistant<|end_header_id|>[New Retrieval]<paragraph>government.us_vice_president.to_president;government.us_president.vice_president;government.government_position_held.district_represented;government.election.winner;government.politician.election_campaigns</paragraph>government.election.winner[Fully Relevant]government.government_position_held.district_represented[Partially Relevant]government.politician.election_campaigns[Partially Relevant]None[Unrelavant]None[Unrelavant]',\n",
       "  'pred': 'No triplets[Unrelavant][Continue to Retrieve Evidence]',\n",
       "  'context': ['m.05kkh', 'm.0fkvn'],\n",
       "  'score': 6.579813904757589e-15,\n",
       "  'parent': 4,\n",
       "  'topic_entity': ['m.05kkh', 'm.0fkvn'],\n",
       "  'processed_pred': 'No triplets[Unrelavant]'}}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_tree"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "self-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
