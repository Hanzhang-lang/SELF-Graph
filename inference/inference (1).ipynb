{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain.embeddings import (\n",
    "    HuggingFaceEmbeddings,\n",
    ")\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"2b219db0d2984f9dae28b651ab8ab3d9\"\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://smsh.openai.azure.com/\"\n",
    "os.environ[\"AZURE_OPENAI_API_VERSION\"] = \"2024-03-01-preview\"\n",
    "# embeddings = AzureOpenAIEmbeddings(\n",
    "#     model=\"text-embedding-3-small\",\n",
    "# )\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "                model_name=\"BAAI/bge-large-en-v1.5\",\n",
    "                # model_kwargs={'device': 'cpu'},\n",
    "                encode_kwargs={'normalize_embeddings': False},\n",
    "            )\n",
    "from langchain.storage import LocalFileStore, RedisStore\n",
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "store = RedisStore(redis_url=\"redis://localhost:6379\")\n",
    "cached_embedder = CacheBackedEmbeddings.from_bytes_store(\n",
    "embeddings, store, namespace=\"bge-large\"\n",
    ")\n",
    "row_string = []\n",
    "with open('../data/clean_relations', 'r') as f:\n",
    "    r_data = [line.strip() for line in f]\n",
    "all_db = FAISS.from_texts(r_data, cached_embedder)\n",
    "# retriever = db.as_retriever(search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create with GPT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "import os\n",
    "\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"2b219db0d2984f9dae28b651ab8ab3d9\"\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://smsh.openai.azure.com/\"\n",
    "os.environ[\"AZURE_OPENAI_API_VERSION\"] = \"2024-02-01\"\n",
    "os.environ[\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"] = \"gpt-35-turbo\"\n",
    "\n",
    "model = AzureChatOpenAI(\n",
    "    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    "    azure_deployment=os.environ[\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"],\n",
    "    temperature=1,\n",
    "    n = 3,\n",
    "    max_retries=5, request_timeout=600\n",
    ")\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1289554/3707533165.py:28: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  llm_chain = LLMChain(llm=model, prompt=few_shot_intepretable_prompt, verbose=True)\n"
     ]
    }
   ],
   "source": [
    "# The name of Justin Bieber's brother is Jaxon Bieber. This is based on the reasoning path that connects Justin Bieber to Jaxon Bieber through the relationship of sibling. The other paths that connect Justin Bieber to Jazmyn Bieber or back to Justin Bieber himself are incorrect in this context.\n",
    "few_shot_intepretable_prompt = FewShotPromptTemplate(\n",
    "        examples=[{\n",
    "            \"query\": \"Who was the prime minister of Japan in 2011, that served in the New Party Sakigake?\",\n",
    "            \"evidence\": \"Relations retrieved: language.human_language.countries_spoken_in\\n Entities retrieved: Japan\",\n",
    "            \"preceding_sentences\": \"\",\n",
    "            \"output\": \"Naoto Kan\",\n",
    "            \"rating\": \"[Continue to Retrieve Evidence]\"\n",
    "        }],\n",
    "        example_prompt=PromptTemplate.from_template(\"\"\"Query: {query}\n",
    "Preceding sentences: {preceding_sentences}\n",
    "Evidence: {evidence}\n",
    "Output: {target_output}\n",
    "Rating: {rating}\"\"\"),\n",
    "        prefix=\n",
    "        \"You will be provided with a query, evidence, output sentence, and preceding sentences (optional). Your task is to determine whether the information in the output sentence can be fully verified by the evidence or if it requires further external verification. There are three cases:\\n\" \n",
    "        \"- If the output sentence can be verified solely with the evidence and the preceding sentences, then respond with [No Retrieval]. \\n\"\n",
    "        \"- If additional information about the tail entity in evidence is needed to verify the output sentence, respond with [Continue to Retrieve Evidence] \\n\"\n",
    "        \"- If more information unrelated to the evidence is needed, e.g. totally new relationship or new entity, reponse with [New Retrieval].\\n\\n\",\n",
    "        suffix=\n",
    "        \"Query: {query}\\n\"\n",
    "        \"Preceding sentences: {preceding_sentences}\\n\"\n",
    "        \"Evidence: {evidence}\\n\"\n",
    "        \"Output: {target_output}\",\n",
    "        input_variables=[\"query\", \"evidence\", \"preceding_sentences\", \"topic\"],\n",
    ")\n",
    "\n",
    "llm_chain = LLMChain(llm=model, prompt=few_shot_intepretable_prompt, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relationship relevance\n",
    "graph_intepretable =  \"\"\"You will receive a query, evidence and optional preceding sentences containing history information. The evidence contains graph relationships or entities may be useful to answer the query. Your task is to filters out valid information from the evidence to answer the given query, evaluate your output and provide explanations on your result.\n",
    "\n",
    "###\n",
    "Query: Name the president of the country whose main spoken language was Brahui in 1980?\n",
    "Topic Entity: Brahui Language\n",
    "Evidence: language.human_language.main_country; language.human_language.language_family; language.human_language.iso_639_3_code; base.rosetta.languoid.parent; language.human_language.writing_system; base.rosetta.languoid.languoid_class; language.human_language.countries_spoken_in; kg.object_profile.prominent_type; base.rosetta.languoid.document; base.ontologies.ontology_instance.equivalent_instances; base.rosetta.languoid.local_name; language.human_language.region\n",
    "Preceding sentences: \n",
    "Output: \n",
    "1. {{language.human_language.main_country (Score: Fully relavant))}}: This relation is highly relevant as it directly relates to the country whose president is being asked for, and the main country where Brahui language is spoken in 1980.\n",
    "2. {{language.human_language.countries_spoken_in (Score: Fully relavant)}}: This relation is also relevant as it provides information on the countries where Brahui language is spoken, which could help narrow down the search for the president.\n",
    "3. {{base.rosetta.languoid.parent (Score: Partially relevant)}}: This relation is less relevant but still provides some context on the language family to which Brahui belongs, which could be useful in understanding the linguistic and cultural background of the country in question.\n",
    "\n",
    "###\n",
    "Query: {query}\n",
    "Topic Entity: {topic}\n",
    "Evidence: {evidence}\n",
    "Preceding sentences: {preceding_sentences}\n",
    "Output: \n",
    "\"\"\"\n",
    "# The name of Justin Bieber's brother is Jaxon Bieber. This is based on the reasoning path that connects Justin Bieber to Jaxon Bieber through the relationship of sibling. The other paths that connect Justin Bieber to Jazmyn Bieber or back to Justin Bieber himself are incorrect in this context.\n",
    "few_shot_intepretable_prompt = FewShotPromptTemplate(\n",
    "        examples=[{\n",
    "            \"query\": \"Name the president of the country whose main spoken language was Brahui in 1980?\",\n",
    "            \"topic\": \"Brahui Language\",\n",
    "            \"evidence\": \"language.human_language.main_country; language.human_language.language_family; language.human_language.iso_639_3_code; base.rosetta.languoid.parent; language.human_language.writing_system; base.rosetta.languoid.languoid_class; language.human_language.countries_spoken_in; kg.object_profile.prominent_type; base.rosetta.languoid.document; base.ontologies.ontology_instance.equivalent_instances; base.rosetta.languoid.local_name; language.human_language.region\",\n",
    "            \"preceding_sentences\": \"\",\n",
    "            \"output\": \"\"\"1. {{language.human_language.main_country (Score: [Fully Relavant])}}: This relation is highly relevant as it directly relates to the country whose president is being asked for, and the main country where Brahui language is spoken in 1980.\n",
    "2. {{language.human_language.countries_spoken_in (Score: [Fully Relavant])}}: This relation is also relevant as it provides information on the countries where Brahui language is spoken, which could help narrow down the search for the president.\n",
    "3. {{base.rosetta.languoid.parent (Score: [Partially Relevant])}}: This relation is less relevant but still provides some context on the language family to which Brahui belongs, which could be useful in understanding the linguistic and cultural background of the country in question.\"\"\"\n",
    "        }],\n",
    "        example_prompt=PromptTemplate.from_template(\"\"\"###\n",
    "Query: {query}\n",
    "Topic Entity: {topic}\n",
    "Evidence: {evidence}\n",
    "Preceding sentences: {preceding_sentences}\n",
    "Output: {output}\n",
    "\"\"\"),\n",
    "        prefix=\n",
    "        \"\"\"You will receive a query, topic entity, evidence and optional preceding sentences containing history information. The evidence contains graph relationships or entities may be useful to answer the query. Your task is to filters out 3 valid information from the evidence that contribute to answering the query and provide a relevance score for each output, output your explanations for the score.\n",
    "The score of relevance range from [Fully Relavant], [Partially Relevant] to [Unrelevant].\"\"\",\n",
    "        suffix=\n",
    "        \"\"\"###\n",
    "Query: {query}\n",
    "Topic Entity: {topic}\n",
    "Evidence: {evidence}\n",
    "Preceding sentences: {preceding_sentences}\n",
    "Output: \"\"\",\n",
    "        input_variables=[\"query\", \"evidence\", \"preceding_sentences\", \"topic\"],\n",
    ")\n",
    "graph_intepretable_prompt = PromptTemplate(input_variables=[\"query\", \"evidence\", \"preceding_sentences\", \"topic\"], template=\n",
    "graph_intepretable)\n",
    "llm_chain = LLMChain(llm=model, prompt=few_shot_intepretable_prompt, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "few_shot_intepretable_prompt = FewShotPromptTemplate(\n",
    "        examples=[{\n",
    "            \"query\": \"what is the name of justin bieber brother?\",\n",
    "            \"output\": \"[Retreive New Relationship]<paragraph>people.sibling_relationship.sibling;fictional_universe.fictional_character.siblings;fictional_universe.sibling_relationship_of_fictional_characters.siblings;people.person.sibling_s;people.family.members;people.person.parents</paragraph>Retrieved relationship: people.person.parents[Fully Relevant][Retrieve Entity]<paragraph>(Justin Bieber, people.person.parents, Pattie Mallette);(Justin Bieber, people.person.parents, Jeremy Bieber);(Justin Bieber, people.person.parents, Jeremy Bieber)</paragraph>Retrieved triplet: (Justin Bieber, people.person.parents, Jeremy Bieber)[Fully Relevant][Continue to Retrieve Evidence]<paragraph>people.sibling_relationship.sibling;people.person.sibling_s;people.person.parents;fictional_universe.fictional_character.siblings;people.family.members;people.person.children</paragraph>Retrieved relationship: people.person.children[Fully Relevant][Retrieve Entity]<paragraph>(Jeremy Bieber, people.person.children, Jazmyn Bieber);(Jeremy Bieber, people.person.children, Justin Bieber);(Jeremy Bieber, people.person.children, Jaxon Bieber);(Jeremy Bieber, people.person.children, Jaxon Bieber)</paragraph>Retrieved triplet: (Jeremy Bieber, people.person.children, Jaxon Bieber)[Fully Relevant][No Retrieval] Answer: Jaxon Bieber\",\n",
    "            \"explanation\": \"The output provides the name of justin bieber's brother. This is based on the reasoning path that connects Justin Bieber to Jaxon Bieber through the relationship of sibling. The other paths that connect Justin Bieber to Jazmyn Bieber or back to Justin Bieber himself are incorrect in this context.\",\n",
    "            \"rating\": \"[Confidence:5]\"\n",
    "        }],\n",
    "        example_prompt=PromptTemplate.from_template(\"\"\"\n",
    "Query: {query}\\n\n",
    "Output: {output}\n",
    "Explanation: {explanation}\n",
    "Rating: {rating}\n",
    "\"\"\"),\n",
    "        prefix=\"\"\"Given a query and an output, rate whether the response and the thought process appears to be a helpful and informative answer to the query, from 1 (lowest) - 5 (highest). We call this confidence score.\n",
    "[Confidence:5]: The response provides a complete and correct reasoning chain to the query, and the final answer is complete and logically correct.\n",
    "[Confidence:4]: The response mostly fulfills the need in the query and provides correct answers, while there can be some minor improvements such as shorter reasoning chain or less repetition.\n",
    "[Confidence:3]: The response is acceptable, but the answers may be not complete or needs minor improvement.\n",
    "[Confidence:2]: The reasoning process still addresses the main request, but the answers are not correct or not relevant to the query.\n",
    "[Confidence:1]: The reasoning is barely irrelevant or does not give an answer in the end.\"\"\",\n",
    "        suffix=\n",
    "        \"\"\"\n",
    "Query: {query}\\n\n",
    "Output: {output}\\n\"\"\",\n",
    "        input_variables=[\"query\", \"output\"],\n",
    ")\n",
    "# confidence_prompt = PromptTemplate(input_variables=[\"query\", \"output\"], template=\n",
    "# graph_intepretable)\n",
    "llm_chain = LLMChain(llm=model, prompt=few_shot_intepretable_prompt, verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run_long_form answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 01-25 17:23:26 cuda.py:22] You are using a deprecated `pynvml` package. Please install `nvidia-ml-py` instead, and make sure to uninstall `pynvml`. When both of them are installed, `pynvml` will take precedence and cause errors. See https://pypi.org/project/pynvml for more information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 17:23:26,835\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-25 17:23:31 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='/media/disk2/llama_factory/generation_0122_no_mask', speculative_config=None, tokenizer='/media/disk2/llama_factory/generation_0122_no_mask', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/media/disk2/llama_factory/generation_0122_no_mask, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "INFO 01-25 17:23:32 model_runner.py:1056] Starting to load model /media/disk2/llama_factory/generation_0122_no_mask...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:00,  5.56it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:00<00:00,  2.05it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  1.56it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.41it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.59it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-25 17:23:35 model_runner.py:1067] Loading model weights took 14.9605 GB\n",
      "INFO 01-25 17:23:36 gpu_executor.py:122] # GPU blocks: 9667, # CPU blocks: 2048\n",
      "INFO 01-25 17:23:36 gpu_executor.py:126] Maximum concurrency for 8192 tokens per request: 18.88x\n",
      "INFO 01-25 17:23:37 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 01-25 17:23:37 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 01-25 17:23:51 model_runner.py:1523] Graph capturing finished in 14 secs.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "# model = LLM(model='/media/disk2/llama_factory/generation_1124_special', trust_remote_code=True, tensor_parallel_size=4)\n",
    "# model = LLM(model='/media/disk2/llama_factory/generation_1209_no_mask', trust_remote_code=True, tensor_parallel_size=1)\n",
    "# model = LLM(model='/media/disk1/chatgpt/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B/snapshots/8cde5ca8380496c9a6cc7ef3a8b46a0372a1d920', trust_remote_code=True, tensor_parallel_size=1)\n",
    "\n",
    "model = LLM(model='/media/disk2/llama_factory/generation_0122_no_mask', trust_remote_code=True, tensor_parallel_size=1)\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('/media/disk2/llama_factory/generation_0122_no_mask')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.52s/it, est. speed input: 11.17 toks/s, output: 65.71 toks/s]\n"
     ]
    }
   ],
   "source": [
    "#是否带special token 分开传入，用于检索和生成\n",
    "sampling_params = SamplingParams(\n",
    "            temperature=0.0, top_p=1.0,max_tokens=100, logprobs=5, skip_special_tokens=False, include_stop_str_in_output=True)\n",
    "PROMPT_DICT = {\"llama3\": '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n{input}<|eot_id|><|start_header_id|>assistant<|end_header_id|>'}\n",
    "test = model.generate([PROMPT_DICT[\"llama3\"].format(input=\"what all does google now do?\")], sampling_params)[0].outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_special_tokens(tokenizer, use_grounding=False, use_utility=False):\n",
    "    rel_tokens = {}\n",
    "    for token in ['[Unrelevant]','[Partially Relevant]','[Fully Relevant]']:\n",
    "        rel_tokens[token] = tokenizer.convert_tokens_to_ids(token)\n",
    "    reason_tokens = {}\n",
    "    for token in ['[Fully Reasonable]', '[Partially Reasonable]', '[Unreasonable]']:\n",
    "        reason_tokens[token] = tokenizer.convert_tokens_to_ids(token)\n",
    "    # ut_tokens = None\n",
    "    # if use_utility is True:\n",
    "    #     ut_tokens = {}\n",
    "    #     for token in utility_tokens_names:\n",
    "    #         ut_tokens[token] = tokenizer.convert_tokens_to_ids(token)\n",
    "\n",
    "    return rel_tokens, reason_tokens\n",
    "rel_tokens, reason_tokens = load_special_tokens(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from src.sparql_utils import *\n",
    "def run_step_generation_batch(model, prompt, topic_entity,new_retrieval, beam_width=3):\n",
    "    pattern = r'(.*?)\\[(.*?)\\]'\n",
    "    rel_score_dict = {}\n",
    "    return_entities = []\n",
    "    final_preds = []\n",
    "    overall_scores = {}\n",
    "    paragraph = ';'.join([page.page_content.strip() for page in retriever.invoke(prompt.split('\\n\\n')[1].split('<|eot_id|>')[0])])\n",
    "    print(paragraph)\n",
    "    if new_retrieval:\n",
    "        retrieval_token = \"[New Retrieval]\"\n",
    "        aug_prompts =  [\"<paragraph>{}</paragraph>\".format(paragraph)]\n",
    "    else:\n",
    "        retrieval_token = \"[Continue to Retrieve Evidence]\"\n",
    "        aug_prompts =  [\"<paragraph>{}</paragraph>\".format(paragraph)]\n",
    "    \n",
    "    pred = model.generate(prompt + retrieval_token + aug_prompts[0], sampling_params)[0]\n",
    "    pred_token_ids = pred.outputs[0].token_ids\n",
    "    pred_text_1 = pred.outputs[0].text\n",
    "    pred_log_probs = pred.outputs[0].logprobs\n",
    "    seq_score = pred.outputs[0].cumulative_logprob / \\\n",
    "        max(len(pred.outputs[0].token_ids), 1)\n",
    "    relevance_indices = []\n",
    "    for tok_idx, tok in enumerate(pred_token_ids):\n",
    "        if tok in rel_tokens.values():\n",
    "            relevance_indices.append(tok_idx)\n",
    "    if len(relevance_indices) > 0:\n",
    "        for idx in relevance_indices:\n",
    "            for token, token_id in rel_tokens.items():\n",
    "                prob = pred_log_probs[idx][token_id].logprob if token_id in pred_log_probs[idx] else -100\n",
    "                rel_score_dict[token] = np.exp(prob)\n",
    "    relevance_score = rel_score_dict['[Fully Relevant]']+ rel_score_dict['[Partially Relevant]'] / np.sum(list(rel_score_dict.values()))\n",
    "    processed_pred = pred_text_1.split('[Retrieve Entity]')[0]\n",
    "    matches =  dict(re.findall(pattern,processed_pred))\n",
    "    \n",
    "    name2id = dict()\n",
    "    entity_prompts = []\n",
    "    for _, entity in enumerate(topic_entity):\n",
    "        entities = []\n",
    "        for k, v in matches.items():\n",
    "            if v in ['Fully Relevant', 'Partially Relevant']:\n",
    "                another_entities = get_another_entity(entity, k, return_label=True)\n",
    "                # print(another_entities)\n",
    "                name2id.update(another_entities)\n",
    "                entities.extend([f'({get_label(entity)}, {k}, {e})' for e in another_entities.values()])\n",
    "        entity_prompts.append(aug_prompts[0] + processed_pred +  '[Retrieve Entity]' + \"<paragraph>{}</paragraph>\".format(';'.join(entities[:10])))\n",
    "    # print(aug_prompts)\n",
    "    preds = model.generate([prompt + retrieval_token+ entity_prompts[i] for i in range(len(entity_prompts))], sampling_params)\n",
    "    \n",
    "    for p_idx, pred in enumerate(preds):\n",
    "        pred_token_ids = pred.outputs[0].token_ids\n",
    "        pred_text_2 = pred.outputs[0].text\n",
    "        pred_log_probs = pred.outputs[0].logprobs\n",
    "        rel_score_dict = {}\n",
    "        relevance_indices = []\n",
    "        for tok_idx, tok in enumerate(pred_token_ids):\n",
    "            if tok in rel_tokens.values():\n",
    "                relevance_indices.append(tok_idx)\n",
    "        if len(relevance_indices) > 0:\n",
    "            # print(relevance_indices)\n",
    "            for idx in relevance_indices:\n",
    "                for token, token_id in rel_tokens.items():\n",
    "                    prob = pred_log_probs[idx][token_id].logprob if token_id in pred_log_probs[idx] else -100\n",
    "                    rel_score_dict[token] = np.exp(prob)\n",
    "        overall_scores[p_idx] = relevance_score + rel_score_dict['[Fully Relevant]'] + rel_score_dict['[Partially Relevant]']/ np.sum(list(rel_score_dict.values()))\n",
    "        if '[Continue to Retrieve Evidence]' in pred_text_2:\n",
    "            processed_pred = pred_text_2.split('[Continue to Retrieve Evidence]')[0]\n",
    "            matches =  dict(re.findall(pattern, processed_pred))\n",
    "            for k, v in matches.items():\n",
    "                if v in ['Fully Relevant', 'Partially Relevant']:\n",
    "                    if k in name2id:\n",
    "                        return_entities.append(name2id[k])\n",
    "            processed_pred += '[Continue to Retrieve Evidence]'\n",
    "        else:\n",
    "            processed_pred = pred_text_2\n",
    "        final_preds.append(aug_prompts[0] + processed_pred)\n",
    "    return final_preds, [overall_scores[p_idx] for p_idx in overall_scores], return_entities\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Charles E. Smith Center': 'm.075ls6'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_another_entity('m.03d0l76', 'sports.sports_team.arena_stadium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "with open('/media/disk1/chatgpt/zh/ToG/data/graliqa.json', 'r',encoding='utf-8') as f:\n",
    "    test_data = json.load(f)\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1639\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('../data/merged/WebQSP_test.json', 'r',encoding='utf-8') as f:\n",
    "    test_data = json.load(f)\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3531\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('./data/merged/CWQ_test.json', 'r', encoding='utf-8') as f:\n",
    "    test_data = json.load(f)\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import random\n",
    "def save_to_json(data: List, data_path=''):\n",
    "    if not os.path.isfile(data_path):\n",
    "        # 文件不存在，创建新列表并写入文件\n",
    "        with open(data_path, 'w', encoding='utf-8') as file:\n",
    "            json.dump(data, file, ensure_ascii=False, indent=4)\n",
    "        return\n",
    "    try:\n",
    "        # 尝试读取现有文件\n",
    "        with open(data_path, 'r', encoding='utf-8') as file:\n",
    "            # 加载现有的JSON数据\n",
    "            existing_data = json.load(file)\n",
    "            existing_data.extend(data)\n",
    "    except json.JSONDecodeError:\n",
    "        # 文件不是有效的JSON，打印错误信息并退出\n",
    "        print(f\"文件 {data_path} 不是有效的JSON格式。\")\n",
    "        return\n",
    "    except ValueError as e:\n",
    "        # 打印错误信息并退出\n",
    "        print(e)\n",
    "        return\n",
    "    # 将更新后的数据写回文件\n",
    "    with open(data_path, 'w', encoding='utf-8') as file:\n",
    "        json.dump(existing_data, file, ensure_ascii=False, indent=4)\n",
    "def random_sample(lst, k=3):\n",
    "    try:\n",
    "        # 尝试从列表中随机抽取k个不重复的元素\n",
    "        return random.sample(lst, k)\n",
    "    except ValueError:\n",
    "        # 如果列表长度小于k，返回整个列表\n",
    "        return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/media/disk1/chatgpt/zh/graph_data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from src.sparql_utils import *\n",
    "import random\n",
    "#维护一个relation 池\n",
    "def run_relation_generation_batch(model, prompt, new_retrieval, context, topic_entity, hypo=True, use_1hop=True, income=None):\n",
    "    rel_score_dict = {}\n",
    "    final_preds = []\n",
    "    overall_scores = []\n",
    "    final_contexts = []\n",
    "    paragraph = set()\n",
    "    if new_retrieval:\n",
    "        retrieval_token = \"[Retrieve Relation]\"\n",
    "    else:\n",
    "        retrieval_token = \"[Retrieve Relation]\"\n",
    "    if use_1hop:\n",
    "        candidate_relations = []\n",
    "        for entity in topic_entity:\n",
    "            try:\n",
    "                candidate_relations.extend(get_1hop_relations_with_odbc(entity))\n",
    "            except:\n",
    "                continue\n",
    "        if len(candidate_relations):\n",
    "            vec_db = FAISS.from_texts(candidate_relations, cached_embedder)\n",
    "        else:\n",
    "            vec_db = all_db\n",
    "        retriever = vec_db.as_retriever(search_kwargs={\"k\": 5})\n",
    "    else:\n",
    "        retriever = all_db.as_retriever(search_kwargs={\"k\": 5})\n",
    "    paragraph.update([page.page_content.strip() for page in retriever.invoke(prompt.split('\\n\\n')[1].split('<|eot_id|>')[0] + ' '+ context)])\n",
    "    if hypo:\n",
    "        hypo_rel = model.generate(prompt + retrieval_token, sampling_params)[0].outputs[0].text\n",
    "        pattern = r'(\\w+\\.\\w+\\.\\w+)\\[(.*?)\\]'\n",
    "        if '[Retrieve Entity]' in hypo_rel:\n",
    "            hypo_rel = hypo_rel.split('[Retrieve Entity]')[0]\n",
    "        matches =  dict(re.findall(pattern, hypo_rel))\n",
    "        string = ''\n",
    "        for k,v in matches.items():\n",
    "            if v in ['Fully Relevant', 'Partially Relevant']:\n",
    "                string += k + ' '\n",
    "        paragraph.update([page.page_content.strip() for page in retriever.invoke(string)])\n",
    "    paragraph.discard(income)\n",
    "    aug_prompts =  [\"<paragraph>{}</paragraph>\".format(';'.join(p))  for p in [list(paragraph)[i: i+5] for i in range(0, len(paragraph), 5)]]\n",
    "    \n",
    "    preds = model.generate([prompt + retrieval_token + aug for aug in aug_prompts], sampling_params)\n",
    "    for p_id, pred in enumerate(preds):\n",
    "        pred_token_ids = pred.outputs[0].token_ids\n",
    "        pred_text_1 = pred.outputs[0].text\n",
    "        pred_log_probs = pred.outputs[0].logprobs\n",
    "        seq_score = pred.outputs[0].cumulative_logprob / \\\n",
    "            max(len(pred.outputs[0].token_ids), 1)\n",
    "        relevance_indices = []\n",
    "        for tok_idx, tok in enumerate(pred_token_ids):\n",
    "            if tok in rel_tokens.values():\n",
    "                relevance_indices.append(tok_idx)\n",
    "        if len(relevance_indices) > 0:\n",
    "            for idx in relevance_indices:\n",
    "                for token, token_id in rel_tokens.items():\n",
    "                    prob = pred_log_probs[idx][token_id].logprob if token_id in pred_log_probs[idx] else -100\n",
    "                    rel_score_dict[token] = np.exp(prob)\n",
    "        relevance_score = rel_score_dict['[Fully Relevant]']+ rel_score_dict['[Partially Relevant]'] / np.sum(list(rel_score_dict.values()))\n",
    "        if '[Retrieve Entity]' in pred_text_1:\n",
    "            processed_pred = pred_text_1.split('[Retrieve Entity]')[0] + '[Retrieve Entity]'\n",
    "        else:\n",
    "            processed_pred = pred_text_1\n",
    "        final_preds.append(retrieval_token + aug_prompts[p_id] + processed_pred)\n",
    "        overall_scores.append(0)\n",
    "        final_contexts.append(pred_text_1.split('[Retrieve Entity]')[0])\n",
    "\n",
    "    return final_preds, overall_scores, final_contexts\n",
    "\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def run_entity_generation_batch(model, prompt, topic_entities, context, score_type='hard'):\n",
    "    # 初始化变量\n",
    "    final_predictions = []\n",
    "    overall_scores = {}\n",
    "    final_entities = []\n",
    "    final_contexts = []\n",
    "    entity_prompts = []\n",
    "    income_rel = []\n",
    "    effective_count = 0\n",
    "    name_to_id = {}\n",
    "\n",
    "    # 提取上下文中的实体和相关性匹配\n",
    "    context_pattern = r'(.*?)\\[(.*?)\\]'\n",
    "    context_matches = dict(re.findall(context_pattern, context))\n",
    "\n",
    "    # 构建实体提示\n",
    "    for entity in topic_entities:\n",
    "        for key, relevance in context_matches.items():\n",
    "            if relevance in ['Fully Relevant', 'Partially Relevant']:\n",
    "                try:\n",
    "                    related_entities = get_another_entity(entity, key, return_label=True)\n",
    "                except Exception:\n",
    "                    related_entities = {}\n",
    "\n",
    "                if related_entities:\n",
    "                    # 初始化分数和实体映射\n",
    "                    income_rel.append(key)\n",
    "                    name_to_id[effective_count] = related_entities\n",
    "                    overall_scores[effective_count] = {\n",
    "                        'r_relevance': 1.0 if relevance == 'Fully Relevant' else 0.5\n",
    "                    }\n",
    "                    # 构造实体提示\n",
    "                    entities = [\n",
    "                        f'({get_label(entity)}, {key}, {rel_entity})'\n",
    "                        for rel_entity in related_entities.keys()\n",
    "                    ]\n",
    "                    entity_prompts.append(\n",
    "                        f\"<paragraph>{';'.join(entities[:5])}</paragraph>\"\n",
    "                    )\n",
    "                    effective_count += 1\n",
    "\n",
    "    # 生成模型预测\n",
    "    augmented_prompts = [\n",
    "        f\"{prompt}[Retrieve Entity]{entity_prompt}\"\n",
    "        for entity_prompt in entity_prompts\n",
    "    ]\n",
    "    predictions = model.generate(augmented_prompts, sampling_params)\n",
    "\n",
    "    # 解析预测结果\n",
    "    for idx, prediction in enumerate(predictions):\n",
    "        pred_output = prediction.outputs[0]\n",
    "        pred_text = pred_output.text\n",
    "        pred_log_probs = pred_output.logprobs\n",
    "        pred_token_ids = pred_output.token_ids\n",
    "        seq_score = pred_output.cumulative_logprob / max(len(pred_token_ids), 1)\n",
    "\n",
    "        processed_pred, rel_score, reason_score, return_entities = process_prediction(\n",
    "            idx, pred_text, context_pattern, name_to_id, overall_scores\n",
    "        )\n",
    "\n",
    "        # 更新分数和最终结果\n",
    "        overall_scores[idx].update({\n",
    "            \"e_relevance\": rel_score,\n",
    "            \"reason\": reason_score,\n",
    "            \"final_score\": np.exp(rel_score) * np.exp(reason_score) * np.exp(overall_scores[idx]['r_relevance'])\n",
    "        })\n",
    "\n",
    "        final_predictions.append(f\"[Retrieve Entity]{entity_prompts[idx]}{processed_pred}\")\n",
    "        final_entities.append(list(return_entities.values()))\n",
    "        final_contexts.append(' '.join(return_entities.keys()))\n",
    "\n",
    "    # 返回最终结果\n",
    "    return final_predictions, [\n",
    "        overall_scores[idx]['final_score'] for idx in overall_scores\n",
    "    ], final_entities, final_contexts, overall_scores, income_rel\n",
    "\n",
    "\n",
    "def process_prediction(idx, pred_text, pattern, name_to_id, overall_scores):\n",
    "    \"\"\"处理单条预测结果，计算相关性分数和返回实体。\"\"\"\n",
    "    rel_score = 0\n",
    "    reason_score = 0\n",
    "    return_entities = {}\n",
    "    count = 0\n",
    "\n",
    "    if '[Retrieve Relation]' in pred_text:\n",
    "        processed_pred = pred_text.split('[Retrieve Relation]')[0]\n",
    "        matches = dict(re.findall(pattern, processed_pred))\n",
    "        for key, relevance in matches.items():\n",
    "            if relevance in ['Fully Relevant', 'Partially Relevant']:\n",
    "                if key in name_to_id[idx]:\n",
    "                    return_entities[key] = name_to_id[idx][key]\n",
    "                elif key == 'Unknown Entity':\n",
    "                    random_key = random.choice(list(name_to_id[idx].keys()))\n",
    "                    return_entities[random_key] = name_to_id[idx][random_key]\n",
    "                # 更新相关性分数\n",
    "                rel_score = ((1 if relevance == 'Fully Relevant' else 0.5) +\n",
    "                             count * rel_score) / (count + 1)\n",
    "                count += 1\n",
    "        processed_pred += '[Retrieve Relation]'\n",
    "\n",
    "    elif '[No Retrieval]' in pred_text:\n",
    "        processed_pred = pred_text\n",
    "        matches = dict(re.findall(pattern, pred_text.split('[No Retrieval]')[0]))\n",
    "        for key, relevance in matches.items():\n",
    "            if relevance in ['Fully Relevant', 'Partially Relevant']:\n",
    "                rel_score = ((1 if relevance == 'Fully Relevant' else 0.5) +\n",
    "                             count * rel_score) / (count + 1)\n",
    "                count += 1\n",
    "    else:\n",
    "        processed_pred = '[No Retrieval]Answer: None'\n",
    "        rel_score = 0\n",
    "\n",
    "    # 处理理由分数\n",
    "    if '[Fully Reasonable]' in processed_pred:\n",
    "        reason_score += 1\n",
    "    elif '[Partially Reasonable]' in processed_pred:\n",
    "        reason_score += 0.5\n",
    "\n",
    "    return processed_pred, rel_score, reason_score, return_entities\n",
    " \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1314"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count / 1600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.54s/it, est. speed input: 12.38 toks/s, output: 65.17 toks/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:01<00:00,  1.98it/s, est. speed input: 104.31 toks/s, output: 126.17 toks/s]\n",
      "Processed prompts: 100%|██████████| 3/3 [00:01<00:00,  1.89it/s, est. speed input: 235.10 toks/s, output: 92.02 toks/s]\n",
      "Processed prompts: 100%|██████████| 3/3 [00:01<00:00,  1.86it/s, est. speed input: 232.57 toks/s, output: 88.92 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.53s/it, est. speed input: 86.33 toks/s, output: 65.40 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.35s/it, est. speed input: 136.31 toks/s, output: 68.16 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  4.04it/s, est. speed input: 1060.05 toks/s, output: 57.07 toks/s]\n"
     ]
    }
   ],
   "source": [
    "# add reasonable score\n",
    "\n",
    "count = 0\n",
    "correct_ids = []\n",
    "logging_res = []\n",
    "for index in range(106, len(test_data)):\n",
    "# index = 483\n",
    "    hit = 0\n",
    "    print(f'Process {index}')\n",
    "    data_input = test_data[index]['question']\n",
    "    prompt = PROMPT_DICT['llama3'].format(input= data_input)\n",
    "    max_depth = 5\n",
    "    # topic_entity = list(test_data[index]['topic_entity'].keys())\n",
    "    topic_entity = list(test_data[index]['gold_entity_map'].keys())\n",
    "    # pred = model.generate([prompt], sampling_params)[0]\n",
    "    # pred_text = pred.outputs[0].text\n",
    "    # if '[New Retrieval]' in pred_text:\n",
    "    curr_depth = 1\n",
    "    terminated = False\n",
    "    node_id = 0\n",
    "    prediction_tree = {}\n",
    "    levels = {}\n",
    "    prediction_tree[node_id] = {\"prompt\": prompt, \"pred\": \"[Retrieve Relation]\",\n",
    "                                \"processed_pred\": \"\", \"score\": None, \"topic_entity\": topic_entity, \"parent\": None, \"context\": '', \"income\": ''}\n",
    "    levels[0] = [0]\n",
    "    while curr_depth < max_depth:\n",
    "        levels[curr_depth] = []\n",
    "        if curr_depth-1 in levels:\n",
    "            for node in levels[curr_depth-1]:\n",
    "                curr_pred = prediction_tree[node][\"pred\"]\n",
    "                if \"<|eot_id|>\" in curr_pred or 'No Retrieval' in curr_pred:\n",
    "                    continue\n",
    "                prompt = prediction_tree[node][\"prompt\"]\n",
    "                prev_generation = prediction_tree[node][\"processed_pred\"]\n",
    "                score = prediction_tree[node][\"score\"]\n",
    "                topic_entity = prediction_tree[node][\"topic_entity\"]\n",
    "                context = prediction_tree[node]['context']\n",
    "                cur_prompt = prompt + prev_generation\n",
    "                income_rel = prediction_tree[node]['income']\n",
    "                if \"Retrieve Entity\" in curr_pred.split('[')[-1]:\n",
    "                    retrieval_results = {}\n",
    "                    preds, scores, next_entities, contexts, overall_scores, income_rel = run_entity_generation_batch(\n",
    "                        model, cur_prompt, topic_entity, context)\n",
    "                    for i, (pred, p_score,next_topic, context, rel) in enumerate(zip(preds, scores, next_entities, contexts, income_rel)):\n",
    "                        retrieval_results[i] = {\n",
    "                            \"pred\": pred, \"score\": p_score, \"next_topic\": next_topic, \"context\": context, \"income\": rel}\n",
    "                    for i, result in retrieval_results.items():\n",
    "                        node_id += 1\n",
    "                        node_score = (result[\"score\"] + score) / (curr_depth // 2)\n",
    "                        pred = result[\"pred\"]\n",
    "                        next_entity = result['next_topic']\n",
    "                        if len(next_entity) == 0:\n",
    "                            next_entity = topic_entity\n",
    "                        prediction_tree[node_id] = {\"prompt\": cur_prompt, \"pred\": pred, \"context\": result['context'],\n",
    "                                                    \"score\": node_score, \"parent\": node,\n",
    "                                                    \"topic_entity\": next_entity, \"income\": result['income']}\n",
    "                        if \"[Continue to Retrieve Evidence]\" in pred:\n",
    "                            gen_result_index = pred.index(\"[Continue to Retrieve Evidence]\")\n",
    "                            prev_generation = pred[:gen_result_index]\n",
    "                        else:\n",
    "                            prev_generation = pred\n",
    "                        prediction_tree[node_id][\"processed_pred\"] = prev_generation\n",
    "                        levels[curr_depth].append(node_id)\n",
    "                #存在前后逻辑粘连   \n",
    "                if \"Retrieve Relation\" in curr_pred.split('[')[-1]:\n",
    "                    retrieval_results = {}\n",
    "                    preds, scores, contexts = run_relation_generation_batch(\n",
    "                        model, cur_prompt, new_retrieval=True if (\"[New Retrieval]\" in curr_pred) else False, context=context, topic_entity=topic_entity, hypo=True, income=income_rel)\n",
    "                    for i, (pred, p_score, context) in enumerate(zip(preds, scores, contexts)):\n",
    "                        retrieval_results[i] = {\n",
    "                            \"pred\": pred, \"score\": p_score, \"context\": context}\n",
    "\n",
    "                    for i, result in retrieval_results.items():\n",
    "                        node_id += 1\n",
    "                        #计算score\n",
    "                        node_score = result[\"score\"] + score if score is not None else result[\"score\"]\n",
    "                        # node_score = result['score']\n",
    "                        pred = result[\"pred\"]\n",
    "                        context = result[\"context\"]\n",
    "                        prediction_tree[node_id] = {\"prompt\": cur_prompt, \"pred\": pred,\n",
    "                                                    \"score\": node_score, \"parent\": node,\n",
    "                                                \"topic_entity\": topic_entity, \"context\": context, 'income': income_rel}\n",
    "                        if \"[Retrieve Entity]\" in pred:\n",
    "                            gen_result_index = pred.index(\"[Retrieve Entity]\")\n",
    "                            prev_generation = pred[:gen_result_index]\n",
    "                        else:\n",
    "                            prev_generation = pred\n",
    "                        prediction_tree[node_id][\"processed_pred\"] = prev_generation\n",
    "                        levels[curr_depth].append(node_id)\n",
    "            current_rank = levels[curr_depth]\n",
    "            node2score = {\n",
    "                node_id: prediction_tree[node_id][\"score\"] for node_id in current_rank}\n",
    "            top_nodes = sorted(node2score.items(), key=lambda x: x[1], reverse=True)[\n",
    "                :3]\n",
    "            levels[curr_depth] = [node[0] for node in top_nodes]\n",
    "            curr_depth += 1\n",
    "        else:\n",
    "            break\n",
    "    labels = [get_label(ans) if ans.startswith('m.') else ans for ans in test_data[index]['answer']]\n",
    "    # labels = [ans['entity_name'] for ans in test_data[index]['answer']]\n",
    "    # print(labels)\n",
    "    max_score = 0\n",
    "    for ind, tree_node in enumerate(prediction_tree.values()):\n",
    "        if 'Answer' in tree_node['processed_pred']:\n",
    "            if tree_node['score'] > max_score:\n",
    "                max_score = tree_node['score']\n",
    "                answer = tree_node['processed_pred'].split('Answer:')[-1]\n",
    "    # for tree_node in prediction_tree.values():\n",
    "    #     if 'Answer' in tree_node['processed_pred']:\n",
    "    #         answer = tree_node['processed_pred'].split('Answer:')[-1]\n",
    "    for label in labels:\n",
    "        if label and label in answer:\n",
    "            hit = 1\n",
    "    if hit == 1:\n",
    "        print('Correct')\n",
    "        count += 1\n",
    "        correct_ids.append(index)\n",
    "    break\n",
    "    # else:\n",
    "        # break\n",
    "\n",
    "        # logging_res.append({\"index\": index, \"tree\": prediction_tree})\n",
    "    # if len(logging_res) == 20:\n",
    "        # save_to_json(logging_res, './output/inference/cwq_test_res_1217.json')\n",
    "    #     logging_res = []\n",
    "#1217修改了relation-1hop,修改了score\n",
    "# save_to_json(logging_res, './output/inference/cwq_test_res_1217.json')\n",
    "    #注意有value标签, e.g. WebQTest-31\n",
    "    # for label in labels:\n",
    "    #     if label and label in prediction_tree[len(prediction_tree)-1]['processed_pred']:\n",
    "    #         count += 1\n",
    "    #         break\n",
    "    # except:\n",
    "    #     print(f'{index} Error')\n",
    "    #     continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Desi Arnaz<|eot_id|>'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7182818284590455  Brussels<|eot_id|>\n",
      "5.935372584634358  Brussels<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "for tree_node in prediction_tree.values():\n",
    "    if 'Answer' in tree_node['processed_pred']:\n",
    "        answer = tree_node['processed_pred'].split('Answer:')[-1]\n",
    "        print(tree_node['score'], answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_score = 0\n",
    "for ind, tree_node in enumerate(prediction_tree.values()):\n",
    "    if 'Answer' in tree_node['processed_pred']:\n",
    "        if tree_node['score'] > max_score:\n",
    "            max_score = tree_node['score']\n",
    "            answer = tree_node['processed_pred'].split('Answer:')[-1]\n",
    "        # print(tree_node['score'])\n",
    "        # print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Desi Arnaz<|eot_id|>'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Comedian', 'Actor', 'Television producer', 'Singer', 'Model']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'prompt': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nwhat was lucille ball<|eot_id|><|start_header_id|>assistant<|end_header_id|>',\n",
       "  'pred': '[Retrieve Relation]',\n",
       "  'processed_pred': '',\n",
       "  'score': None,\n",
       "  'topic_entity': ['m.04nw9'],\n",
       "  'parent': None,\n",
       "  'context': '',\n",
       "  'income': ''},\n",
       " 1: {'prompt': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nwhat was lucille ball<|eot_id|><|start_header_id|>assistant<|end_header_id|>',\n",
       "  'pred': '[Retrieve Relation]<paragraph>people.person.nationality;people.person.profession;people.person.date_of_birth;base.americancomedy.double_act.comic;people.person.religion</paragraph>people.person.nationality[Partially Relevant]people.person.profession[Partially Relevant]people.person.date_of_birth[Unrelevant]base.americancomedy.double_act.comic[Fully Relevant]people.person.religion[Unrelevant][Retrieve Entity]',\n",
       "  'score': 0,\n",
       "  'parent': 0,\n",
       "  'topic_entity': ['m.04nw9'],\n",
       "  'context': 'people.person.nationality[Partially Relevant]people.person.profession[Partially Relevant]people.person.date_of_birth[Unrelevant]base.americancomedy.double_act.comic[Fully Relevant]people.person.religion[Unrelevant]',\n",
       "  'income': '',\n",
       "  'processed_pred': '[Retrieve Relation]<paragraph>people.person.nationality;people.person.profession;people.person.date_of_birth;base.americancomedy.double_act.comic;people.person.religion</paragraph>people.person.nationality[Partially Relevant]people.person.profession[Partially Relevant]people.person.date_of_birth[Unrelevant]base.americancomedy.double_act.comic[Fully Relevant]people.person.religion[Unrelevant]'},\n",
       " 2: {'prompt': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nwhat was lucille ball<|eot_id|><|start_header_id|>assistant<|end_header_id|>',\n",
       "  'pred': '[Retrieve Relation]<paragraph>base.popstra.celebrity.religion;people.person.place_of_birth;people.person.places_lived;people.profession.people_with_this_profession</paragraph>base.popstra.celebrity.religion[Unrelevant]people.person.place_of_birth[Partially Relevant]people.person.places_lived[Partially Relevant]people.profession.people_with_this_profession[Partially Relevant][Retrieve Entity]',\n",
       "  'score': 0,\n",
       "  'parent': 0,\n",
       "  'topic_entity': ['m.04nw9'],\n",
       "  'context': 'base.popstra.celebrity.religion[Unrelevant]people.person.place_of_birth[Partially Relevant]people.person.places_lived[Partially Relevant]people.profession.people_with_this_profession[Partially Relevant]',\n",
       "  'income': '',\n",
       "  'processed_pred': '[Retrieve Relation]<paragraph>base.popstra.celebrity.religion;people.person.place_of_birth;people.person.places_lived;people.profession.people_with_this_profession</paragraph>base.popstra.celebrity.religion[Unrelevant]people.person.place_of_birth[Partially Relevant]people.person.places_lived[Partially Relevant]people.profession.people_with_this_profession[Partially Relevant]'},\n",
       " 3: {'prompt': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nwhat was lucille ball<|eot_id|><|start_header_id|>assistant<|end_header_id|>[Retrieve Relation]<paragraph>people.person.nationality;people.person.profession;people.person.date_of_birth;base.americancomedy.double_act.comic;people.person.religion</paragraph>people.person.nationality[Partially Relevant]people.person.profession[Partially Relevant]people.person.date_of_birth[Unrelevant]base.americancomedy.double_act.comic[Fully Relevant]people.person.religion[Unrelevant]',\n",
       "  'pred': '[Retrieve Entity]<paragraph>(Lucille Ball, people.person.nationality, United States of America)</paragraph>United States of America[Partially Relevant][Partially Reasonable][No Retrieval]Answer: United States of America<|eot_id|>',\n",
       "  'context': '',\n",
       "  'score': 4.481689070338065,\n",
       "  'parent': 1,\n",
       "  'topic_entity': ['m.04nw9'],\n",
       "  'income': 'people.person.nationality',\n",
       "  'processed_pred': '[Retrieve Entity]<paragraph>(Lucille Ball, people.person.nationality, United States of America)</paragraph>United States of America[Partially Relevant][Partially Reasonable][No Retrieval]Answer: United States of America<|eot_id|>'},\n",
       " 4: {'prompt': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nwhat was lucille ball<|eot_id|><|start_header_id|>assistant<|end_header_id|>[Retrieve Relation]<paragraph>people.person.nationality;people.person.profession;people.person.date_of_birth;base.americancomedy.double_act.comic;people.person.religion</paragraph>people.person.nationality[Partially Relevant]people.person.profession[Partially Relevant]people.person.date_of_birth[Unrelevant]base.americancomedy.double_act.comic[Fully Relevant]people.person.religion[Unrelevant]',\n",
       "  'pred': '[Retrieve Entity]<paragraph>(Lucille Ball, people.person.profession, Actor);(Lucille Ball, people.person.profession, Singer);(Lucille Ball, people.person.profession, Model);(Lucille Ball, people.person.profession, Comedian);(Lucille Ball, people.person.profession, Television producer)</paragraph>Actor[Fully Relevant]Singer[Partially Relevant]Model[Partially Relevant]Comedian[Partially Relevant]Television producer[Partially Relevant][Partially Reasonable][No Retrieval]Answer: Actor;Comedian;Television producer;Singer;Model<|eot_id|>',\n",
       "  'context': '',\n",
       "  'score': 4.953032424395115,\n",
       "  'parent': 1,\n",
       "  'topic_entity': ['m.04nw9'],\n",
       "  'income': 'people.person.profession',\n",
       "  'processed_pred': '[Retrieve Entity]<paragraph>(Lucille Ball, people.person.profession, Actor);(Lucille Ball, people.person.profession, Singer);(Lucille Ball, people.person.profession, Model);(Lucille Ball, people.person.profession, Comedian);(Lucille Ball, people.person.profession, Television producer)</paragraph>Actor[Fully Relevant]Singer[Partially Relevant]Model[Partially Relevant]Comedian[Partially Relevant]Television producer[Partially Relevant][Partially Reasonable][No Retrieval]Answer: Actor;Comedian;Television producer;Singer;Model<|eot_id|>'},\n",
       " 5: {'prompt': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nwhat was lucille ball<|eot_id|><|start_header_id|>assistant<|end_header_id|>[Retrieve Relation]<paragraph>people.person.nationality;people.person.profession;people.person.date_of_birth;base.americancomedy.double_act.comic;people.person.religion</paragraph>people.person.nationality[Partially Relevant]people.person.profession[Partially Relevant]people.person.date_of_birth[Unrelevant]base.americancomedy.double_act.comic[Fully Relevant]people.person.religion[Unrelevant]',\n",
       "  'pred': '[Retrieve Entity]<paragraph>(Lucille Ball, base.americancomedy.double_act.comic, Desi Arnaz & Lucille Ball)</paragraph>Desi Arnaz & Lucille Ball[Partially Relevant][Partially Reasonable][Retrieve Relation]',\n",
       "  'context': 'Desi Arnaz & Lucille Ball',\n",
       "  'score': 7.38905609893065,\n",
       "  'parent': 1,\n",
       "  'topic_entity': ['m.01wnvbg'],\n",
       "  'income': 'base.americancomedy.double_act.comic',\n",
       "  'processed_pred': '[Retrieve Entity]<paragraph>(Lucille Ball, base.americancomedy.double_act.comic, Desi Arnaz & Lucille Ball)</paragraph>Desi Arnaz & Lucille Ball[Partially Relevant][Partially Reasonable][Retrieve Relation]'},\n",
       " 6: {'prompt': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nwhat was lucille ball<|eot_id|><|start_header_id|>assistant<|end_header_id|>[Retrieve Relation]<paragraph>base.popstra.celebrity.religion;people.person.place_of_birth;people.person.places_lived;people.profession.people_with_this_profession</paragraph>base.popstra.celebrity.religion[Unrelevant]people.person.place_of_birth[Partially Relevant]people.person.places_lived[Partially Relevant]people.profession.people_with_this_profession[Partially Relevant]',\n",
       "  'pred': '[Retrieve Entity]<paragraph>(Lucille Ball, people.person.place_of_birth, Jamestown)</paragraph>Jamestown[Partially Relevant][Partially Reasonable][No Retrieval]Answer: Jamestown<|eot_id|>',\n",
       "  'context': '',\n",
       "  'score': 4.481689070338065,\n",
       "  'parent': 2,\n",
       "  'topic_entity': ['m.04nw9'],\n",
       "  'income': 'people.person.place_of_birth',\n",
       "  'processed_pred': '[Retrieve Entity]<paragraph>(Lucille Ball, people.person.place_of_birth, Jamestown)</paragraph>Jamestown[Partially Relevant][Partially Reasonable][No Retrieval]Answer: Jamestown<|eot_id|>'},\n",
       " 7: {'prompt': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nwhat was lucille ball<|eot_id|><|start_header_id|>assistant<|end_header_id|>[Retrieve Relation]<paragraph>base.popstra.celebrity.religion;people.person.place_of_birth;people.person.places_lived;people.profession.people_with_this_profession</paragraph>base.popstra.celebrity.religion[Unrelevant]people.person.place_of_birth[Partially Relevant]people.person.places_lived[Partially Relevant]people.profession.people_with_this_profession[Partially Relevant]',\n",
       "  'pred': '[Retrieve Entity]<paragraph>(Lucille Ball, people.person.places_lived, m.03pq_js)</paragraph>Unknown Entity[Partially Relevant][Partially Reasonable][Retrieve Relation]',\n",
       "  'context': 'm.03pq_js',\n",
       "  'score': 4.481689070338065,\n",
       "  'parent': 2,\n",
       "  'topic_entity': ['m.03pq_js'],\n",
       "  'income': 'people.person.places_lived',\n",
       "  'processed_pred': '[Retrieve Entity]<paragraph>(Lucille Ball, people.person.places_lived, m.03pq_js)</paragraph>Unknown Entity[Partially Relevant][Partially Reasonable][Retrieve Relation]'},\n",
       " 8: {'prompt': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nwhat was lucille ball<|eot_id|><|start_header_id|>assistant<|end_header_id|>[Retrieve Relation]<paragraph>base.popstra.celebrity.religion;people.person.place_of_birth;people.person.places_lived;people.profession.people_with_this_profession</paragraph>base.popstra.celebrity.religion[Unrelevant]people.person.place_of_birth[Partially Relevant]people.person.places_lived[Partially Relevant]people.profession.people_with_this_profession[Partially Relevant]',\n",
       "  'pred': '[Retrieve Entity]<paragraph>(Lucille Ball, people.profession.people_with_this_profession, Actor);(Lucille Ball, people.profession.people_with_this_profession, Singer);(Lucille Ball, people.profession.people_with_this_profession, Model);(Lucille Ball, people.profession.people_with_this_profession, Comedian);(Lucille Ball, people.profession.people_with_this_profession, Television producer)</paragraph>Actor[Fully Relevant]Singer[Partially Relevant]Model[Partially Relevant]Comedian[Partially Relevant]Television producer[Partially Relevant][Partially Reasonable][No Retrieval]Answer: Comedian;Actor;Television producer;Singer;Model<|eot_id|>',\n",
       "  'context': '',\n",
       "  'score': 4.953032424395115,\n",
       "  'parent': 2,\n",
       "  'topic_entity': ['m.04nw9'],\n",
       "  'income': 'people.profession.people_with_this_profession',\n",
       "  'processed_pred': '[Retrieve Entity]<paragraph>(Lucille Ball, people.profession.people_with_this_profession, Actor);(Lucille Ball, people.profession.people_with_this_profession, Singer);(Lucille Ball, people.profession.people_with_this_profession, Model);(Lucille Ball, people.profession.people_with_this_profession, Comedian);(Lucille Ball, people.profession.people_with_this_profession, Television producer)</paragraph>Actor[Fully Relevant]Singer[Partially Relevant]Model[Partially Relevant]Comedian[Partially Relevant]Television producer[Partially Relevant][Partially Reasonable][No Retrieval]Answer: Comedian;Actor;Television producer;Singer;Model<|eot_id|>'},\n",
       " 9: {'prompt': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nwhat was lucille ball<|eot_id|><|start_header_id|>assistant<|end_header_id|>[Retrieve Relation]<paragraph>people.person.nationality;people.person.profession;people.person.date_of_birth;base.americancomedy.double_act.comic;people.person.religion</paragraph>people.person.nationality[Partially Relevant]people.person.profession[Partially Relevant]people.person.date_of_birth[Unrelevant]base.americancomedy.double_act.comic[Fully Relevant]people.person.religion[Unrelevant][Retrieve Entity]<paragraph>(Lucille Ball, base.americancomedy.double_act.comic, Desi Arnaz & Lucille Ball)</paragraph>Desi Arnaz & Lucille Ball[Partially Relevant][Partially Reasonable][Retrieve Relation]',\n",
       "  'pred': '[Retrieve Relation]<paragraph>type.type.instance;base.americancomedy.double_act_straight_man.straight_man_in_double_act;base.americancomedy.double_act.straight_man;base.americancomedy.double_act_comic.comic_in_double_act;common.topic.alias</paragraph>type.type.instance[Unrelevant]base.americancomedy.double_act_straight_man.straight_man_in_double_act[Partially Relevant]base.americancomedy.double_act.comic_in_double_act[Partially Relevant]common.topic.alias[Unrelevant][Retrieve Entity]',\n",
       "  'score': 7.38905609893065,\n",
       "  'parent': 5,\n",
       "  'topic_entity': ['m.01wnvbg'],\n",
       "  'context': 'type.type.instance[Unrelevant]base.americancomedy.double_act_straight_man.straight_man_in_double_act[Partially Relevant]base.americancomedy.double_act.comic_in_double_act[Partially Relevant]common.topic.alias[Unrelevant]',\n",
       "  'income': 'base.americancomedy.double_act.comic',\n",
       "  'processed_pred': '[Retrieve Relation]<paragraph>type.type.instance;base.americancomedy.double_act_straight_man.straight_man_in_double_act;base.americancomedy.double_act.straight_man;base.americancomedy.double_act_comic.comic_in_double_act;common.topic.alias</paragraph>type.type.instance[Unrelevant]base.americancomedy.double_act_straight_man.straight_man_in_double_act[Partially Relevant]base.americancomedy.double_act.comic_in_double_act[Partially Relevant]common.topic.alias[Unrelevant]'},\n",
       " 10: {'prompt': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nwhat was lucille ball<|eot_id|><|start_header_id|>assistant<|end_header_id|>[Retrieve Relation]<paragraph>people.person.nationality;people.person.profession;people.person.date_of_birth;base.americancomedy.double_act.comic;people.person.religion</paragraph>people.person.nationality[Partially Relevant]people.person.profession[Partially Relevant]people.person.date_of_birth[Unrelevant]base.americancomedy.double_act.comic[Fully Relevant]people.person.religion[Unrelevant][Retrieve Entity]<paragraph>(Lucille Ball, base.americancomedy.double_act.comic, Desi Arnaz & Lucille Ball)</paragraph>Desi Arnaz & Lucille Ball[Partially Relevant][Partially Reasonable][Retrieve Relation][Retrieve Relation]<paragraph>type.type.instance;base.americancomedy.double_act_straight_man.straight_man_in_double_act;base.americancomedy.double_act.straight_man;base.americancomedy.double_act_comic.comic_in_double_act;common.topic.alias</paragraph>type.type.instance[Unrelevant]base.americancomedy.double_act_straight_man.straight_man_in_double_act[Partially Relevant]base.americancomedy.double_act.comic_in_double_act[Partially Relevant]common.topic.alias[Unrelevant]',\n",
       "  'pred': '[Retrieve Entity]<paragraph>(Desi Arnaz & Lucille Ball, base.americancomedy.double_act_straight_man.straight_man_in_double_act, Desi Arnaz)</paragraph>Desi Arnaz[Partially Relevant][Partially Reasonable][No Retrieval]Answer: Desi Arnaz<|eot_id|>',\n",
       "  'context': '',\n",
       "  'score': 5.935372584634358,\n",
       "  'parent': 9,\n",
       "  'topic_entity': ['m.01wnvbg'],\n",
       "  'income': 'base.americancomedy.double_act_straight_man.straight_man_in_double_act',\n",
       "  'processed_pred': '[Retrieve Entity]<paragraph>(Desi Arnaz & Lucille Ball, base.americancomedy.double_act_straight_man.straight_man_in_double_act, Desi Arnaz)</paragraph>Desi Arnaz[Partially Relevant][Partially Reasonable][No Retrieval]Answer: Desi Arnaz<|eot_id|>'}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [None, 0, 2, 3], 1: [None, 0, 2, 4], 2: [None, 0, 1, 5]}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def backtracking_prediction_tree(levels: dict[int,list[int]], curr_depth: int, prediction_tree: dict[int, dict]) -> dict[int,list[int]]:\n",
    "    '''\n",
    "    get best tracking from prediction_tree base on levels\n",
    "    '''\n",
    "    parent = 0 \n",
    "    best_selections = {}\n",
    "    # Traverse from the bottom \n",
    "    levels = {k: v for k, v in levels.items() if len(v) > 0 and k != 0} # remove empty list in levels\n",
    "    for path_i, node in enumerate(levels[len(levels)]): # beam search \n",
    "        if node == 0:\n",
    "            break\n",
    "        best_selections[path_i] = [node] \n",
    "        current_node = node \n",
    "        current_level = curr_depth \n",
    "        if current_node is None:\n",
    "            continue\n",
    "        while current_level > 0 and current_node is not None:\n",
    "            parent = prediction_tree[current_node][\"parent\"]\n",
    "            best_selections[path_i] = [parent] + best_selections[path_i] \n",
    "            current_node = parent \n",
    "            current_level -= 1\n",
    "    return best_selections\n",
    "backtracking_prediction_tree(levels, 5, prediction_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_eval_metric(best_pred, preds, answers):\n",
    "    correct, total = 0.0, 0.0\n",
    "    for entity in preds:\n",
    "        if entity in answers:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "    if len(answers) == 0:\n",
    "        if total == 0:\n",
    "            return 1.0, 1.0, 1.0, 1.0 # precision, recall, f1, hits\n",
    "        else:\n",
    "            return 0.0, 1.0, 0.0, 1.0 # precision, recall, f1, hits\n",
    "    else:\n",
    "        hits = float(best_pred in answers)\n",
    "        if total == 0:\n",
    "            return 1.0, 0.0, 0.0, hits # precision, recall, f1, hits\n",
    "        else:\n",
    "            precision, recall = correct / total, correct / len(answers)\n",
    "            f1 = 2.0 / (1.0 / precision + 1.0 / recall) if precision != 0 and recall != 0 else 0.0\n",
    "            return precision, recall, f1, hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_eval_metric(preds, answers):\n",
    "    correct, total = 0.0, 0.0\n",
    "    for entity in preds:\n",
    "        if entity in answers:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "    # if len(answers) == 0:\n",
    "    #     if total == 0:\n",
    "    #         return 1.0, 1.0, 1.0, 1.0 # precision, recall, f1, hits\n",
    "    #     else:\n",
    "    #         return 0.0, 1.0, 0.0, 1.0 # precision, recall, f1, hits\n",
    "    # else:\n",
    "    if total != 0:\n",
    "        hits = 1\n",
    "    else: \n",
    "        hits = 0\n",
    "    if total == 0:\n",
    "        return 1.0, 0.0, 0.0, hits # precision, recall, f1, hits\n",
    "    else:\n",
    "        precision, recall = correct / total, correct / len(answers)\n",
    "        f1 = 2.0 / (1.0 / precision + 1.0 / recall) if precision != 0 and recall != 0 else 0.0\n",
    "        return precision, recall, f1, hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_eval_metric(preds, answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "correct_ids = []\n",
    "logging_res = []\n",
    "for index in range(0, len(test_data)):\n",
    "# index = 483\n",
    "    hit = 0\n",
    "    print(f'Process {index}')\n",
    "    data_input = test_data[index]['question']\n",
    "    prompt = PROMPT_DICT['llama3'].format(input= data_input)\n",
    "    max_depth = 5\n",
    "    # topic_entity = list(test_data[index]['topic_entity'].keys())\n",
    "    topic_entity = list(test_data[index]['gold_entity_map'].keys())\n",
    "    # pred = model.generate([prompt], sampling_params)[0]\n",
    "    # pred_text = pred.outputs[0].text\n",
    "    # if '[New Retrieval]' in pred_text:\n",
    "    curr_depth = 1\n",
    "    terminated = False\n",
    "    node_id = 0\n",
    "    prediction_tree = {}\n",
    "    levels = {}\n",
    "    prediction_tree[node_id] = {\"prompt\": prompt, \"pred\": \"[New Retrieval]\",\n",
    "                                \"processed_pred\": \"\", \"score\": None, \"topic_entity\": topic_entity, \"parent\": None, \"context\": ''}\n",
    "    levels[0] = [0]\n",
    "    while curr_depth < max_depth:\n",
    "        levels[curr_depth] = []\n",
    "        if curr_depth-1 in levels:\n",
    "            for node in levels[curr_depth-1]:\n",
    "                curr_pred = prediction_tree[node][\"pred\"]\n",
    "                if \"<|eot_id|>\" in curr_pred:\n",
    "                    continue\n",
    "                prompt = prediction_tree[node][\"prompt\"]\n",
    "                prev_generation = prediction_tree[node][\"processed_pred\"]\n",
    "                score = prediction_tree[node][\"score\"]\n",
    "                topic_entity = prediction_tree[node][\"topic_entity\"]\n",
    "                context = prediction_tree[node]['context']\n",
    "                cur_prompt = prompt + prev_generation\n",
    "                if \"Retrieve Entity\" in curr_pred.split('[')[-1]:\n",
    "                    retrieval_results = {}\n",
    "                    preds, scores, next_entities, contexts = run_entity_generation_batch(\n",
    "                        model, cur_prompt, topic_entity, context)\n",
    "                    for i, (pred, p_score,next_topic, context) in enumerate(zip(preds, scores, next_entities, contexts)):\n",
    "                        retrieval_results[i] = {\n",
    "                            \"pred\": pred, \"score\": p_score, \"next_topic\": next_topic, \"context\": context}\n",
    "\n",
    "                    for i, result in retrieval_results.items():\n",
    "                        node_id += 1\n",
    "                        node_score = result[\"score\"] * \\\n",
    "                            score if score is not None else result[\"score\"]\n",
    "                        pred = result[\"pred\"]\n",
    "                        next_entity = result['next_topic']\n",
    "                        if len(next_entity) == 0:\n",
    "                            next_entity = topic_entity\n",
    "                        prediction_tree[node_id] = {\"prompt\": cur_prompt, \"pred\": pred, \"context\": result['context'],\n",
    "                                                    \"score\": node_score, \"parent\": node,\n",
    "                                                    \"topic_entity\": next_entity}\n",
    "                        if \"[Continue to Retrieve Evidence]\" in pred:\n",
    "                            gen_result_index = pred.index(\"[Continue to Retrieve Evidence]\")\n",
    "                            prev_generation = pred[:gen_result_index]\n",
    "                        else:\n",
    "                            prev_generation = pred\n",
    "                        prediction_tree[node_id][\"processed_pred\"] = prev_generation\n",
    "                        levels[curr_depth].append(node_id)\n",
    "                #存在前后逻辑粘连   \n",
    "                if \"New Retrieval\" in curr_pred.split('[')[-1] or \"Continue to Retrieve Evidence\" in curr_pred.split('[')[-1]:\n",
    "                    retrieval_results = {}\n",
    "                    preds, scores, contexts = run_relation_generation_batch(\n",
    "                        model, cur_prompt, new_retrieval=True if (\"[New Retrieval]\" in curr_pred) else False, context=context, topic_entity=topic_entity, hypo=True)\n",
    "                    for i, (pred, p_score, context) in enumerate(zip(preds, scores, contexts)):\n",
    "                        retrieval_results[i] = {\n",
    "                            \"pred\": pred, \"score\": p_score, \"context\": context}\n",
    "\n",
    "                    for i, result in retrieval_results.items():\n",
    "                        node_id += 1\n",
    "                        node_score = result[\"score\"] * \\\n",
    "                            score if score is not None else result[\"score\"]\n",
    "                        pred = result[\"pred\"]\n",
    "                        context = result[\"context\"]\n",
    "                        prediction_tree[node_id] = {\"prompt\": cur_prompt, \"pred\": pred,\n",
    "                                                    \"score\": node_score, \"parent\": node,\n",
    "                                                    \"topic_entity\": topic_entity, \"context\": context}\n",
    "                        if \"[Retrieve Entity]\" in pred:\n",
    "                            gen_result_index = pred.index(\"[Retrieve Entity]\")\n",
    "                            prev_generation = pred[:gen_result_index]\n",
    "                        else:\n",
    "                            prev_generation = pred\n",
    "                        prediction_tree[node_id][\"processed_pred\"] = prev_generation\n",
    "                        levels[curr_depth].append(node_id)\n",
    "            current_rank = levels[curr_depth]\n",
    "            node2score = {\n",
    "                node_id: prediction_tree[node_id][\"score\"] for node_id in current_rank}\n",
    "            top_nodes = sorted(node2score.items(), key=lambda x: x[1], reverse=True)[\n",
    "                :3]\n",
    "            levels[curr_depth] = [node[0] for node in top_nodes]\n",
    "            curr_depth += 1\n",
    "        else:\n",
    "            break\n",
    "    labels = [get_label(ans) if ans.startswith('m.') else ans for ans in test_data[index]['answer']]\n",
    "    # labels = [ans['entity_name'] for ans in test_data[index]['answer']]\n",
    "    # print(labels)\n",
    "    for tree_node in prediction_tree.values():\n",
    "        if 'Answer' in tree_node['processed_pred']:\n",
    "            answer = tree_node['processed_pred'].split('Answer:')[-1]\n",
    "            for label in labels:\n",
    "                if label and label in answer:\n",
    "                    hit = 1\n",
    "    if hit == 1:\n",
    "        print('Correct')\n",
    "        count += 1\n",
    "        correct_ids.append(index)\n",
    "        break\n",
    "    # else:\n",
    "        # break\n",
    "\n",
    "        # logging_res.append({\"index\": index, \"tree\": prediction_tree})\n",
    "    # if len(logging_res) == 20:\n",
    "        # save_to_json(logging_res, './output/inference/cwq_test_res_1217.json')\n",
    "    #     logging_res = []\n",
    "#1217修改了relation-1hop,修改了score\n",
    "# save_to_json(logging_res, './output/inference/cwq_test_res_1217.json')\n",
    "    #注意有value标签, e.g. WebQTest-31\n",
    "    # for label in labels:\n",
    "    #     if label and label in prediction_tree[len(prediction_tree)-1]['processed_pred']:\n",
    "    #         count += 1\n",
    "    #         break\n",
    "    # except:\n",
    "    #     print(f'{index} Error')\n",
    "    #     continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('./output/inference/cwq_test_res_1217.json', 'r', encoding='utf-8') as f:\n",
    "    cwq_result = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, error in enumerate(result):\n",
    "    if error['index'] == 14:\n",
    "        print(i, error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwq_result[18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_another_entity('m.0y496z9', 'location.location.time_zones')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "self-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
